[
  {
    "LearningObjective": "AI-1.1.1",
    "LearningObjectiveDescription": "Describe the AI effect and show how it influences the definition of AI.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q1",
        "QuestionText": "Which of the following statements provides the BEST example of the 'AI Effect'?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "People lose their jobs as AI-based systems perform their roles cheaper and better",
          "b": "Competitive computer games lose popularity as AI-based systems always win",
          "c": "Rule-based expert systems for medical diagnosis are no longer considered to be AI",
          "d": "People believe AI will take over the world, as shown in films"
        },
        "CorrectAnswer": "c",
        "Explanation": "c) Is correct. The 'AI Effect' refers to the phenomenon where once an AI technology becomes common and widely used, it's no longer considered 'true AI' but just another software tool. Option c is the best example as rule-based expert systems were once considered cutting-edge AI but are now seen as conventional software."
      }
    ]
  },
  {
    "LearningObjective": "AI-1.4.1",
    "LearningObjectiveDescription": "Recognize the different technologies used to implement AI.",
    "KnowledgeType": "K1",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q2",
        "QuestionText": "Which of the following options is NOT a technology used to implement AI?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Support vector machine",
          "b": "Decision tree",
          "c": "Evolutionary reasoning",
          "d": "Bayesian optimization"
        },
        "CorrectAnswer": "c",
        "Explanation": "Evolutionary reasoning is not a standard AI technology. Support vector machines, decision trees, and Bayesian optimization are all established machine learning techniques used to implement AI systems."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.1.1",
    "LearningObjectiveDescription": "Explain how the testing of ML systems can help prevent adversarial attacks and data poisoning.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q3",
        "QuestionText": "An ML-based toll charging solution determines the type of incoming vehicles from the images captured by a camera. There are different types of cameras available and the solution provider claims to be able to use cameras of different resolutions. The images need to be in jpeg format with a size of 320X480 pixels for the purpose of training the model as well as for predicting the outcome. The model should be able to classify the vehicle types with certain desired high level of accuracy and should be tested against vulnerabilities. Each toll plaza will have its own complete system unconnected to any other system. Which of the following types of testing are the MOST appropriate options for the tests you would choose for system testing?",
        "AnswerOption": "Select 2 options. (2 out of 5)",
        "Answers": {
          "a": "Testing for concept drift",
          "b": "Adversarial testing",
          "c": "Scalability testing",
          "d": "Fairness testing",
          "e": "Data pipeline testing"
        },
        "CorrectAnswer": [
          "b",
          "e"
        ],
        "Explanation": "For this toll charging system, the most appropriate testing types are:<br><br> <strong>Adversarial testing</strong> is important because the system should be tested against vulnerabilities, especially since it's an image classification system that could be susceptible to adversarial attacks.<br><br> <strong>Data pipeline testing</strong> is required because the images can come in various formats and resolutions. For the model to be trained all images should have same format hence this testing is important.<br><br> Component Integration Testing tests that the inputs from the <strong>data pipeline</strong> are received as expected by the model, and that any predictions produced by the model are exchanged with the relevant system components (e.g., the user interface) and used correctly by them."
      }
    ]
  },
  {
    "LearningObjective": "AI-1.8.1",
    "LearningObjectiveDescription": "TBD",
    "KnowledgeType": "TBD",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q4",
        "QuestionText": "You are to develop an AI model \"Flower\" that classifies plants by species based on images of their flowers. You consider reusing an existing AI model \"Leaf\" that classifies the same plants by species based on images of their leaves, as a pre-trained basis for your new AI model \"Flower\". Which of the following statements regarding reusing the pre-trained model \"Leaf\" compared to developing a new model \"Flower\" is LEAST valid?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "The classification behavior of the AI model \"Flower\" is more difficult to understand when it is reused compared to when it is newly developed.",
          "b": "The possible inputs of the \"Leaf\" and \"Flower\" AI models are so different that reuse has few advantages over redevelopment.",
          "c": "When reused, the probability of misclassification of the AI model \"Flower\" is higher than when it is newly developed.",
          "d": "The possible outputs of the \"Leaf\" and \"Flower\" AI models are so different that reuse has few advantages over new development."
        },
        "CorrectAnswer": "d",
        "Explanation": "The least valid statement is option d. The outputs of both the <strong>Leaf</strong> and <strong>Flower</strong> models are identical, as both classify the same plant species. Therefore, reusing the pre-trained <strong>Leaf</strong> model for <strong>Flower</strong> has advantages because the output layers can be directly adapted or fine-tuned, leveraging the shared classification task. In contrast, the other options highlight potential challenges that may arise from reusing the model, such as input differences (option b), interpretability issues (option a), or possible increased misclassification (option c), which are more valid concerns in transfer learning scenarios."
      }
    ]
  },
  {
    "LearningObjective": "AI-4.1.1",
    "LearningObjectiveDescription": "Describe the activities and challenges related to data preparation.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q5",
        "QuestionText": "Which of the following statements regarding data preparation as part of the ML workflow is correct?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "One challenge of data gathering as part of data preparation is obtaining high quality data from multiple sources",
          "b": "Sampling is a data preparation activity and is so well researched that it is no longer considered risky.",
          "c": "A key challenge in data transformation is the removal or correction of erroneous data,",
          "d": "Since data preparation is very time-consuming, all its steps should be automated"
        },
        "CorrectAnswer": "a",
        "Explanation": "Option A is correct because one of the main challenges in data preparation is obtaining high-quality data from multiple sources, which often have different formats, quality levels, and potential inconsistencies."
      }
    ]
  },
  {
    "LearningObjective": "AI-7.7.1",
    "LearningObjectiveDescription": "For a given scenario determine a test approach to be followed when developing an ML system.",
    "KnowledgeType": "K4",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q6",
        "QuestionText": "A manufacturer of high-quality digital cameras has demanding customers. For the new camera model, the existing self-timer function for smiling faces is to be replaced for the first time by an AI-based function. The non-self-learning ML model planned for this purpose is trained by a newly hired ML expert, who can draw on a high-quality and representative image dataset for this purpose. The rest of the team has so far only developed conventional software and created efficient test automation solutions for it. The ML expert, on the other hand, has no experience yet with digital cameras, their operational data pipeline and use by users. Which two of the following choices are BEST as test approaches for the situation described?",
        "AnswerOption": "Select 2 options. (2 out of 5)",
        "Answers": {
          "a": "You include experience-based testing in system testing to prevent users from being dissatisfied with the function.",
          "b": "You plan the review of operating procedures in the ML workflow by technical experts and developers to prevent poor workflows.",
          "c": "You plan reviews and exploratory data analysis of image datasets to reduce the risk of lack of representativeness of these data",
          "d": "You include regular manual regression testing of the camera function and track the evolution of the functional performance characteristics,",
          "e": "You apply the test cases of the old camera model at the integration test level, no further dynamic tests of the operational data pipeline are necessary."
        },
        "CorrectAnswer": [
          "a",
          "b"
        ],
        "Explanation": "Explanation for Selecting Options A and B as the Best Test Approaches<br><br>In the given scenario, a digital camera manufacturer is introducing an AI-based self-timer function to replace an existing one. The ML expert developing the model is new to digital cameras and their operational data pipeline, while the rest of the team has expertise in conventional software and test automation but not in ML. The model is non-self-learning, and a high-quality dataset is available. Based on ISTQB guidelines, options A and B are the most appropriate test approaches for the following reasons:<br><br><strong>Option A: Experience-Based Testing in System Testing</strong><br><br><strong>Validity</strong>: This approach is highly effective because it leverages the team's existing strengths in conventional software testing while addressing the ML expert's lack of experience with camera users. Experience-based testing techniques (such as usability testing, user scenario testing, or exploratory testing) focus on real-world usage and user satisfaction. Since the customers are demanding, ensuring that the AI function meets their expectations is critical. The ML expert may not fully understand how users interact with the camera, so involving the team in system testing based on their experience can identify usability issues, edge cases, or unexpected behaviors that automated tests might miss. This proactive measure reduces the risk of user dissatisfaction after deployment<br><br><strong>Option B: Review of Operating Procedures in the ML Workflow</strong><br><br><strong>Validity:</strong> This approach is essential due to the ML expert's unfamiliarity with the camera's operational data pipeline and the team's lack of ML expertise. Reviewing operating procedures (e.g., data preprocessing, model integration, deployment workflows) through technical reviews or walkthroughs involving both the ML expert and the camera developers ensures that the ML workflow aligns with the camera's existing architecture and constraints. This collaborative review prevents poor workflows, integration failures, or performance bottlenecks that could arise from misalignment between the ML component and the camera system. It mitigates risks associated with the ML expert's knowledge gap by leveraging the team's deep understanding of the camera hardware and software.<br><br><strong>Why Other Options Are Less Suitable:</strong><br><br><strong>Option C:</strong> While reviews and exploratory data analysis of datasets are generally important, the scenario explicitly states that the dataset is already <strong>high-quality and representative.</strong> Thus, this approach may offer diminishing returns compared to addressing more critical risks like user experience or workflow integration. The ML expert should have already handled data quality during training, making this less urgent.<br><br><strong>Option D:</strong> Regular manual regression testing and performance tracking are inefficient given the team's expertise in test automation. For a non-self-learning model, performance tracking is valuable, but manual testing is time-consuming and error-prone. Automating these tests would be more aligned with the team's skills, and focusing solely on manual efforts misses opportunities for efficiency.<br><br><strong>Option E:</strong> Reusing test cases from the old model and skipping dynamic tests of the data pipeline is risky because the AI-based function is fundamentally different from the conventional one. The old tests may not cover AI-specific aspects like model inference, data pipeline integrity, or new failure modes. Dynamic testing of the data pipeline is crucial since the ML expert is new to it, and omissions could lead to operational failures.<br><br><strong>In summary, options A and B directly address the key knowledge gaps and risks in the scenario</strong>, making them the best choices for ensuring a successful AI function implementation."
      }
    ]
  },
  {
    "LearningObjective": "AI-2.4.1",
    "LearningObjectiveDescription": "Describe the different causes and types of bias found in AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q7",
        "QuestionText": "The training of an ML model for the optical inspection of the workpieces of a tool manufacturer is completed with the tuning of the hyperparameters. The ML model is intended to alert quality assurance personnel to any quality defects. What type of bias is LEAST important to look for when testing the model?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Inappropriate bias",
          "b": "Algorithmic bias",
          "c": "Automation bias",
          "d": "Sample bias"
        },
        "CorrectAnswer": "c",
        "Explanation": "<strong>Automation bias</strong> is least important in this context as it refers to human tendency to favor automated system suggestions over human judgment. For optical inspection, sample bias (unrepresentative training data) and algorithmic bias (flaws in the model) are more critical concerns."
      }
    ]
  },
  {
    "LearningObjective": "AI-1.3.1",
    "LearningObjectiveDescription": "Differentiate between AI-based systems and conventional systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q8",
        "QuestionText": "A system is to be developed to detect lung cancer using X-ray images. Which of the following statements BEST describes the difference between a conventional system and an AI system with supervised machine learning?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "An AI system independently determines patterns in X-rays during training; a conventional system requires a human to program in those patterns.",
          "b": "The results of analyzing an X-ray for lung cancer when using an AI system are more understandable than by using a conventional system.",
          "c": "The X-ray images that an AI system can analyze must be structurally different than the X-ray images for a conventional system.",
          "d": "The implementation of an AI system consists mainly of training data, whereas that of a conventional system consists of branches and loops."
        },
        "CorrectAnswer": "a",
        "Explanation": "Option A best describes the fundamental difference: AI systems learn patterns from data during training, while conventional systems require explicit programming of those patterns by humans."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.1.1",
    "LearningObjectiveDescription": "Explain how the testing of ML systems can help prevent adversarial attacks and data poisoning.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q9",
        "QuestionText": "An ML engineer is trying to find exploitable inputs and then use these inputs to retrain the models to make them immune to these inputs. Which of the following options BEST describes the approach being used by the ML engineer?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Validation",
          "b": "Adversarial testing",
          "c": "Data pipeline testing",
          "d": "Scalability testing"
        },
        "CorrectAnswer": "b",
        "Explanation": "This approach describes adversarial testing, where testers deliberately try to find inputs that can fool the model, then use those inputs to retrain and strengthen the model against such attacks."
      }
    ]
  },
  {
    "LearningObjective": "AI-2.8.1",
    "LearningObjectiveDescription": "Recall the characteristics that make it difficult to use AI-based systems in safety-related applications.",
    "KnowledgeType": "K1",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q10",
        "QuestionText": "Which of the following characteristics of AI-based systems makes it difficult to ensure that AI-based systems are safe (e.g., in terms of not harming humans)?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Complexity",
          "b": "Determinism",
          "c": "Interpretability",
          "d": "Robustness"
        },
        "CorrectAnswer": "a",
        "Explanation": "Complexity is the characteristic that makes AI systems difficult to ensure safety. AI systems can have complex, non-linear behaviors that are hard to predict and verify, unlike deterministic systems."
      }
    ]
  },
  {
    "LearningObjective": "AI-7.2.1",
    "LearningObjectiveDescription": "Describe how AI-based systems are tested at each test level.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q11",
        "QuestionText": "Which of the following statements about testing levels for AI-based systems is correct?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Input data testing checks whether the inputs from the data pipeline are received by the model as expected and exchanged with all other system components.",
          "b": "Acceptance testing checks in particular non-functional requirements on the system, e.g. with regard to whether the system can be explained",
          "c": "ML model testing ensures that the relevant, functional performance criteria of ML are met by the tested model.",
          "d": "In case AI is offered as a service, system testing also includes the API tests of this service."
        },
        "CorrectAnswer": "c",
        "Explanation": "Option C is correct. ML model testing specifically focuses on verifying that the model meets its functional performance criteria, such as accuracy, precision, recall, etc."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.7.1",
    "LearningObjectiveDescription": "For a given scenario, select appropriate test techniques when testing an AI-based system.",
    "KnowledgeType": "K4",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q12",
        "QuestionText": "The newly developed AI-based software application CleverPropose is designed to support a bank's customer advisors, assessing the customer's financial situation based on the customer's history and openly accessible data from the financial market. Furthermore, it suggests suitable advisory solutions to the customer advisor. CleverPropose is intended to replace the conventional software application successfully used by customer advisors to date. The model used by CleverPropose has been generated by experts for ML and data based on the bank's internal history of customers and external analysis data from the financial market and has already been successfully tested. There are few existing regression tests for the upcoming system testing. However, the bank's customer advisors understand the key factors influencing an advisory solution and are available for system testing. Which two of the following test procedures are BEST suited for CleverPropose system testing?",
        "AnswerOption": "Select 2 options. (2 out of 5)",
        "Answers": {
          "a": "Back-to-back testing",
          "b": "Metamorphic testing",
          "c": "Exploratory data analysis",
          "d": "Pairwise testing",
          "e": "Adversarial testing"
        },
        "CorrectAnswer": [
          "a",
          "b"
        ],
        "Explanation": "Back-to-back testing (comparing AI system outputs with the conventional system) and metamorphic testing (testing properties that should remain consistent) are most appropriate given the context of replacing an existing system with limited regression tests."
      }
    ]
  },
  {
    "LearningObjective": "HO-9.6.1",
    "LearningObjectiveDescription": "Apply exploratory testing to an AI-based system.",
    "KnowledgeType": "H2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q13",
        "QuestionText": "System testing of an AI-based system is being planned. It has been suggested that exploratory testing is used in addition to scripted test techniques. Which of the following scenarios is MOST likely to be an example of exploratory testing being performed?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Training data is visualized using tools to look at various aspects of the data",
          "b": "Tests written using equivalence partitioning during the previous test cycle are being run",
          "c": "The Google 'ML test checklist' is being used",
          "d": "ML functional performance metrics are being calculated"
        },
        "CorrectAnswer": "a",
        "Explanation": "Visualizing training data to explore various aspects is a characteristic activity of exploratory testing in the AI context, where testers learn about the system and its data while designing and executing tests."
      }
    ]
  },
  {
    "LearningObjective": "AI-1.6.1",
    "LearningObjectiveDescription": "Compare the choices available for hardware to implement AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q14",
        "QuestionText": "Which of the following statements about the hardware used to implement AI-based systems is MOST likely to be CORRECT?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "The processors used to train a mobile recommendation system must be the same as the processors on the mobile phone",
          "b": "Graphical processing units (GPUs) are a reasonable choice to implement an AI-based computer vision system",
          "c": "Deep learning systems need to be trained, evaluated, and tested using AI-specific chips",
          "d": "It is always best to choose processors with more bits to achieve sufficient accuracy for AI-based systems"
        },
        "CorrectAnswer": "b",
        "Explanation": "a) Is not correct. The two activities of training a ML model and inference from that model are quite different so there is normally no reason that they should be performed on the same processors.<br><br><strong>b) Is correct</strong>. GPUs are designed for the parallel processing of images using thousands of cores, which is close to what is required for an AI-based computer vision system that would most likely be implemented as a neural network.<br><br>c) Is not correct. It is still possible to train, evaluate and test a simple deep-learning system on a PC with limited GPU support – so specific chips for AI are not needed, but they would be far faster.<br><br>d) Is not correct. Many AI-based systems are not focused on exact calculations, but rather on probabilistic determinations and so the accuracy of processors with many bits is often unnecessary."
      }
    ]
  },
  {
    "LearningObjective": "AI-1.8.1",
    "LearningObjectiveDescription": "TBD",
    "KnowledgeType": "TBD",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q15",
        "QuestionText": "There are a number of good quality pre-trained models available in the market and you want to use one of them for an image-based classifier. You have decided to ask the provider of the model about the data used for training the model and its format. Which of the following statements is the BEST example of a risk that you are trying to mitigate by asking these questions?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Bad classification accuracy of the pre-trained models",
          "b": "Differences in the data used to train the model and the operational data",
          "c": "Performance efficiency issues of the pre-trained model",
          "d": "Lack of explainability of the pre-trained model compared to that of a model trained by you"
        },
        "CorrectAnswer": "b",
        "Explanation": "a) Is not correct. The question mentions the good quality of the pre-trained model, so this risk should be negligible.<br><br><strong>b) Is correct</strong>. The data used to train the model should be similar to the data used for making the predictions.<br><br>c) Is not correct. Performance does not appear to be an issue in this situation.<br><br>d) Is not correct. Explainability does not appear to be an issue in this situation, nor can it be achieved by looking at the training data and its format."
      }
    ]
  },
  {
    "LearningObjective": "AI-2.2.1",
    "LearningObjectiveDescription": "Explain the relationship between autonomy and AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q16",
        "QuestionText": "Which of the following statements is MOST likely to be specifying a requirement for autonomy in an AI-based system?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "The system shall maintain a safe distance to other vehicles until the brake or accelerator is pushed by the driver",
          "b": "The system shall learn the preferred style of response to emails by remotely monitoring the email traffic",
          "c": "The system shall compare its predictions of house prices with actual selling prices to determine if it needs to be retrained",
          "d": "It shall be possible to modify the system’s behavior to work with different types of users in less than a day"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>a) Is correct</strong>. This requirement defines the human interventions that define the end of the system working autonomously.<br><br>b) Is not correct. This requirement is specifying a required function for how the system shall perform self-learning.<br><br>c) Is not correct. This requirement is specifying how the system will manage concept drift, in this case most likely caused by the house market changing.<br><br>d) Is not correct. This is specifying an adaptability requirement – the maximum time it should take to make a change to the system."
      }
    ]
  },
  {
    "LearningObjective": "AI-2.4.1",
    "LearningObjectiveDescription": "Describe the different causes and types of bias found in AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q17",
        "QuestionText": "Which of the following statements about bias in AI-based systems is NOT correct?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Bias may be caused by users of a book recommendation system making choices that deliberately cause the system to make poor suggestions",
          "b": "Bias may be caused in the employee age of death prediction system by collecting the training data from a dataset of patients who are all retired",
          "c": "Bias may be caused in the creditworthiness system by using training data obtained from those who own and use a credit card",
          "d": "Bias may be caused in the navigation system by using a route planning algorithm that is too complex to be explained to typical users"
        },
        "CorrectAnswer": "d",
        "Explanation": "a) Is not correct. Bias can be caused by users deliberately poisoning the self-learning of an AI-based system.<br><br>b) Is not correct. Bias can be caused when the training data does not correctly match those who the system will be applied to. For instance, employees will typically be younger than retired patients.<br><br>c) Is not correct. Bias can be caused when the training data does not correctly match those who the system will be applied to. For instance, most people using credit cards are already considered creditworthy, which is a typical example of sample bias.<br><br>d) Is correct. If the algorithm cannot be explained, then it lacks explainability, but that does not mean it is biased nor unbiased."
      }
    ]
  },
  {
    "LearningObjective": "AI-2.6.1",
    "LearningObjectiveDescription": "Explain the occurrence of side effects and reward hacking in AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q18",
        "QuestionText": "Which of the following is MOST likely to be an example of reward hacking?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "The programmer’s assistant tool optimizes the code to provide reduced response times, while still ensuring that functional requirements are met",
          "b": "An anesthetic supply device with a goal of keeping patients stable during surgery supplies too many doses and patients do not wake up as quickly as expected",
          "c": "The third-party development organization paid their AI programmers based on the number of lines of code they write",
          "d": "A type of AI used to play competitive computer games against humans that is focused on getting the highest score"
        },
        "CorrectAnswer": "b",
        "Explanation": "a) Is not correct. It appears that the tool is achieving its two goals with there being no detrimental effects, so this is unlikely to be 'reward hacking'.<br><strong>b) Is correct</strong>. This could be 'reward hacking' if the system achieves one goal to the detriment of others, in this case the need for patients to wake up.<br>c) Is not correct. Reward hacking is not a form of paying AI developers<br>d) Is not correct. Some game-playing AI-based systems are driven by a reward function, but this is not known as 'reward hacking'."
      }
    ]
  },
  {
    "LearningObjective": "AI-2.8.1",
    "LearningObjectiveDescription": "Recall the characteristics that make it difficult to use AI-based systems in safety-related applications.",
    "KnowledgeType": "K1",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q19",
        "QuestionText": "Given the following attributes for an AI-based system (I-V): I. Probabilistic II. Explicable III. Unfair IV. Non-deterministic V. Deterministic Which list of attributes below is likely to cause the MOST difficulties if the system is to be used as part of a safety-related system?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "I, IV",
          "b": "II, IV",
          "c": "II, III, V",
          "d": "I, III, V"
        },
        "CorrectAnswer": "a",
        "Explanation": "Considering the given attributes:<br>I. Probabilistic – a definite problem for safety-related systems as this causes non-determinism<br>II. Explicable – normally needed for safety-related systems<br>III. Unfair – not ideal, but sometimes unavoidable – and not a special problem for safety-related systems<br>IV. Non-deterministic - a definite problem for safety-related systems<br>V. Deterministic - normally needed for safety-related systems<br>Therefore, I and IV are the attributes that are MOST problematic for safety-related systems.<br><br><strong>a) Is correct</strong>. <br>b) Is not correct. <br>c) Is not correct. <br>d) Is not correct."
      }
    ]
  },
  {
    "LearningObjective": "AI-3.1.1",
    "LearningObjectiveDescription": "Describe classification and regression as part of supervised learning.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q20",
        "QuestionText": "Which of the following statements BEST describes classification and regression as part of supervised learning?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Regression is checking that the ML model test results do not change when the same test data is executed",
          "b": "Classification is the grouping of unlabeled data into separate classes",
          "c": "Classification is the labelling of the data for training the ML model",
          "d": "Regression is predicting the number of classes that are output by the ML model"
        },
        "CorrectAnswer": "b",
        "Explanation": "a) Is not correct. Regression in the context of supervised learning is generally when the ML model outputs a numeric result.<br><strong>b) Is correct.</strong> Classification is when input data to a ML model is classified into one of a few predefined classes.<br>c) Is not correct. Training data needs to be labelled for training in supervised learning, but this activity is not known as classification. It is simply labelling.<br>d) Is not correct. Regression is when the output from the ML model is numeric, but the output is not a number of classes."
      }
    ]
  },
  {
    "LearningObjective": "AI-3.1.3",
    "LearningObjectiveDescription": "Describe reinforcement learning.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q21",
        "QuestionText": "Which of the following options BEST describes an example of reinforcement learning?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "The mobile game app updates its feedback, response timing and the number of user options it provides based on how much the players spend",
          "b": "The language translation app searches the internet to find text provided in multiple languages to improve its translation function",
          "c": "The factory quality control system uses video cameras and audio analysis to identify manufactured items that are faulty based on monitoring a human quality control operative",
          "d": "The software component test prediction system uses a range of quality measures to identify which components are likely to contain the most defects"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>a) Is correct</strong>. The amount spent can be considered the reward function for this system, with the system changing its behavior to increase the amount spent.<br>b) Is not correct. The app is using text in what can be considered a source language and a <strong>correct</strong> translation of this source. Therefore, it is relying on a form of supervised learning with no reward function mentioned.<br>c) Is not correct. The system is using the human quality control operative as a form of <strong>gold</strong> standard and so is relying on a form of supervised learning.<br>d) Is not correct. There is no suggestion that any reward function is used, instead it is most likely that the prediction system bases its determination of defects on past experience. Therefore, it is probably also relying on a supervised learning system."
      }
    ]
  },
  {
    "LearningObjective": "AI-3.3.1",
    "LearningObjectiveDescription": "Given a project scenario, identify an appropriate ML approach (from classification, regression, clustering, association, or reinforcement learning).",
    "KnowledgeType": "K3",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q22",
        "QuestionText": "You have been asked for your opinion on the ML approach to be used for a new system that is part of the traffic management for a SMART city. The idea is that the new system will control the traffic lights in the city to ensure traffic flows easily through and around the city. Which of the following approaches do you expect MOST likely to succeed?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Unsupervised learning that is based on identifying clusters around the city where the traffic density is higher than average",
          "b": "A supervised learning regression solution based on thousands of journeys labelled with both journey length and duration",
          "c": "Reinforcement learning that is based on a reward function that penalizes solutions that result in higher levels of traffic congestion",
          "d": "A supervised learning classification solution that is based on drivers and passengers submitting their favorite routes for traversing the city"
        },
        "CorrectAnswer": "c",
        "Explanation": "a) Is not correct. It should be possible for the unsupervised learning system to identify areas that are congested, but this alone will not provide the solution.<br>b) Is not correct. A regression solution is unlikely to provide us with what we want as the predicted speed of individual journeys will not provide an overall solution to citywide congestion.<br><strong>c) Is correct.</strong> Continually improving reinforcement learning system with a reward function based on lower levels of congestion as a measure of success is valid for this type of system.<br>d) Is not correct. This solution is dependent on volunteers submitting subjective opinions that will most likely result in a solution that changes back and forth as the system adopts favorite routes that then become congested."
      }
    ]
  },
  {
    "LearningObjective": "AI-3.5.1",
    "LearningObjectiveDescription": "Summarize the concepts of underfitting and overfitting.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q23",
        "QuestionText": "When performing testing of a trained model, an ML engineer found that the model was highly accurate when evaluated with validation data but that it performed poorly with independent test data. Which of the following options is MOST likely to cause this situation?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Underfitting",
          "b": "Concept drift",
          "c": "Overfitting",
          "d": "Poor acceptance criteria"
        },
        "CorrectAnswer": "c",
        "Explanation": "a) Is not correct. The model performs well on validation data, so it is not a case of underfitting.<br>b) Is not correct. Concept drift refers to changes after the model training and validation stage.<br><strong>c) Is correct.</strong> The bad performance on test data and good on validation data suggests overfitting<br>d) Is not correct. Poor acceptance criteria should be consistent with different sets of data, so are unlikely to lead to a difference between the test results with validation data and independent test data."
      }
    ]
  },
  {
    "LearningObjective": "AI-4.1.1",
    "LearningObjectiveDescription": "Describe the activities and challenges related to data preparation.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q24",
        "QuestionText": "Which of the following is an example of a challenge that is likely to be encountered in the course of developing and testing an ML solution?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Data anonymization operations typically require knowledge of various ML algorithms",
          "b": "The data used might be unstructured data",
          "c": "A large percentage of the budget gets spent just in data preparation",
          "d": "The data pipeline scalability is a challenge when training the model"
        },
        "CorrectAnswer": "c",
        "Explanation": "a) Is not correct. Data anonymization operations do not require knowledge of ML algorithms.<br>b) Is not correct. Unstructured data is not a challenge. Images, audio, freeflowing text are all examples of unstructured data.<br><strong>c) Is correct.</strong> Up to 43% of ML workflow effort may be spent in data preparation.<br>d) Is not correct. Scalability typically is a requirement at deployment, rather than when training."
      }
    ]
  },
  {
    "LearningObjective": "AI-4.3.1",
    "LearningObjectiveDescription": "Describe typical dataset quality issues.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q25",
        "QuestionText": "The data scientist has complained that the model cannot be trained with one particular algorithm, although other algorithms work with the same training data. Which of the following options is the MOST likely reason for this?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Wrong data",
          "b": "Missing data",
          "c": "Badly labelled data",
          "d": "Insufficient data"
        },
        "CorrectAnswer": "d",
        "Explanation": "a) Is not correct. Since models based on some learning algorithms can be trained with the data but not one particular algorithm, it indicates that the data is <strong>correct</strong>.<br>b) Is not correct. Since models based on some learning algorithms can be trained with the data but not one particular algorithm, it indicates that there is no missing data.<br>c) Is not correct. Since models based on some learning algorithms can be trained with the data but not one particular algorithm, it indicates that the data is correctly labelled.<br><strong>d) Is correct.</strong> Since models based on some learning algorithms can be trained with the data. However, if it does not work for one particular algorithm, it is MOST likely to be that the quantity of the data that is not sufficient for that particular algorithm."
      }
    ]
  },
  {
    "LearningObjective": "AI-4.5.1",
    "LearningObjectiveDescription": "Recall the different approaches to the labelling of data in datasets for supervised learning.",
    "KnowledgeType": "K1",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q26",
        "QuestionText": "DataSure is a start-up with a product that promises to improve the quality of ML models. DataSure claim that this improvement comes from checking if the data has been labeled correctly. Which of the following defects is MOST likely to have been prevented by using this product?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "The model will have security vulnerabilities",
          "b": "The model will have poor accuracy",
          "c": "The model will not fulfill its intended function",
          "d": "The model will produce biased outputs"
        },
        "CorrectAnswer": "b",
        "Explanation": "a) Is not correct. Data privacy and security issues are not being handled. Hence the product is not going to prevent security issues. <br><strong>b) Is correct.</strong> Mislabeled data results in reduced accuracy of the ML model. <br>c) Is not correct. A model not being fit for purpose arises from is not correct or unfair data, not mislabeled data. <br>d) Is not correct. A biased model results from incomplete data, unbalanced data, unfair data, data lacking diversity, or duplicate data, rather than from mislabeled data."
      }
    ]
  },
  {
    "LearningObjective": "AI-4.5.1",
    "LearningObjectiveDescription": "Recall the different approaches to the labelling of data in datasets for supervised learning.",
    "KnowledgeType": "K1",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q27",
        "QuestionText": "An ML engineer, upon finding insufficient training data, is rotating labeled images to create additional training data. Which of the following approaches to labeling is being applied in this above example?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Crowdsourcing",
          "b": "Augmentation",
          "c": "AI-based labeling",
          "d": "Outsourcing"
        },
        "CorrectAnswer": "b",
        "Explanation": "a) Is not correct. Crowdsourcing is when you use a large number of people to provide some work. In this case only one person is performing the task.<br><strong>b) Is correct.</strong> Augmentation is being performed here by transforming existing labelled data.<br>c) Is not correct. AI is not being used for labeling of the data.<br>d) Is not correct. The ML engineer has not outsourced the task to a third party."
      }
    ]
  },
  {
    "LearningObjective": "AI-5.1.1",
    "LearningObjectiveDescription": "Calculate the ML functional performance metrics from a given set of confusion matrix data.",
    "KnowledgeType": "K3",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q28",
        "QuestionText": "The confusion matrix for an image classifier is shown below: Confusion Matrix | Actual Positive | Actual Negative Predicted Positive | 78 | 22 Predicted Negative | 6 | 14",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "20/120 *100",
          "b": "78/120 *100",
          "c": "78/100 *100",
          "d": "22/100 *100"
        },
        "CorrectAnswer": "c",
        "Explanation": "a) Is not correct. See option c for the correct formula and calculation.<br>b) Is not correct. See option c for the correct formula and calculation.<br><strong>c) Is correct.</strong> The formula for Precision = TP/ (TP+FP) *100 = 78/(78+22) = 78/100 *100<br>d) Is not correct. See option c for the correct formula and calculation."
      }
    ]
  },
  {
    "LearningObjective": "AI-5.2.1",
    "LearningObjectiveDescription": "Contrast and compare the concepts behind the ML functional performance metrics for classification, regression and clustering methods.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q29",
        "QuestionText": "ThermalSpace is a solution provider that helps thermal power plants to optimize their power output. Their solution is based on an ML model created using past data with clearly marked output. The model helps determine the amount of electricity to be generated at a given time of the day. To determine the quality of the model using ML functional performance metrics, which of the following metrics is MOST likely to be used?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "R-squared",
          "b": "Precision",
          "c": "Recall",
          "d": "False Positives"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>a) Is correct.</strong> It is a supervised regression problem because the model outputs a continuous value, the amount of electricity to be generated, which uses the R-Squared or MSE/RMSE metric.<br>b) Is not correct. This is a metric for classification<br>c) Is not correct. This is a metric for classification<br>d) Is not correct. This is a metric for classification"
      }
    ]
  },
  {
    "LearningObjective": "AI-5.4.1",
    "LearningObjectiveDescription": "Select appropriate ML functional performance metrics and/or their values for a given ML model and scenario.",
    "KnowledgeType": "K4",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q30",
        "QuestionText": "KnowYourPet is an app utilizing ML to determine whether a pet is hungry or not. It is understood that a dog is likely to be not hungry most of the time, as reflected in the training data. If the dog is mis-diagnosed as hungry then it may lead to overfeeding of the dog and this could lead to serious health issues. Which of the following metrics would you choose for determining the suitability of the model under test?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Accuracy",
          "b": "Precision",
          "c": "Recall",
          "d": "F1-score"
        },
        "CorrectAnswer": "b",
        "Explanation": "a) Is not correct. Accuracy is not useful when there is an imbalance in the expected classes and the not hungry class dominates in this case.<br><strong>b) Is correct.</strong> Precision should be used because the cost of false-positives (overfeeding the dog) is high (serious health issues).<br>c) Is not correct. Recall is useful when the positives should not be missed. In this case, precision is also important (see b) and hence recall alone is not very useful. F1-score is a better choice.<br>d) Is not correct. F1-score is useful when there is an imbalance in the expected classes and when precision and recall are similarly important, but in this case precision appears to be far more important than recall."
      }
    ]
  },
  {
    "LearningObjective": "AI-6.1.1",
    "LearningObjectiveDescription": "Explain the structure and function of a neural network including a DNN.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q31",
        "QuestionText": "Which of the following options BEST describes a deep neural network?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "It is comprised of a hierarchical structure of neurons with the lowest (deepest) neurons making most of the decisions",
          "b": "It is comprised of connected neurons where each neuron has an associated bias and each connection has an associated weight",
          "c": "It is made up several layers with each layer (except input and output layers) connected to each other layer and errors are propagated backwards through the network",
          "d": "It is made up of layers of neurons, each of which generates an activation value based on the other neurons in the same layer"
        },
        "CorrectAnswer": "b",
        "Explanation": "a) Is not correct. A neural network does not have a hierarchical structure.<br><strong>b) Is correct.</strong> As with the human brain, an artificial neural network is comprised of connected neurons. To perform its calculation of an activation value, each neuron is assigned a bias and each connection is assigned a weight.<br>c) Is not correct. A neural network is made up of several layers and errors are propagated backwards through the network, but the layers of a neural network are only connected to the next layers (not each other layer).<br>d) Is not correct. A neural network is made up of layers of neurons, but the activation value is based on the neurons in the preceding layer (not the same layer)."
      }
    ]
  },
  {
    "LearningObjective": "AI-6.2.1",
    "LearningObjectiveDescription": "Describe the different coverage measures for neural networks.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q32",
        "QuestionText": "Which of the following statements CORRECTLY describes a test coverage measure for neural networks?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Value change coverage is based on individual neurons being seen to affect the overall output of the neural network",
          "b": "Threshold coverage is based on neurons outputting an activation value greater than a pre-set value between zero and one",
          "c": "Neuron coverage is a measure of the proportion of neurons that are activated at any time during the testing",
          "d": "Sign change coverage measures the coverage of neurons that output both positive, negative and zero activation values"
        },
        "CorrectAnswer": "b",
        "Explanation": "a) Is not correct. Value change coverage is a measure of the proportion of neurons activated where their activation values differ by more than a preset change amount. It is not concerned with the overall output of the neural network.<br><strong>b) Is correct.</strong> Threshold coverage measures the proportion of neurons activated during testing with a value greater than a preset threshold value.<br>c) Is not correct. All neurons are potentially 'activated' each time a neural network is 'run', however the values output by the neurons change, which is what is measured by neuron coverage (coverage achieved by a value greater than zero).<br>d) Is not correct. Sign change coverage is a measure of the proportion of neurons activated with both positive and negative activation values, but not zero activation values."
      }
    ]
  },
  {
    "LearningObjective": "AI-7.1.1",
    "LearningObjectiveDescription": "Explain how system specifications for AI-based systems can create challenges in testing.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q33",
        "QuestionText": "Which of the following requirements for an AI-based system is MOST likely to cause a significant challenge in testing?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "The system shall be more accurate than the system it is replacing",
          "b": "The AI component in the system shall have 100% accuracy",
          "c": "A human operator should be able to override the system in 1 second",
          "d": "The system shall mimic the human emotions of a typical game player"
        },
        "CorrectAnswer": "d",
        "Explanation": "a) Is not correct. This is a specific requirement with a test oracle, so should not usually cause a testing challenge.<br>b) Is not correct. This may be a difficult requirement to achieve but should not create a testing challenge.<br>c) Is not correct. This is a testable requirement.<br><strong>d) Is correct.</strong> This requirement is extremely complex to test without defining all human emotions and how the system might mimic them."
      }
    ]
  },
  {
    "LearningObjective": "AI-7.3.1",
    "LearningObjectiveDescription": "Recall those factors associated with test data that can make testing AI-based systems difficult.",
    "KnowledgeType": "K1",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q34",
        "QuestionText": "Which of the following is a factor associated with the test data that can make the testing of AI-based systems difficult?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Sourcing big data with high velocity",
          "b": "Sourcing data from a single source",
          "c": "Sourcing data separately from the data scientists",
          "d": "Sourcing data from public websites"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>a) Is correct.</strong> Sourcing data for AI systems that use large quantities of high-velocity data can be difficult.<br>b) Is not correct. Sourcing consistent data from multiple sources can be difficult.<br>c) Is not correct. Sourcing data separately is good practice as it prevents common failures with the data scientists<br>d) Is not correct. Sourcing data from public websites is straightforward."
      }
    ]
  },
  {
    "LearningObjective": "AI-7.4.1",
    "LearningObjectiveDescription": "Explain automation bias and how this affects testing.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q35",
        "QuestionText": "Why would the accuracy of human decisions be considered in testing as well as the accuracy of AI-based systems?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Intuitive human decisions can be made faster than a corresponding AI-based system in some situations",
          "b": "Unethical decisions can be made by humans as well as AI-based systems",
          "c": "The accuracy of human decisions is not relevant to testing AI-based systems",
          "d": "Human decisions may be of lower quality when they have been recommended by an AI-based system"
        },
        "CorrectAnswer": "d",
        "Explanation": "a) Is not correct. Speed of decision making is not related to accuracy.<br>b) Is not correct. The ethical choices made by humans are not related to testing AI-based systems.<br>c) Is not correct. The accuracy of human decisions is relevant as systems may make recommendations that humans approve or review.<br><strong>d) Is correct.</strong> Human decisions supported by recommendations by AI-based systems may be of lower quality than human decisions without recommendations from a system, and this should be considered in testing."
      }
    ]
  },
  {
    "LearningObjective": "AI-8.1.1",
    "LearningObjectiveDescription": "Explain the challenges in testing created by the self-learning of AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q36",
        "QuestionText": "Which of the following statements BEST describes a testing challenge that specifically applies to a self-learning system?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "The system requires regular retraining and therefore requires regular testing",
          "b": "The system is regularly released which means regression testing is required",
          "c": "The system changes in such a way that tests that previously passed can fail",
          "d": "The system requires a human operator, who is also required for testing"
        },
        "CorrectAnswer": "c",
        "Explanation": "a) Is not correct. A system that requires regular retraining cannot be described as self-learning.<br>b) Is not correct. A system that needs to be regularly released cannot be described as self-learning as it requires frequent releases to adapt to change.<br><strong>c) Is correct.</strong> Tests on a system that makes changes to itself may start to fail, even if they previously passed.<br>d) Is not correct. A system that requires a human operator is unlikely to be self-learning."
      }
    ]
  },
  {
    "LearningObjective": "AI-8.3.1",
    "LearningObjectiveDescription": "Explain how to test for bias in an AI-based system.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q37",
        "QuestionText": "Which of the following is NOT likely to be required to test a system for bias?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Involving selected users that are known to be biased",
          "b": "Measuring how changes in test inputs change test outputs",
          "c": "Observing how production outputs correlate to production inputs",
          "d": "Obtaining additional data from other sources"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>a) Is correct.</strong> Users that exhibit bias are not required to test a system for bias because they do not help to determine whether the behavior of the system is biased.<br>b) Is not correct. Measuring how test inputs change test outputs is important when testing for bias as it can show how the system is biased towards or against particular inputs.<br>c) Is not correct. Measuring how production inputs change production outputs is important when testing for bias because different results might be seen in production.<br>d) Is not correct. Obtaining external data sources can be essential when testing for bias in case the bias is based on 'hidden' variables."
      }
    ]
  },
  {
    "LearningObjective": "AI-8.5.1",
    "LearningObjectiveDescription": "Explain the challenges in testing created by the complexity of AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q38",
        "QuestionText": "Which of the following statements BEST describes how system complexity can create challenges when testing an AI-based system?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Testing for bias may require data that the team does not have",
          "b": "Manual generation of white-box tests can be difficult",
          "c": "Determining whether a system is ethical can be subjective",
          "d": "It can be difficult to find representative data to train a model"
        },
        "CorrectAnswer": "b",
        "Explanation": "a) Is not correct. Bias does not usually relate to system complexity.<br><strong>b) Is correct.</strong> Understanding how the system works and creating enough tests to achieve effective coverage are challenges caused by the complexity of AI-based systems.<br>c) Is not correct. Ethics is not usually related to AI-based system complexity.<br>d) Is not correct. Difficulty finding representative data to train a model is not related to testing or AI-based system complexity."
      }
    ]
  },
  {
    "LearningObjective": "AI-8.8.1",
    "LearningObjectiveDescription": "Select appropriate test objectives and acceptance criteria for the AI-specific quality characteristics of a given AI-based system.",
    "KnowledgeType": "K4",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q39",
        "QuestionText": "An AI-based system is being used by the health ministry to identify vulnerable groups of patients, who will be provided with support and advice to help prevent them suffering future illnesses to which they may be susceptible. The results will also be shared with other government agencies and medical insurance companies. The system is initially being trained on a large set of data collected by the health ministry from two surveys of 5,000 men over 50 years of age and 25,000 women over 30 years of age. The system will continue to identify vulnerable patients by gathering information from publicly available social media. Which of the following attributes should be MOST carefully considered when specifying the objectives and acceptance criteria for the system?",
        "AnswerOption": "Select 2 options. (2 out of 5)",
        "Answers": {
          "a": "Adaptability",
          "b": "Bias",
          "c": "Explainability",
          "d": "Flexibility",
          "e": "Autonomy"
        },
        "CorrectAnswer": [
          "b",
          "c"
        ],
        "Explanation": null
      }
    ]
  },
  {
    "LearningObjective": "AI-9.2.1",
    "LearningObjectiveDescription": "Explain how pairwise testing is used for AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q40",
        "QuestionText": "A test manager has to select test techniques to be used for testing autonomous vehicle software. There are a large number of environmental conditions (>50) that need to be considered for seven vehicle functions. Which of the following test techniques is MOST likely to be used when testing the variety of vehicle functions (VF) in different environmental conditions (EC)?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "A/B testing based on the VF and EC parameters",
          "b": "Combination testing of all the parameters of VF and EC",
          "c": "Pairwise testing of the relevant values of VF and EC",
          "d": "Back-to-back testing of relevant VF and EC values"
        },
        "CorrectAnswer": "c",
        "Explanation": "a) Is not correct. A/B testing is not useful for combinatorial testing.<br>b) Is not correct. All combinations would be almost impossible to do in practice, resulting from the near infinite number of potential combinations.<br><strong>c) Is correct.</strong> Pairwise testing is best suited to reduce the number of combinations without sacrificing defect detection too much.<br>d) Is not correct. Back-to-back testing is not useful for combinatorial testing."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.3.1",
    "LearningObjectiveDescription": "Explain how back-to-back testing is used for AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q41",
        "QuestionText": "A test manager decides to have a non-AI system with similar functionality to the AI based system under test (SUT) built to support system testing. Which of the following statements is most likely to be CORRECT?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "The test manager has chosen back-to-back testing because it helps solve the test oracle problem by using a pseudo-oracle",
          "b": "The test manager has chosen A/B testing because it helps solve the test oracle problem by using a pseudo-oracle",
          "c": "The test manager has chosen back-to-back testing because the non-functional requirements of the SUT can be verified against the pseudo-oracle",
          "d": "The test manager has chosen A/B testing because the non-functional requirements of the SUT can be verified against the pseudo-oracle"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>a) Is correct.</strong> It is an example of back-to-back testing where the non-AI system is used as a pseudo-oracle.<br>b) Is not correct. With A/B testing, we use a variant of the SUT to compare with the SUT.<br>c) Is not correct. The resources and non-functional characteristics of the pseudo-oracle and the SUT are likely to be different, hence the alternate system cannot be used for non-functional testing.<br>d) Is not correct. It is an example of back-to-back testing and also the resources and non-functional characteristics of the pseudo-oracle and the SUT are likely to be different."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.5.1",
    "LearningObjectiveDescription": "Apply metamorphic testing for the testing of AI-based systems.",
    "KnowledgeType": "K3",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q42",
        "QuestionText": "An AI-based mobile phone search system provides a list of phones that it believes are most suitable for the user based on its knowledge of the user’s previous mobile phone usage and their specified preferences. Given that metamorphic testing is being used with the following source test case: Inputs Outputs Selected price range: $200-$300 Recommended Phones: 3D camera: Don’t care SnapHappy_X1 Screen size: mid to large SnapHappy_M2 OS: Android or iOS SnapHappy_M3 Battery Life: Don’t care ClickNow_1000x ClickNow_1000xs",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "T1: SnapHappy_X1, SnapHappy_M2; T2: ClickNow_1000x, ClickNow_1000xs",
          "b": "T1: SnapHappy_M2, SnapHappy_M3, ClickNow_1000xs; T2: SnapHappy_X1, ClickNow_1000x",
          "c": "T1: SnapHappy_X1, SnapHappy_M2, SnapHappy_M3, ClickNow_1000x, ClickNow_1000xs; T2: SnapHappy_X1, SnapHappy_M2, SnapHappy_M3",
          "d": "T1: SnapHappy_X1, SnapHappy_M2, SnapHappy_M3, ClickNow_1000x, ClickNow_1000xs; T2: SnapHappy_X1, SnapHappy_M2, SnapHappy_M3, ClickNow_1000x, ClickNow_1000xs"
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>Follow-up test case T1</strong> differs from the source test case by the change in requirements for a 3D camera; it is now more specific. A 3D camera must be included. So, that means the follow-up expected results can only include the original test results at most (the previously recommended phones with a 3D camera).<br><strong>Follow-up test case T2</strong> also differs from the source test case by the change in requirements for a 3D camera; it is also more specific. No 3D camera should be included. So, that means the follow-up expected results can only include the original test results at most (the previously recommended phones without a 3D camera).<br>As T1 lists phones with a 3D camera, the remaining phones from the source test case must be those with no 3D camera – and so they should be in T2.<br>Therefore, T1 and T2 combined should contain all the cameras from the source test case, but with no overlap between the two.<br>a) Is not correct.<br><strong>b) Is correct.</strong><br>c) Is not correct.<br>d) Is not correct."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.7.1",
    "LearningObjectiveDescription": "For a given scenario, select appropriate test techniques when testing an AI-based system.",
    "KnowledgeType": "K4",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q43",
        "QuestionText": "LAIgal systems has an AI-based product for extracting relevant favorable judgements similar to a given legal case. This product is used by judges in the courts. Details of the current case are provided, and the system produces relevant judgements. The system needs to be safe from malicious inputs. A similar open-source product exists and is available. Not having a suitable test oracle is a challenge when testing. Which of the following test techniques should be selected to test the new version during system testing?",
        "AnswerOption": "Select 2 options. (2 out of 5)",
        "Answers": {
          "a": "A/B testing",
          "b": "Back-to-back testing",
          "c": "Adversarial testing",
          "d": "State transition testing",
          "e": "ML functional performance metrics calculation"
        },
        "CorrectAnswer": [
          "b",
          "c"
        ],
        "Explanation": "a) Is not correct. A/B testing is most useful when comparing two variants for the purpose of deciding if the new variant is an improvement over the older variant.<br><strong>b) Is correct.</strong> Back-to-back testing uses a similar product as a pseudo-oracle for testing.<br><strong>c) Is correct.</strong> Adversarial testing is important here as it being used for very important purpose and adversarial data can cause harm.<br>d) Is not correct. While state transition testing might be useful, nothing in the scenario suggests it; therefore, it is not the most relevant technique.<br>e) Is not correct. This testing is appropriate at the model testing stage for classification problems. It is not appropriate at the system testing stage for non-classification problems."
      }
    ]
  },
  {
    "LearningObjective": "AI-10.1.1",
    "LearningObjectiveDescription": "Describe the main factors that differentiate the test environments for AI-based systems from those required for conventional systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q44",
        "QuestionText": "Which one of the following statements is an example of a difference between a test environment for AI-based systems and a test environment for conventional systems?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Test environments for AI-based systems may require some mechanism to determine how a particular decision is made",
          "b": "Test environments for AI-based systems need simulators and virtual environments whereas conventional systems do not need these",
          "c": "Test environments for AI-based systems need large amounts of data, whereas conventional systems do not need large amount of data",
          "d": "GPUs are required for test environments for AI-based systems whereas conventional systems do not need these"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>a) Is correct.</strong> Explainability mechanism may need to be provided for AI environments.<br>b) Is not correct. Simulators and virtual environments are often required for conventional systems.<br>c) Is not correct. Large amount of data may be required for conventional systems, as well.<br>d) Is not correct. GPUs may be required for many other systems as well, for example, games."
      }
    ]
  },
  {
    "LearningObjective": "AI-11.2.1",
    "LearningObjectiveDescription": "Explain how AI can assist in supporting the analysis of new defects.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q45",
        "QuestionText": "In which of the following situations would AI be MOST useful when categorizing new defects?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "A small number of defects requires categorization on a new application",
          "b": "A large number of defects is reported on a small application",
          "c": "Minimal data is provided in typical defect reports",
          "d": "A new development team needs to know the most appropriate developer to fix a defect"
        },
        "CorrectAnswer": "b",
        "Explanation": "a) Is not correct. Where a small number of defects requires categorization and there is no historical data, AI would not have training data to be used.Genetic algorithms and neural networks can be used for test generation, and even be combined. Clustering produces results applicable to test generation.<br><strong>b) Is correct.</strong> Where a large number of defects is reported on a small application there is most likely to be benefit and opportunity to identify duplicates.<br>c) Is not correct. Where minimal data is provided in the defect reports, the usefulness of the tool will be lower, as less data will be available to the algorithm.<br>d) Is not correct. For AI to recommend developers to fix defects it would need to be based on historical data. However, because a new development team is taking over, any recommendations would be inaccurate until historical data is available."
      }
    ]
  },
  {
    "LearningObjective": "AI-11.3.1",
    "LearningObjectiveDescription": "Explain how AI can assist in test case generation.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q46",
        "QuestionText": "Which of the following is an AI tool MOST likely to use as the basis for generating functional test cases?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "A test charter",
          "b": "A picture of the system as a flow chart",
          "c": "Web server logs",
          "d": "Crash reports"
        },
        "CorrectAnswer": "c",
        "Explanation": "a) Is not correct. A test charter provides a focus for exploratory testing, and it rarely leads to the generation of test cases, even if an AI-based tool could interpret it.<br>b) Is not correct. A flow chart could be used to generate tests, but it needs to be machine readable, rather than simply a picture.<br><strong>c) Is correct.</strong> Web server logs may reflect production use of the system and provide a way for AI to generate tests.<br>d) Is not correct. Crash reports are unlikely to be used as they would describe unexpected failures rather than the functions performed by the application."
      }
    ]
  },
  {
    "LearningObjective": "AI-11.5.1",
    "LearningObjectiveDescription": "Explain how AI can assist in defect prediction.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q47",
        "QuestionText": "Which of the following options CORRECTLY states how an AI-based tool can perform defect prediction?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Using natural language to ask developers where they predict defects will occur",
          "b": "By analyzing the causes of defects raised on a similar code base",
          "c": "By analyzing false positive def",
          "d": "Scanning code to identify defects using rule"
        },
        "CorrectAnswer": "b",
        "Explanation": "a) Is not correct. While natural language processing is an AI application, it is not used for defect prediction.<br><strong>b) Is correct.</strong> Defect prediction is performed by looking for correlations between code/process/people measures and defects on the same or a similar code base.<br>c) Is not correct. The goal of defect prediction is not to identify defects with a false positive result. To analyze them would have little value.<br>d) is not correct. Defect prediction does not involve scanning of code using rules. This is static analysis."
      }
    ]
  },
  {
    "LearningObjective": "AI-1.2.1",
    "LearningObjectiveDescription": "Distinguish between narrow AI, general AI, and super AI.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q48",
        "QuestionText": "Which of the following statements about AI is MOST likely to be **CORRECT?**",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "An autonomous robot that can act as a worker in a house, shop or office is an example of general AI",
          "b": "A robot exhibiting similar skill levels as a human is considered to have achieved singularity",
          "c": "AI-based systems that support a range of test management functions are considered to possess general AI",
          "d": "An AI-based system that cannot access the internet is said to exhibit narrow AI"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>a) Is correct.</strong> General AI exhibits intelligent behavior comparable to a human and an autonomous robot that has a wide enough range of skills to perform as a worker in quite different environments is likely to be general AI.<br>b) Is not correct. Singularity is that point when AI has surpassed humans – not when they are at similar levels.<br>c) Is not correct. Performing test management would be considered by most as a single specialized set of tasks and would be classed as narrow AI.<br>d) Is not correct. Narrow AI is being able to perform a single specialized task; it is not relevant whether the AI has access to the internet or not."
      }
    ]
  },
  {
    "LearningObjective": "AI-1.3.1",
    "LearningObjectiveDescription": "Differentiate between AI-based systems and conventional systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q49",
        "QuestionText": "Which of the following statements is MOST likely to be describing a conventional system (as opposed to an AI-based system)?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "This system assigns customers into groups, based on their historical buying patterns",
          "b": "This system controls the braking of the car dependent on its speed",
          "c": "This system taught itself to recognize different words by listening to recordings",
          "d": "This system detects anomalies from its experience of seeing anomalies in many X-rays"
        },
        "CorrectAnswer": "b",
        "Explanation": "a) is not correct. This system appears to be describing clustering of customers as would be performed by an unsupervised learning system; therefore, it is likely to be AI-based.<br><strong>b) Is correct.</strong> There is no evidence that this system implements AI.<br>c) Is not correct. This system appears to have programmed itself by listening to recordings; therefore, it is likely to be AI-based.<br>d) Is not correct. This system appears to be describing a form of supervised learning based on anomalies in X-rays; therefore, it is likely to be AI-based."
      }
    ]
  },
  {
    "LearningObjective": "AI-1.5.1",
    "LearningObjectiveDescription": "Identify popular AI development frameworks.",
    "KnowledgeType": "K1",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q50",
        "QuestionText": "Which of the following options is NOT a framework used to develop AI-based software?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "scikit-learn",
          "b": "CNTK",
          "c": "MxNet",
          "d": "EZPy-AI"
        },
        "CorrectAnswer": "d",
        "Explanation": "a) Is not correct. This is a free software machine learning library for the Python programming language.<br>b) Is not correct. This is the Microsoft Cognitive Toolkit (CNTK), an open-source deep-learning toolkit.<br>c) Is not correct. this is a deep-learning open-source framework used by Amazon for AWS.<br><strong>d) Is correct.</strong> There is currently no AI development framework with this name"
      }
    ]
  },
  {
    "LearningObjective": "AI-1.7.1",
    "LearningObjectiveDescription": "Explain the concept of AI as a Service (AIaaS).",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q51",
        "QuestionText": "Which of the following statements is MOST likely to be describing a system that includes the use of AI as a Service (AIaaS)?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "The image classifier identifies defects in the gyroscope casings produced by the company and was built using a transfer learning approach so that it is of high accuracy",
          "b": "The underwater AI-based vehicle steering system uses a third-party obstacle avoidance component based on decision trees and Bayesian optimization",
          "c": "The contract checker uses an exclusive algorithm for determining levels of legal liability, but the pricing part of the contract is separately checked by a generic contract pricing AI component",
          "d": "The car rental pricing system is built using AI to support a demand-based algorithm and is hosted in the cloud and made available to all of the company’s car rental offices"
        },
        "CorrectAnswer": "c",
        "Explanation": "a) Is not correct. The image classifier seems to be built for a highly specific problem area and is unlikely to be made widely available as AIaaS.<br>b) Is not correct. Although the obstacle avoidance system is provided by a third party, it is highly likely to be embedded within the vehicle and it also appears to be a specialist component, so it is unlikely to be AIaaS.<br><strong>c) Is correct.</strong> The exclusive algorithm cannot be AIaaS, however the pricing part is generic and could well be provided as AIaaS.<br>d) Is not correct. The described situation suggests that the system is made available across the web, but only internally to their own car rental offices, so it does not appear to be AIaaS."
      }
    ]
  },
  {
    "LearningObjective": "AI-1.9.1",
    "LearningObjectiveDescription": "TBD",
    "KnowledgeType": "TBD",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q52",
        "QuestionText": "Which of the following options is the MOST likely use of a regulatory standard/regulation for an AI-based system?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Use of ISO/PAS 21448 (SOTIF) for an unmanned autonomous submarine",
          "b": "Use of GDPR for a bank loan decision-making system",
          "c": "Use of ISO 26262 for a fully self-driving car",
          "d": "Use of GDPR for a drone collision-avoidance system"
        },
        "CorrectAnswer": "b",
        "Explanation": "a) Is not correct. SOTIF is for road vehicles, not submarines.<br><strong>b) Is correct.</strong> A bank loan decision-making system will work with personal data and this is covered by GDPR.<br>c) Is not correct. A fully self-driving car is likely to include non-deterministic systems, which are not allowed by ISO 26262.<br>d) Is not correct. Drone collision systems are unlikely to include any personal data, which is the focus of GDPR"
      }
    ]
  },
  {
    "LearningObjective": "AI-2.1.1",
    "LearningObjectiveDescription": "Explain the importance of flexibility and adaptability as characteristics of AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q53",
        "QuestionText": "Which of the following statements about flexibility and adaptability is MOST likely to be **CORRECT?**",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Adaptability is important in unsupervised learning as it allows the ML model to learn from data without labels",
          "b": "Flexibility is important in supervised learning as it allows the ML model to recognize meaning even when data is poorly labelled",
          "c": "Adaptability is important in reinforcement learning systems as such systems must adapt themselves to optimize their reward function",
          "d": "Flexibility is important in self-learning systems as it allows them to adapt themselves to unexpected changes in their environment"
        },
        "CorrectAnswer": "d",
        "Explanation": "a) Is not correct. In unsupervised learning the system is expected to learn from unlabeled data and adaptability as a characteristic is associated with a system being changed rather than changing itself.<br>b) Is not correct. Supervised learning is dependent on labelled data, but flexibility is not a characteristic associated with reading poorly labelled data.<br>c) Is not correct. Reinforcement learning systems are expected to optimize the reward function, and adaptability as a characteristic is associated with a system that can be changed rather than a system that changes itself.<br><strong>d) Is correct.</strong> Flexibility is a characteristic associated with the ability of systems to be used in contexts outside the original requirements."
      }
    ]
  },
  {
    "LearningObjective": "AI-2.3.1",
    "LearningObjectiveDescription": "Explain the importance of managing evolution for AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q54",
        "QuestionText": "Which of the following statements about the evolution of AI-based systems is **CORRECT?**",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Self-learning AI-based systems that continue to work in the same operational environment are not expected to change their behavior",
          "b": "Side effects are not a concern for AI-based systems that change themselves to cope with changes in their environment",
          "c": "AI-based systems must change themselves to cope with changes in system requirements during development",
          "d": "Self-learning systems that physically interact with people, need to be managed to ensure system changes are not dangerous"
        },
        "CorrectAnswer": "d",
        "Explanation": "a) Is not correct. Self-learning systems that work in the same operational environment could still optimize themselves and so change their behavior.<br>b) Is not correct. AI-based systems that change themselves to adapt to changes in their environment could still cause negative side-effects.<br>c) Is not correct. Evolution as a characteristic is not about system development, but about how an AI-based system changes after deployment in its operational environment.<br><strong>d) Is correct.</strong> If the self-learning system physically interacts with people, then any changes it makes to itself could potentially harm people."
      }
    ]
  },
  {
    "LearningObjective": "AI-2.5.1",
    "LearningObjectiveDescription": "Discuss the ethical principles that should be respected in the development, deployment and use of AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q55",
        "QuestionText": "Which of the following examples of an AI-based system is LEAST likely to require special attention with regard to ethical issues?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "A computer game that teaches children the benefits of democracy by challenging them to become president",
          "b": "An application that uses data available on social media to provide a trustworthiness rating for job applicants",
          "c": "An autonomous self-powered underwater marine mapping system to create a map of the seabed in international waters",
          "d": "A mobile app that monitors each employee’s daily exercise and rewards them with health insurance benefits"
        },
        "CorrectAnswer": "c",
        "Explanation": "a) Is not correct. This game is aimed at children, a vulnerable group, and may give them an unfavorable view of democracy.<br>b) Is not correct. The application may pick up private and/or biased information from social media, which adversely affects the job prospects of applicants.<br><strong>c) Is correct.</strong> The ethical principles of respect for human autonomy, prevention of harm, fairness and explicability are unlikely to be affected by an underwater marine mapping system.<br>d) Is not correct. Such an app has the potential to be unfair to vulnerable groups, such as those with disabilities and it may also create unwanted pressure on employees."
      }
    ]
  },
  {
    "LearningObjective": "AI-2.7.1",
    "LearningObjectiveDescription": "Explain how transparency, interpretability and explainability apply to AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q56",
        "QuestionText": "Which of the following statements about the transparency, interpretability and explainability for AI-based systems is MOST likely to be CORRECT?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "The search engine algorithm used for training students on search engine technology was selected as it was considered to be the most explainable",
          "b": "The loan system was considered transparent as for each loan application it was clear to users how it decided whether to approve agree to the loan or not",
          "c": "The doctors were happy with the level of interpretability of the rule-based oncology system as they could understand how the given rules were implemented in the system",
          "d": "The drone operators were happy with the transparency of the control system as they felt that the system responded correctly to their instructions"
        },
        "CorrectAnswer": "c",
        "Explanation": "a) Is not correct. Understanding the underlying technology is considered to be interpretability rather than explainability.<br>b) Is not correct. Understanding how the loan system made a decision is more likely to be explainability.<br><strong>c) Is correct.</strong> Understanding the technology underlying the oncology system is likely to be interpretability.<br>d) Is not correct. The system responding correctly is simply referring to functional correctness – not transparency, interpretability or explainability."
      }
    ]
  },
  {
    "LearningObjective": "AI-3.1.2",
    "LearningObjectiveDescription": "Describe clustering and association as part of unsupervised learning.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q57",
        "QuestionText": "Which of the following BEST describes the unsupervised approach to machine learning?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Data and labels are analyzed to group them into clusters",
          "b": "A system automatically learns by satisfying a fitness function",
          "c": "A system teaches itself to meet objectives based on rewards",
          "d": "Data are analyzed to identify patterns in the data"
        },
        "CorrectAnswer": "d",
        "Explanation": "a) Is not correct. With unsupervised machine learning there are no labels.<br>b) Is not correct. A reinforcement learning system uses a fitness function to drive its learning.<br>c) Is not correct. A self-learning reinforcement system uses rewards to teach itself.<br><strong>d) Is correct.</strong> With unsupervised machine learning there are no labels and patterns are determined from the data itself."
      }
    ]
  },
  {
    "LearningObjective": "AI-3.2.1",
    "LearningObjectiveDescription": "Summarize the workflow used to create an ML system.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q58",
        "QuestionText": "Given the following descriptions: I. Model performance is checked using validation data II. The origin of test data used to test the model is identified III. The tuned model is made ready for its target hardware IV. Test data are used to ensure the agreed ML functional performance criteria are met V. The model is created from source code VI. The critical data features are identified Which of the following options BEST matches the descriptions with the activities in the ML workflow?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "III - Build & Compile Model / II - Prepare the Data / IV - Evaluate the Model",
          "b": "V - Build & Compile Model / II - Prepare the Data / I - Evaluate the Model",
          "c": "V - Build & Compile Model / VI - Prepare the Data / IV - Evaluate the Model",
          "d": "III - Build & Compile Model / VI - Prepare the Data / I - Evaluate the Model"
        },
        "CorrectAnswer": "b",
        "Explanation": "Considering the given descriptions:I.Model performance is checked using validation data. This is carried out during model evaluation.II.The origin of the test data used to test the model is identified. This is carried out as part of data preparation.III.The tuned model is made ready for its target hardware. This is carried out as part of deployment.IV.Test data are used to ensure the agreed performance criteria are met. This is carried out as part of testing the model.V.The model is created from source code. This is carried out as part of the build and compile model activity.VI.The critical data features are identified. This is carried out as part of data preparation.<br>a) Is not correct.<br><strong>b) Is correct.</strong><br>c) Is not correct.<br>d) Is not correct."
      }
    ]
  },
  {
    "LearningObjective": "AI-3.4.1",
    "LearningObjectiveDescription": "Explain the factors involved in the selection of ML algorithms.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q59",
        "QuestionText": "Which of the following statements is LEAST likely to be used as a rationale for selecting an ML algorithm?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "The amount of memory available for training the translation system in the mobile device",
          "b": "The maximum time allowed for retraining the embedded health monitoring system",
          "c": "The number of measured characteristics used as the basis for a sports prediction system",
          "d": "The number of expected clusters of customer types for a retail marketing system"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>a) Is correct.</strong> It is unlikely that the ML algorithm will be trained on the mobile device.<br>b) Is not correct. If the health monitoring system is embedded and needs to be retrained, we would likely set a maximum duration for this as the patient must either attend a facility for it to happen or be unmonitored while it is being done remotely.<br>c) Is not correct. The number of features expected to be used by the model will affect the choice of model.<br>d) Is not correct. Knowing the number of classes for clustering is a useful input when choosing the ML model."
      }
    ]
  },
  {
    "LearningObjective": "AI-4.2.1",
    "LearningObjectiveDescription": "Contrast the use of training, validation and test datasets in the development of an ML model.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q60",
        "QuestionText": "Which of the following statements about the test dataset is **CORRECT?**",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "The test dataset comes from a source totally different from the validation dataset",
          "b": "The format of the test dataset is different from the format of the validation dataset",
          "c": "The test dataset can be used as the validation dataset but not as the training dataset",
          "d": "The test dataset should not be exposed to the model during the training process"
        },
        "CorrectAnswer": "d",
        "Explanation": "a) Is not correct. The test dataset typically comes from the same source as the validation dataset.<br>b) Is not correct. The test dataset and the validation dataset should both have the same format.<br>c) Is not correct. The test dataset is used for neither training nor for validation.<br><strong>d) Is correct.</strong> Incorporation of the test dataset in training will lead to bias in the evaluation of model."
      }
    ]
  },
  {
    "LearningObjective": "AI-4.5.2",
    "LearningObjectiveDescription": "Recall reasons for the data in datasets being mislabelled.",
    "KnowledgeType": "K1",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q61",
        "QuestionText": "Which of the following options would MOST likely be a reason for poor labeling of data?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Insufficient data",
          "b": "Synthetic data",
          "c": "Translation errors",
          "d": "Algorithm chosen for the ML model"
        },
        "CorrectAnswer": "c",
        "Explanation": "a) Is not correct. Having insufficient data doesn't determine the likelihood of data mislabeling.<br>b) Is not correct. Synthetic data does not lead to poor labeling.<br><strong>c) Is correct.</strong> Translation errors may lead to correctly labeled data in one language being mislabeled in a second language<br>d) Is not correct. Data labeling is not related to the choice of ML algorithm."
      }
    ]
  },
  {
    "LearningObjective": "AI-5.3.1",
    "LearningObjectiveDescription": "Summarize the limitations of using ML functional performance metrics to determine the quality of the ML system.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q62",
        "QuestionText": "An ML team asserts that the ML functional performance metrics based on validation data collected as part of training an ML model are sufficient for determining the quality of the system. Which of the following statements is a valid reason to show that this may be **INCORRECT?**",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "The ML functional performance metrics may not work well if the ground truth is not correct",
          "b": "The ML functional performance metrics cannot be used for measuring quality as these are tool dependent",
          "c": "Validation data is biased resulting in skewed functional performance measurements",
          "d": "Data may need to be transformed prior to training the model, so the functional performance measurements do not reflect the quality of the model"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>a) Is correct.</strong> The model quality depends on labeling quality. Wrong labeling leads to wrong ground truth. For incorrectly labeled data the functional performance measurements may indicate a good quality model but it would be producing wrong outputs.<br>b) Is not correct. Values of the ML functional performance metrics are not dependent on the tool used to measure them.<br>c) Is not correct. Validation data may or may not have bias in it.<br>d) Is not correct. Data transformation is often performed and it does not necessarily impact the quality of the model. Wrong transformations may result in data quality issues and subsequent model quality issues, but a general statement relating data transformation to poor model quality cannot be made."
      }
    ]
  },
  {
    "LearningObjective": "AI-5.5.1",
    "LearningObjectiveDescription": "Explain the use of benchmark suites in the context of ML.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q63",
        "QuestionText": "Which of the following options regarding benchmark suites, BEST completes the following statement?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "ML benchmark suites help choose a particular model by indicating the time it takes to train",
          "b": "ML benchmark suites help choose a particular model by indicating the time it takes to test",
          "c": "ML benchmark suites help choose a particular model by indicating the time it takes to validate",
          "d": "ML benchmark suites help choose a particular model by indicating the time it takes to deploy"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>a) Is correct.</strong> Benchmark suites indicate the training time.<br>b) Is not correct. Benchmark suites do not indicate the test time.<br>c) Is not correct. Benchmark suites do not indicate the validation time.<br>d) Is not correct. Benchmark suites do not indicate the deployment time."
      }
    ]
  },
  {
    "LearningObjective": "AI-8.3.1",
    "LearningObjectiveDescription": "Explain how to test for bias in an AI-based system.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q64",
        "QuestionText": "Which of the following test levels provides the BEST choice for performing **bias-related testing?**",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Component testing",
          "b": "Input data testing",
          "c": "System testing",
          "d": "Model testing"
        },
        "CorrectAnswer": "b",
        "Explanation": "a) Is not correct. Component testing is applied to non-model components and is conventional testing.<br><strong>b) Is correct.</strong> Input data testing is performed for testing bias, among other things.<br>c) Is not correct. While independent bias testing can be also done as part of system testing, ideally it should be done before training the model.<br>d) Is not correct. Model testing is used to check that the model alone meets any specified requirements, e.g., ML functional performance criteria and non-functional criteria."
      }
    ]
  },
  {
    "LearningObjective": "AI-7.5.1",
    "LearningObjectiveDescription": "Describe the documentation of an AI component and understand how documentation supports the testing of AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q65",
        "QuestionText": "Which of the following statements about the documentation of AI components is **CORRECT?**",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Because non-functional requirements are not a part of the documentation of an AI component, non-functional testing cannot be performed",
          "b": "White-box testing of the interaction of AI and non-AI components is not possible if the interfaces are a part of the documentation",
          "c": "Checking for bias in the data is made possible by including the source of the data in the documentation",
          "d": "Self-adapting AI systems require each change made by the system to be fully documented"
        },
        "CorrectAnswer": "c",
        "Explanation": "a) Is not correct because non-functional requirements are a part of the documentation.<br>b) Is not correct because the documentation of interfaces supports the white-box testing of component interactions.<br><strong>c) Is correct</strong> because bias testing on the data on the basis of its source and meta-data is possible.<br>d) Is not correct because self-adapting AI systems rarely provide the documentation of the changes they make to themselves."
      }
    ]
  },
  {
    "LearningObjective": "AI-7.6.1",
    "LearningObjectiveDescription": "Explain the need for frequently testing the trained model to handle concept drift.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q66",
        "QuestionText": "An e-commerce application recommends products to the user on the basis of their purchase history and purchases being made by other people using the site, among other factors. As the tester, you have been asked to measure the current conversion rate of recommendations to compare with the original required conversion rate. Which of the following is the MOST likely underlying reason for this request?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "AI effect",
          "b": "Adversarial attacks",
          "c": "Concept drift",
          "d": "Lack of fairness"
        },
        "CorrectAnswer": "c",
        "Explanation": "a) Is not correct as AI effect refers to change in the perception of what is AI over time and not the accuracy of a given solution.<br>b) Is not correct. An adversarial attack is where an attacker subtly perturbs valid inputs that are passed to the trained model to cause it to provide incorrect predictions. In this case the data is being gathered from the purchase history and not direct inputs from the users. Hence the chances of this being an adversarial attack are low.<br><strong>c) Is correct.</strong> This is an example of a model providing reduced accuracy because of changes in customer behavior.<br>d) Is not correct. Fairness is using positively biased data for training which is not true in this case as we are dealing with a live system which was performing well in the past."
      }
    ]
  },
  {
    "LearningObjective": "AI-8.2.1",
    "LearningObjectiveDescription": "Describe how autonomous AI-based systems are tested.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q67",
        "QuestionText": "Which of the following options is MOST likely to be relevant when testing a system’s autonomy?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Testing over a sustained period of time",
          "b": "Testing the accuracy of system predictions",
          "c": "Testing how quickly the system can adapt",
          "d": "Static analysis of training data"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>a) Is correct.</strong> This is relevant to testing a system's autonomy because it may be required to test over a specific period of time to check how often the system requires intervention.<br>b) Is not correct. A system can be autonomous and very inaccurate.<br>c) Is not correct. A system can be autonomous and not adaptable.<br>d) Is not correct. Static analysis of training data is unlikely to be related to testing for autonomy."
      }
    ]
  },
  {
    "LearningObjective": "AI-8.4.1",
    "LearningObjectiveDescription": "Explain the challenges in testing created by the probabilistic and non-deterministic nature of AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q68",
        "QuestionText": "Which of the following statements demonstrates how non-deterministic systems can create challenges in testing?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Non-deterministic systems produce a different result each time, normally preventing the generation of expected results",
          "b": "Non-deterministic systems are difficult to test because they are not explainable, which hinders the investigation and fixing of defects",
          "c": "A system that is given the same inputs and initial state may produce different outputs, so can require multiple test executions",
          "d": "Non-deterministic systems are usually biased and require additional tests to allow this bias to be excluded from the results"
        },
        "CorrectAnswer": "c",
        "Explanation": "a) Is not correct. It is normally possible to specify expected results, but this may require the inclusion of a tolerance or making multiple test runs.<br>b) Is not correct. Non-deterministic systems do not necessarily have explainability problems.<br><strong>c) Is correct.</strong> This is a valid description of a non-deterministic system, and, as a result tests may need to be run several times to give a statistically valid test result.<br>d) Is not correct. There is no correlation between bias and non-deterministic systems."
      }
    ]
  },
  {
    "LearningObjective": "AI-2.7.1",
    "LearningObjectiveDescription": "Explain how transparency, interpretability and explainability apply to AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q69",
        "QuestionText": "When testing a deep neural network, which of the following characteristics can be evaluated without using dynamic testing?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Explainability",
          "b": "Transparency",
          "c": "Automation bias",
          "d": "Accuracy"
        },
        "CorrectAnswer": "b",
        "Explanation": "a) Is not correct. Methods for testing explainability of deep neural networks, which are complex to understand, require execution of the system.<br><strong>b) Is correct.</strong> Testing transparency does not require execution of the system under test, as it only requires comparing the documentation to the implementation.<br>c) Is not correct. Automation bias requires execution to provide results, and determine how they are processed by users.<br>d) Is not correct. Accuracy cannot be verified without executing the system under test."
      }
    ]
  },
  {
    "LearningObjective": "AI-8.7.1",
    "LearningObjectiveDescription": "Explain the challenges in creating test oracles resulting from the specific characteristics of AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q70",
        "QuestionText": "Which of the following characteristics of an example AI-based system might cause a test oracle problem?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "It is not known where the training data was obtained",
          "b": "The output of the system is a prediction for which the ground truth is unknown",
          "c": "The system runs with no human intervention and is considered to be autonomous",
          "d": "There is a lack of transparency into how the system was implemented"
        },
        "CorrectAnswer": "b",
        "Explanation": "a) Is not correct. Data does not cause a test oracle problem because it does not mean it is difficult to determine the expected result.<br><strong>b) Is correct.</strong> If the ground truth of a predicted system is unknown, that is likely to cause a test oracle problem.<br>c) Is not correct. Autonomy is unrelated to test oracles.<br>d) Is not correct. Knowing how the system was implemented is not generally required to determine the expected result."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.4.1",
    "LearningObjectiveDescription": "Explain how A/B testing is applied to the testing of AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q71",
        "QuestionText": "Which of the following statements is CORRECT with regards to A/B testing?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "A/B testing is also known as differential testing because two different programs are used for this type of testing",
          "b": "A/B testing is mostly useful for testing simple ML models as it does not produce accurate results for complex models",
          "c": "A/B testing requires multiple expected results from the same inputs to be compared to identify significant differences in tested models",
          "d": "A/B testing is a good technique for writing test cases for various types of ML models, especially self-learning systems"
        },
        "CorrectAnswer": "c",
        "Explanation": "a) Is not correct. Back-to-back testing, and not A/B testing, is known as differential testing.<br>b) Is not correct. A/B testing can be used for both simple and complex ML models.<br><strong>c) Is correct.</strong> A/B testing requires well-defined acceptance criteria to choose between the two models being tested.<br>d) Is not correct. A/B testing doesn't help in writing test cases."
      }
    ]
  },
  {
    "LearningObjective": "AI-10.2.1",
    "LearningObjectiveDescription": "Describe the benefits provided by virtual test environments in the testing of AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q72",
        "QuestionText": "Which of the following statements is LEAST likely to be an example of a benefit provided by virtual test environments in the testing of an AI-based system?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "A smart city application is tested in a virtual test environment to allow scenarios that very rarely happen (e.g., crowd control at a new year event) to be tested",
          "b": "An AI-based money market trading system is tested in a virtual test environment as it would be impractical to test on real money markets due to the potential costs",
          "c": "A smart fruit picker is tested in a virtual test environment to allow many picking scenarios to be run in an accelerated timeframe to check that fruit is not damaged",
          "d": "An autonomous car is tested in a virtual test environment to allow potentially dangerous test scenarios to be run safely"
        },
        "CorrectAnswer": "c",
        "Explanation": "a) Is not correct. Running rare scenarios in a virtual test environment is useful, especially when the cost of running the tests in the real world would be prohibitive.<br>b) Is not correct. Running tests for a money market trading system on a virtual test environment is sensible as we could not run these tests on the real market due to the potential for losing money.<br><strong>c) Is correct.</strong> Running a fruit picker faster in a virtual environment will not help to determine if fruit is undamaged in real-time production because checking damage to fruit in the virtual environment would be far more difficult than checking on damage to real fruit in a real (non-virtual) environment.<br>d) Is not correct. It is good practice to run dangerous test scenarios for an autonomous car in a virtual test environment on safety grounds."
      }
    ]
  },
  {
    "LearningObjective": "AI-11.1.1",
    "LearningObjectiveDescription": "Categorize the artificial intelligence technologies used in software testing.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q73",
        "QuestionText": "It is possible to use an AI-assisted test automation tool to recognize objects through image processing, rather than using references to their location. To which type of AI software engineering technology is this example MOST likely referring?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Classification, Learning and Prediction",
          "b": "Probabilistic Software Engineering",
          "c": "Search-based Software Engineering",
          "d": "Clustering"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>a) Is correct.</strong> Image recognition in the form of classification models is used to recognize objects in GUIs.<br>b) Is not correct. Probabilistic software engineering can be used for estimating the probability of certain things occurring.<br>c) Is not correct. Search-based software engineering techniques are used for reducing a large problem space. These may be useful for guessing where the image objects are located, but not recognizing them as images.<br>d) Is not correct. Clustering is a type of algorithm, not a type of AI software engineering technology."
      }
    ]
  },
  {
    "LearningObjective": "AI-11.6.1",
    "LearningObjectiveDescription": "Explain the use of AI in testing user interfaces.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q74",
        "QuestionText": "Which of the following statements is CORRECT with respect to visual testing?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Visual testing makes use of object IDs to determine changes",
          "b": "Visual testing uses images to do pixel-by-pixel comparisons",
          "c": "Visual testing helps find overlapping user interface elements",
          "d": "Visual testing fails when the screen layout changes"
        },
        "CorrectAnswer": "c",
        "Explanation": "a) Is not correct. Visual testing makes use of images and does not use object IDs.<br>b) Is not correct. Visual testing makes use of images but does not perform pixel by pixel comparison. It uses AI to do the comparison.<br><strong>c) Is correct.</strong> Visual Testing can find overlapping user interface elements similar to a human tester.<br>d) Is not correct. Visual testing works even when the layout changes"
      }
    ]
  },
  {
    "LearningObjective": "AI-11.5.1",
    "LearningObjectiveDescription": "Explain how AI can assist in defect prediction.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q75",
        "QuestionText": "Which of the following options accurately states how or for what purpose an AI-based testing tool can perform error prediction?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "By analyzing anomalies in source code metrics – such as the number of lines of code and cyclomatic complexity – errors can be predicted.",
          "b": "The prediction of error states, based on previous experiences with the same codebase or the same developers, should be done using an AI-based approach.",
          "c": "By analyzing false positive test results observed in the past, errors can be predicted.",
          "d": "For prioritizing the testing of components that contain more error states than others, even immature testing tools are suitable if they are AI-based."
        },
        "CorrectAnswer": "b",
        "Explanation": "a) INCORRECT – Source code metrics such as the number of lines of code and cyclomatic complexity are not the best predictors for forecasting errors (see Chap. 11.5, 3rd paragraph, 2nd sentence).<br><strong>b) CORRECT</strong> – Error prediction is performed by the AI-based search for correlations between code/process/personnel metrics and errors in the same codebase or with the same developers (see Chap. 11.5, 2nd paragraph).<br>c) INCORRECT – The goal of error prediction is not to identify errors with a false-positive result, but rather how many error states are present at all and whether they can be found (see Chap. 11.5, 1st paragraph).<br>d) INCORRECT – 'The results of error prediction are typically used to set priorities for testing (e.g., more tests for components for which more error states are predicted).' 'This capability depends on the maturity of the tool used.' (see Chap. 11.5, 1st paragraph, 3rd and 2nd sentence)."
      }
    ]
  },
  {
    "LearningObjective": "AI-11.4.1",
    "LearningObjectiveDescription": "Explain how AI can assist in optimization of regression test suites.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q76",
        "QuestionText": "Which of the following options correctly states how an AI-based tool can perform optimization of regression test suites?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "By analyzing false positive test results",
          "b": "By analyzing information from previous test activities",
          "c": "By using genetic algorithms to create new test cases",
          "d": "By updating expected results to counteract concept drift"
        },
        "CorrectAnswer": "b",
        "Explanation": "a) INCORRECT – The goal of regression test optimization is to reduce the scope of a test suite, set priorities, or expand it (see Chap. 11.4, 1st paragraph), not to reduce the number of false alarms.<br><strong>b) CORRECT</strong> – The optimization of regression test suites is performed by analyzing information about previous test executions (see Chap. 11.4, 2nd paragraph). <br>c) INCORRECT – According to Section 11.4 of the curriculum, regression test optimization is typically performed using data from previous test executions (see Chap. 11.4, 2nd paragraph). Using genetic algorithms to create new tests does not optimize the regression test suite.<br>d) INCORRECT – Concept drift refers to changes in the software's operational environment (see Chap. 7.6). If the software is modified as a result, additional regression test cases are needed. However, this does not lead to optimization of the regression test suite."
      }
    ]
  },
  {
    "LearningObjective": "AI-11.3.1",
    "LearningObjectiveDescription": "Explain how AI can assist in test case generation.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q77",
        "QuestionText": "Which of the following statements about AI-based test case generation is correct?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "The basis for AI-based test case creation is the source code, but not a machine-readable test model.",
          "b": "The test oracle problem in AI-based test case creation can be solved through comparative testing.",
          "c": "For AI-generated functional test cases, predictable results are always available.",
          "d": "Research shows that AI-based test case generation tools cannot achieve coverage levels equivalent to fuzz testing tools."
        },
        "CorrectAnswer": "b",
        "Explanation": "a) INCORRECT – The basis for AI-based test case creation is both the source code and a machine-readable test model (see Chap. 11.3, 1st paragraph, 2nd sentence).<br><strong>b) CORRECT</strong> – The test oracle problem in AI-based test case creation is solved through comparative testing when a suitable system is available that can be used as a pseudo-oracle (see Chap. 11.3, 2nd paragraph, 2nd sentence).<br>c) INCORRECT – 'However, if no test model defining the required behaviors is used as basis for the test cases, ... test case generation generally suffers from a test oracle problem, as the AI-based tool doesn't know what results to expect for a particular set of test data.' (Chap. 11.3, 2nd paragraph, 1st sentence).<br>d) INCORRECT – Research comparing AI-based test case generation tools with similar fuzz testing tools shows that the AI-based tools achieve equivalent coverage levels (see Chap. 11.3, 3rd paragraph, 1st sentence)."
      }
    ]
  },
  {
    "LearningObjective": "AI-11.2.1",
    "LearningObjectiveDescription": "Explain how AI can assist in supporting the analysis of new defects.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q78",
        "QuestionText": "Which of the following statements regarding the use of AI for analyzing new errors is MOST accurate?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "A small number of error states in a new application can be well categorized using a suitable ML approach.",
          "b": "If a large number of error states are reported for a small application, the bug-fixing time can be optimized through error triage.",
          "c": "AI-based error categorization is not useful for automated error reporting systems and large projects.",
          "d": "For a new development team, ML models can suggest which developer is best suited to fix certain errors."
        },
        "CorrectAnswer": "b",
        "Explanation": "a) INCORRECT – If a small number of errors need to be categorized and no historical data is available, an ML approach cannot use training data (see Chap. 3.3, 1st bullet point).<br>b) CORRECT – If a large number of error states are reported for a small application, there is most likely a benefit from the ability to identify duplicates and thus optimize the bug-fixing time (see Chap. 11.2, 1st paragraph and 1st bullet point, 'Error Triage').<br>c) INCORRECT – AI-based error categorization is particularly useful for automated error reporting systems and large projects (see Chap. 11.2, 1st bullet point, last sentence).<br>d)INCORRECT – For ML models to recommend which developer is best suited to fix certain errors, they would need to have previous/historical data about the contents of error reports and developer assignments available (see Chap. 11.2, last bullet point 'Assignment')"
      }
    ]
  },
  {
    "LearningObjective": "AI-9.6.1",
    "LearningObjectiveDescription": "Explain how experience-based testing can be applied to the testing of AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q79",
        "QuestionText": "Which of the following statements regarding AI-based systems is a correct statement about experience-based testing?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "In exploratory testing, training data is visualized using tools to examine different aspects of the data.",
          "b": "In intuitive test case identification, existing test cases are dynamically adapted, for example based on metamorphic testing.",
          "c": "For exploratory testing, the \"ML Test Checklist\" from Google is used, among other things.",
          "d": "Experience-based testing requires the calculation of functional ML performance metrics."
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>a) CORRECT</strong> – This refers to exploratory data analysis, which is an exploratory method (see Chap. 9.6, 5th paragraph, last two sentences).<br>b) INCORRECT – This is an approach in exploratory testing (see Chap. 9.6, 3rd paragraph). In contrast, intuitive test case identification is based on testers' knowledge of typical developer errors and failure effects in similar systems (see Chap. 9.6, 2nd paragraph).<br>c) INCORRECT – This is a checklist-based review (see Chap. 9.6, first and sixth paragraphs). In exploratory testing, checklists are not used (see Chap. 9.6, 3rd paragraph).<br>d) INCORRECT – Calculating functional ML performance metrics (see Chap. 5) is important for assessing the quality of an AI system but has nothing to do with experience-based testing (see Chap. 9.6)."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.5.1",
    "LearningObjectiveDescription": "Apply metamorphic testing for the testing of AI-based systems.",
    "KnowledgeType": "K3",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q80",
        "QuestionText": "An AI-supported mobile phone recommendation system provides a list of mobile phones based on its knowledge of the user's specified preferences, i.e., the largest possible display, the largest possible storage, and long battery life. The system indicates an acceptable price for each combination of these features. For testing the mobile phone recommendation system, three initial test cases (T1 to T3) are used. Additionally, new test cases A to D are proposed. Test Case Display Diagonal Storage Size Battery Life Acceptable Price T1 4 inches 8 GB 36 hours =250€ T3 5 inches 8 GB 16 hours >=220€",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "A, B are follow-up test cases; C, D are not follow-up test cases",
          "b": "A, C are follow-up test cases; B, D are not follow-up test cases",
          "c": "A, B, C are follow-up test cases; D is not a follow-up test case",
          "d": "B, C, D are follow-up test cases; A is not a follow-up test case"
        },
        "CorrectAnswer": "c",
        "Explanation": "The correct answer is c) A, B, C are follow-up test cases; D is not a follow-up test case.<br>The metamorphic relations are based on the principle that increasing input values (display diagonal, storage size, battery life) indicates better features and should increase the acceptable price.<br>•\\t<strong>Test A</strong> is a follow-up test case derived from T1. Compared to T1, the battery life is lower (24 hours vs. 36 hours), while other values are the same. Therefore, the acceptable price should be at most as high as in T1 (<=200 €), which is consistent.<br>•\\t<strong>Test B</strong> is a follow-up test case derived from T2. Compared to T2, the battery life is higher (30 hours vs. 24 hours), while other values are the same. Therefore, the acceptable price should be at least as high as in T2 (>=250 €), which is consistent.<br>•\\t<strong>Test C</strong> is a follow-up test case derived from T3. Compared to T3, the battery life is higher (20 hours vs. 16 hours), while other values are the same. Therefore, the acceptable price should be at least as high as in T3 (>=220 €), which is consistent.<br>•\\t<strong>Test D</strong> is not a follow-up test case. Compared to T3, the battery life is higher (24 hours vs. 16 hours), while other values are the same. Therefore, the acceptable price should be at least as high as in T3 (>=220 €), but Test D has a price of <=200 €, which is inconsistent.<br><br><strong>Thus, only tests A, B, and C</strong> are valid follow-up test cases for metamorphic testing."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.3.1",
    "LearningObjectiveDescription": "Explain how back-to-back testing is used for AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q81",
        "QuestionText": "A test manager decides to build a non-AI system with similar functionality to the AI-based system under test (SUT) to support system testing. Which of the following statements is **MOST CORRECT?**",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "The test manager has chosen back-to-back testing because it can solve the test oracle problem by using a pseudo-oracle",
          "b": "The test manager has chosen A/B testing because it can solve the test oracle problem by using a pseudo-oracle",
          "c": "The test manager has chosen back-to-back testing because the non-functional requirements of the SUT can be verified using the pseudo-oracle",
          "d": "The test manager has chosen A/B testing because the non-functional requirements of the SUT can be verified using the pseudo-oracle"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>a) CORRECT</strong> – This is an example of comparative testing where the non-AI system is used as a pseudo-oracle. See CT-AI Curriculum, Chapter 9.3, Paragraph 1: In comparative testing, an alternative version of the system is used as a pseudo-oracle.<br>b) INCORRECT – In A/B testing, we typically compare two variants of the same system. See CT-AI Curriculum, Chapter 9.4, last paragraph: <strong>'In A/B testing, two variants of the same system are usually compared.'</strong> Here we have two systems with similar functionality but fundamentally different architectures (AI-based vs. non-AI).<br>c) INCORRECT – The resources and non-functional characteristics of the pseudo-oracle and the SUT are likely different, so the alternative system cannot be used for non-functional testing. In comparative testing, the performance of the two systems may vary significantly. See CT-AI Curriculum, Chapter 9.3, Paragraph 1, last sentence: <strong>'For example, it doesn't have to execute as quickly...'</strong><br>d) INCORRECT – Same reasoning as option b). A/B testing is not appropriate here as we're comparing two fundamentally different systems rather than variants of the same system."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.2.1",
    "LearningObjectiveDescription": "Explain how pairwise testing is used for AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q82",
        "QuestionText": "For which of the following situations is pairwise testing MOST suitable?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "With a high number of software testers for AI-based systems",
          "b": "With a high number of system components of the AI-based system",
          "c": "With a high number of test cases for the AI-based system",
          "d": "With a very high number of parameters for the AI-based system"
        },
        "CorrectAnswer": "d",
        "Explanation": "a) INCORRECT – Pairwise testing refers to a high number of parameters that are of interest for the AI-based system. See CT-AI Curriculum, Chapter 9.2, Paragraphs 1 and 3. The number of testers is irrelevant here. Although pair programming might involve multiple developers/testers working at the same computer, this is rarely due to having too many testers.<br>b) INCORRECT – Pairwise testing refers to a high number of parameters that are of interest for the AI-based system. See CT-AI Curriculum, Chapter 9.2, Paragraphs 1 and 3. The number of system components is irrelevant here.<br>c) INCORRECT – Pairwise testing refers to a high number of parameters that are of interest for the AI-based system. See CT-AI Curriculum, Chapter 9.2, Paragraphs 1 and 3. A high number of test cases is irrelevant here.<br><strong>d) CORRECT</strong> – See CT-AI Curriculum, Chapter 9.2, Paragraphs 1 and 3. Pairwise testing specifically addresses situations with a high number of parameters."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.1.1",
    "LearningObjectiveDescription": "Explain how the testing of ML systems can help prevent adversarial attacks and data poisoning.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q83",
        "QuestionText": "Which of the following options describes a valid relationship between testing ML systems and preventing adversarial attacks or data contamination?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Adversarial testing consists of finding and fixing vulnerabilities by performing adversarial attacks.",
          "b": "Comparative testing can be used to find deviations between the previous version of the system and the new version.",
          "c": "After identifying adversarial examples, it must be prevented that these can come into contact with the system, e.g., through a firewall",
          "d": "Since AI-based systems continuously learn and adapt their behavior, the use of regression testing is not meaningful."
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>a) CORRECT</strong> – See CT-AI Curriculum, Chapter 9.1.1, last paragraph.<br>b) INCORRECT – Comparative testing is used when an alternative version of the system is to be used as a pseudo-oracle (possibly already existing or developed by another team). See CT-AI Curriculum, Chapter 9.3, Paragraph 1. Here, however, versions of the same system are to be compared. So this refers to A/B testing. See CT-AI Curriculum, Chapter 9.1.2, last paragraph.<br>c) INCORRECT – The identified examples are added to the training data so that the system can learn to handle them. See Chapter 9.1.1, last paragraph.<br>d) INCORRECT – With a sufficiently trustworthy test suite, it can be determined whether a system has been contaminated. Compare CT-AI Curriculum, Chapter 9.1.2, last paragraph, last sentence"
      }
    ]
  },
  {
    "LearningObjective": "AI-8.5.1",
    "LearningObjectiveDescription": "Explain the challenges in testing created by the complexity of AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q84",
        "QuestionText": "Which of the following options describes a challenge in testing complex, AI-based systems?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "The test oracle problem in this context states that AI-based systems often find worse test oracles than human testers.",
          "b": "Due to the complexity of behavior, only white-box testing methods are meaningful, but no longer black-box testing methods.",
          "c": "The complexity of AI-based systems and their tests increases when these systems operate probabilistically and are non-deterministic.",
          "d": "The complexity of AI-based systems and their tests does not increase further just because the systems consist of multiple interacting components."
        },
        "CorrectAnswer": "c",
        "Explanation": "a) INCORRECT – The test oracle problem consists of the behavior of such systems being too complex for humans to understand in the necessary depth to derive a test oracle from it. See CT-AI Curriculum, Chapter 8.5, Paragraph 1.<br>b) INCORRECT – The opposite is the case. Since the structure of such systems is often automatically generated, it is too complex to derive meaningful tests for white-box coverage from it. Therefore, often only black-box tests are meaningful. See CT-AI Curriculum, Chapter 8.5, Paragraph 2.<br>c) CORRECT – Probabilistic results and non-determinism exacerbate the complexity of the systems and thus also the testing for them. See CT-AI Curriculum, Chapter 8.5, Paragraph 3.<br>d) INCORRECT – See CT-AI Curriculum, Chapter 8.5, Paragraph 4: 'The problems with non-deterministic systems are further exacerbated when an AI-based system consists of multiple interacting components...'"
      }
    ]
  },
  {
    "LearningObjective": "AI-8.3.1",
    "LearningObjectiveDescription": "Explain how to test for bias in an AI-based system.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q85",
        "QuestionText": "Which of the following options describes a valid test for bias?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Review of the source of training data and data collection procedures to identify algorithmic biases",
          "b": "Measurement of the influence of system inputs on outputs with focus on individuals for or against whom the system is inappropriately biased",
          "c": "Acquisition of additional information about input data to examine data preprocessing for biases",
          "d": "Review of training or model optimization activities to identify sampling biases"
        },
        "CorrectAnswer": "b",
        "Explanation": "a) INCORRECT – Reviewing the source of training data and data collection procedures mainly identifies sampling biases. See CT-AI Curriculum, Chapter 8.3, Bullet Points 1 and 2.<br><strong>b) CORRECT</strong> – This option describes a valid test for bias. Compare CT-AI Curriculum, Chapter 8.3, Bullet Point 4.<br>c) INCORRECT – Acquiring additional information can be useful to find 'hidden' variables that are relevant for bias assessment but do not themselves represent model inputs (see CT-AI Curriculum, Chapter 8.3, Bullet Point 5). Reviewing data preprocessing can be useful (see CT-AI Curriculum, Chapter 8.3, Bullet Point 3). However, the 'hidden' variables from acquiring additional information cannot influence the currently implemented data preprocessing since they are not being processed. Therefore, this option is internally illogical.<br>d) INCORRECT – Reviewing training or model optimization activities primarily identifies algorithmic biases. See CT-AI Curriculum, Chapter 8.3, Bullet Points 1 and 2."
      }
    ]
  },
  {
    "LearningObjective": "AI-8.1.1",
    "LearningObjectiveDescription": "Explain the challenges in testing created by the self-learning of AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q86",
        "QuestionText": "Which of the following options describes a challenge for testing self-learning systems?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "The system behavior can change so significantly that initially designed test cases are no longer valid. This cannot be fixed even by particularly clever test design.",
          "b": "As part of system evolution, the acceptance criteria for system changes must also be developed in a self-learning manner.",
          "c": "Since the operational environment of the self-learning system can change, tests must be designed that cover system behavior without knowledge of possible operational environments.",
          "d": "The execution of tests can influence the behavior of the self-learning system. Depending on the test cases used, unwanted behavioral changes may occur."
        },
        "CorrectAnswer": "d",
        "Explanation": "a) INCORRECT – Designing test cases that remain valid even when system behavior changes is a testing challenge. Compare CT-AI Curriculum, Chapter 8.1, first bullet point.<br>b) INCORRECT – The acceptance criteria for changes to the self-learning system must be defined in advance. The acceptance of every system change is controlled by this. If these criteria also change over time, future system changes might not enforce positive changes but possibly negative ones. Compare CT-AI Curriculum, Chapter 8.1, second bullet point.<br>c) INCORRECT – The goal is not to design tests without knowledge of the operational environment. This would enable possible attacks through data contamination. Instead, the goal must be to cover all operational environments or specify acceptance criteria for all possible changes to the operational environment. Compare CT-AI Curriculum, Chapter 8.1, fifth bullet point.<br><strong>d) CORRECT</strong> – Unwanted behavioral changes are a challenge when testing self-learning systems. Compare CT-AI Curriculum, Chapter 8.1, seventh/last bullet point."
      }
    ]
  },
  {
    "LearningObjective": "AI-7.4.1",
    "LearningObjectiveDescription": "Explain automation bias and how this affects testing.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q87",
        "QuestionText": "Which of the following statements regarding automation bias is correct?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "In automation bias, the human checks the system's recommendations against inputs from other sources.",
          "b": "The attention of a human vehicle occupant has nothing to do with automation bias.",
          "c": "Due to automation bias, the quality of human inputs must be tested, but not the quality of system recommendations",
          "d": "Human decisions can be of lower quality when recommended by an AI-based system."
        },
        "CorrectAnswer": "d",
        "Explanation": "a) INCORRECT – 'One form of automation bias consists of the human accepting the system's recommendations and not considering inputs from other sources...' (see Chapter 7.4, first bullet point, first sentence).<br>b) INCORRECT – The attention of a human vehicle occupant is indeed related to automation bias, because according to the curriculum, Chapter 7.4, second bullet point, 3rd sentence: 'Typically, the human vehicle occupant gradually becomes too confident in the system's ability to control the vehicle and begins to be less attentive.'<br>c) INCORRECT – Testers should 'test both the quality of the system recommendations and the quality of the corresponding human inputs by representative users' (Chapter 7.4, last paragraph).<br><strong>d) CORRECT</strong> – 'It has been shown that the first form of automation bias ('the human accepts the system's recommendations') typically reduces the quality of decisions made by about 5%, but depending on the system context, this value can be much higher.' (Chapter 7.4, first bullet point, 1st and 3rd sentences)"
      }
    ]
  },
  {
    "LearningObjective": "AI-5.2.1",
    "LearningObjectiveDescription": "Contrast and compare the concepts behind the ML functional performance metrics for classification, regression and clustering methods.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q88",
        "QuestionText": "For different types of ML problems, there are different functional performance metrics. Which of the following statements about this is correct?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "The Area Under the Curve (AUC) shows how well the model fits the dependent variables.",
          "b": "The Mean Squared Error (MSE) indicates, based on the measurement of the ROC curve, how well the model distinguishes between different classes.",
          "c": "The R-Squared metric represents how well the classifier separates, i.e., how well the model distinguishes between classes.",
          "d": "The Silhouette value is based on the measurement of the average cross-cluster and intra-cluster distances of the data points."
        },
        "CorrectAnswer": "d",
        "Explanation": "a) INCORRECT – The Area Under the Curve (AUC) represents the degree of separability of a classifier (see Chapter 5.2, 2nd paragraph, 2nd bullet point). How well the model fits the dependent variables is assessed by the R-Squared metric (see Chapter 5.2, 3rd paragraph, 2nd bullet point).<br>b) INCORRECT – The Mean Squared Error is the average of the squared differences between the actual value and the predicted value (see Chapter 5.2, 3rd paragraph, 1st bullet point). How well the model distinguishes between different classes is shown by the Area Under the Curve (AUC) (see Chapter 5.2, 2nd paragraph, 2nd bullet point).<br>c) INCORRECT – The R-Squared metric is used for supervised regression (see Chapter 5.2, 3rd paragraph, 2nd bullet point) and not for classification problems; this is shown by the Area Under the Curve (AUC) (see Chapter 5.2, 2nd paragraph, Metrics for Supervised Classification, 2nd bullet point).<br><strong>d) CORRECT</strong> – See AI Curriculum, Section 5.2, 4th paragraph (Metrics for Unsupervised Clustering), 3rd bullet point."
      }
    ]
  },
  {
    "LearningObjective": "AI-4.5.1",
    "LearningObjectiveDescription": "Recall the different approaches to the labelling of data in datasets for supervised learning.",
    "KnowledgeType": "K1",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q89",
        "QuestionText": "When an ML engineer determines that the training data is insufficient, they rotate labeled images to create additional training data. Which of the following labeling approaches is being used in this example?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Crowdsourcing",
          "b": "Internal",
          "c": "AI-supported",
          "d": "Outsourced"
        },
        "CorrectAnswer": "b",
        "Explanation": "a) INCORRECT – Crowdsourcing means that a large number of people perform a specific task. In this case, only one person is performing the task. Compare the definition of Crowdsourced in CT-AI Curriculum, Chapter 4.5.1, Paragraph 1, Bullet Point 3.<br><strong>b) CORRECT</strong> – Here, the enrichment of already labeled data is done by the ML developer themselves, i.e., within the company responsible for data labeling. Compare the definition of internal labeling in CT-AI Curriculum, Chapter 4.5.1, Paragraph 1, Bullet Point 1.<br>c) INCORRECT – AI is not being used to label the data. Compare the definition of AI-supported labeling in CT-AI Curriculum, Chapter 4.5.1, Paragraph 1, Bullet Point 4.<br>d) INCORRECT – The ML engineer has not outsourced the task to a third party. Compare the definition of outsourced labeling in CT-AI Curriculum, Chapter 4.5.1, Paragraph 1, Bullet Point 2."
      }
    ]
  },
  {
    "LearningObjective": "AI-4.3.1",
    "LearningObjectiveDescription": "Describe typical dataset quality issues.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q90",
        "QuestionText": "Insufficient data is being used for training. Which of the following options MOST indicates this data quality problem?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Training the model with the available data is particularly laborious. However, if part of the dataset is not used for training, the training yields positive results more quickly.",
          "b": "The model works correctly but excessively favors certain constellations in its predictions.",
          "c": "Real customer data is used for training the model, even though customer consent is lacking.",
          "d": "The model cannot be trained with a specific algorithm, although other algorithms work with the same training data."
        },
        "CorrectAnswer": "d",
        "Explanation": "a) INCORRECT – If training the model is particularly laborious and resource-intensive, this indicates irrelevant data in the dataset. If the irrelevant data is not used during training, the training is completed faster. Compare the explanation of irrelevant data in CT-AI Curriculum, Chapter 4.3, Table 1, Row 11.<br>b) INCORRECT – This refers to a biased model, which can be caused by incomplete, unbalanced, unfair, low-diversity, or duplicate data. Compare the explanation of biased models in CT-AI Curriculum, Chapter 4.4, Paragraph 2, Bullet Point 2.<br>c) INCORRECT – This indicates that data privacy issues were not considered. Compare the explanation of data privacy concerns in CT-AI Curriculum, Chapter 4.3, Table 1, Row 12.<br><strong>d) CORRECT</strong> – The fact that models based on some learning algorithms can be trained with the data, but it doesn't work with a specific algorithm, is most likely because the data volume is insufficient for that particular algorithm. The required minimum data volume can vary for different algorithms. Compare the explanation of insufficient data in CT-AI Curriculum, Chapter 4.3, Table 1, Row 5."
      }
    ]
  },
  {
    "LearningObjective": "AI-2.8.1",
    "LearningObjectiveDescription": "Recall the characteristics that make it difficult to use AI-based systems in safety-related applications.",
    "KnowledgeType": "K1",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q91",
        "QuestionText": "Which of the following characteristics of an AI-based system is likely to cause difficulties when applied in a safety-critical area?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Probabilistic",
          "b": "Explainable",
          "c": "Interpretable",
          "d": "Deterministic"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>a) CORRECT</strong> – Probabilistic behavior poses a clear problem for safety-critical systems. See CT-AI Curriculum, Chapter 2.9 (should be 2.8), Bullet Point 3.<br>b) INCORRECT – Explainability is usually required for safety-critical systems and is therefore not problematic. Compare CT-AI Curriculum, Chapter 2.8, last bullet point 'Explainability'.<br>c) INCORRECT – Interpretability is usually required for safety-critical systems and is therefore not problematic. Compare CT-AI Curriculum, Chapter 2.8, second-to-last bullet point 'Interpretability'.<br>d) INCORRECT – Deterministic behavior is typically required for safety-critical systems. See also CT-AI Curriculum, Chapter 2.9, second bullet point 'Non-determinism' which describes a problem. Therefore, determinism is not problematic."
      }
    ]
  },
  {
    "LearningObjective": "AI-1.4.1",
    "LearningObjectiveDescription": "Recognize the different technologies used to implement AI.",
    "KnowledgeType": "K1",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q92",
        "QuestionText": "Which of the following is a Machine Learning technology used to implement Artificial Intelligence?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Rule Engine",
          "b": "Neural Network",
          "c": "Fuzzy Logic",
          "d": "Procedural Reasoning"
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>Correct! b) Neural Network</strong> because neural networks are one of the key <strong>machine learning techniques</strong> listed in section 1.4 of the syllabus (AI Technologies).<br><br><strong>Incorrect a) Rule Engine</strong> because rule engines are a reasoning technology and are part of expert systems, which are usually explicitly programmed and do not 'learn' from data.<br><strong>Incorrect c) Fuzzy Logic</strong> because fuzzy logic is a separate technology used to handle uncertainty and partial truth values, but it is not itself a Machine Learning method.<br><strong>Incorrect d) Procedural Reasoning</strong> because procedural reasoning is also a reasoning technique, not a Machine Learning one."
      }
    ]
  },
  {
    "LearningObjective": "AI-2.1.1",
    "LearningObjectiveDescription": "Explain the importance of flexibility and adaptability as characteristics of AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q93",
        "QuestionText": "According to the CT-AI syllabus, which of the following is the correct definition of adaptability of an AI-based system?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "The ability of the system to be used in situations that were not part of the original requirements.",
          "b": "The ease with which the system can be modified for new situations, such as different hardware or changing operational environments.",
          "c": "The ability of the system to work independently of human oversight and control for prolonged periods.",
          "d": "The ability of the system to improve itself in response to changing external constraints."
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>Correct! b) The ease with which the system can be modified for new situations...</strong> because this is the exact definition of <strong>adaptability</strong> according to section 2.1 of the syllabus.<br><br><strong>Incorrect a) The ability of the system to be used in situations that were not part of the original requirements</strong> because this is the definition of <strong>flexibility</strong> according to the same paragraph.<br><strong>Incorrect c) The ability of the system to work independently of human oversight and control for prolonged periods</strong> because this defines <strong>autonomy</strong> as described in section 2.2.<br><strong>Incorrect d) The ability of the system to improve itself in response to changing external constraints</strong> because this defines <strong>evolution</strong> as described in section 2.3."
      }
    ]
  },
  {
    "LearningObjective": "AI-2.4.1",
    "LearningObjectiveDescription": "Describe the different causes and types of bias found in AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q94",
        "QuestionText": "An ML-based recruitment system, trained mainly with CVs from male candidates from a specific region, performs poorly in evaluating female candidates from other regions. This type of bias is most likely:",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Algorithmic Bias",
          "b": "Sample Bias",
          "c": "Inappropriate Bias",
          "d": "Configuration Bias"
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>Correct! b) Sample Bias</strong> because <strong>sample bias</strong> occurs when training data is not fully representative of the entire data space to which ML is applied. In this case, the dataset (men from a specific region) is not representative of the general population (all candidates).<br><br><strong>Incorrect a) Algorithmic Bias</strong> because algorithmic bias would be caused by incorrect configuration of the learning algorithm itself, not by data.<br><strong>Incorrect c) Inappropriate Bias</strong> because although sample bias can lead to inappropriate bias (which is the undesirable effect), the root cause in this scenario is the nature of the training data (sample bias).<br><strong>Incorrect d) Configuration Bias</strong> because 'configuration bias' is not a term defined in the syllabus."
      }
    ]
  },
  {
    "LearningObjective": "AI-2.7.1",
    "LearningObjectiveDescription": "Explain how transparency, interpretability and explainability apply to AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q95",
        "QuestionText": "Which of the following desirable characteristics of an AI-based system refers to \"the ease with which users can determine how the system came up with a particular result\"?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Transparency",
          "b": "Interpretability",
          "c": "Explainability",
          "d": "Robustness"
        },
        "CorrectAnswer": "c",
        "Explanation": "<strong>Correct! c) Explainability</strong> because this is the exact definition of <strong>explainability</strong> according to section 2.7 of the syllabus.<br><br><strong>Incorrect a) Transparency</strong> because transparency refers to the ease with which the algorithm and training data used to generate the model can be determined.<br><strong>Incorrect b) Interpretability</strong> because interpretability refers to the understandability of the AI technology by various stakeholders.<br><strong>Incorrect d) Robustness</strong> because robustness is a different quality characteristic related to resistance to disturbances or unexpected conditions."
      }
    ]
  },
  {
    "LearningObjective": "AI-3.1.1",
    "LearningObjectiveDescription": "Describe classification and regression as part of supervised learning.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q96",
        "QuestionText": "Choose the problem that is best solved by regression using supervised learning.",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Predicting whether an email is spam or not.",
          "b": "Grouping customers into segments for marketing.",
          "c": "Predicting the price of a house based on its characteristics.",
          "d": "Identifying flower species in an image."
        },
        "CorrectAnswer": "c",
        "Explanation": "<strong>Correct! c) Predicting the price of a house based on its characteristics</strong> because <strong>regression</strong> is used to predict a continuous numerical output value. The price of a house is such an example, according to section 3.1.1.<br><br><strong>Incorrect a) Predicting whether an email is spam or not</strong> because this is a <strong>classification</strong> problem (the classes being 'spam' and 'not spam').<br><strong>Incorrect b) Grouping customers into segments for marketing</strong> because grouping (clustering) is an <strong>unsupervised learning</strong> technique.<br><strong>Incorrect d) Identifying flower species in an image</strong> because this is also a <strong>classification</strong> problem, where the output is a categorical label (the flower species)."
      }
    ]
  },
  {
    "LearningObjective": "AI-3.1.2",
    "LearningObjectiveDescription": "Describe clustering and association as part of unsupervised learning.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q97",
        "QuestionText": "An online retailer wants to analyze customer purchase data to identify products that are often bought together (e.g., \"customers who buy product X also buy product Y\"). What is the most appropriate form of machine learning for this task?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Classification",
          "b": "Regression",
          "c": "Association",
          "d": "Reinforcement Learning"
        },
        "CorrectAnswer": "c",
        "Explanation": "<strong>Correct! c) Association</strong> because <strong>association</strong> is an unsupervised learning technique used to identify interesting relationships or dependencies between data attributes, such as product co-occurrence, according to section 3.1.2.<br><br><strong>Incorrect a) Classification</strong> because classification involves predicting a category label for an input.<br><strong>Incorrect b) Regression</strong> because regression involves predicting a numerical value.<br><strong>Incorrect d) Reinforcement Learning</strong> because reinforcement learning involves an agent learning through interaction with an environment and receiving rewards/penalties."
      }
    ]
  },
  {
    "LearningObjective": "AI-3.5.1",
    "LearningObjectiveDescription": "Summarize the concepts of underfitting and overfitting.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q98",
        "QuestionText": "A Machine Learning model shows very high accuracy on the training dataset, but significantly lower accuracy on the test dataset, which consists of \"unseen\" data. This phenomenon is known as:",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Underfitting",
          "b": "Overfitting",
          "c": "Sample Bias",
          "d": "Regression Error"
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>Correct! b) Overfitting</strong> because <strong>overfitting</strong> occurs when a model fits too closely to the training data, including noise and outliers, and fails to generalize to new, unseen data. This is exactly the scenario described, according to section 3.5.1.<br><br><strong>Incorrect a) Underfitting</strong> because underfitting occurs when the model is too simplistic and cannot learn even the patterns in the training data, having poor performance on both training and test data.<br><strong>Incorrect c) Sample Bias</strong> because although sample bias can contribute to overfitting, the specific term for this lack of generalization is 'overfitting'.<br><strong>Incorrect d) Regression Error</strong> because 'regression error' is not a standard term for this concept."
      }
    ]
  },
  {
    "LearningObjective": "AI-4.2.1",
    "LearningObjectiveDescription": "Contrast the use of training, validation and test datasets in the development of an ML model.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q99",
        "QuestionText": "What is the main purpose of the validation dataset in the ML workflow?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "To be used for the initial training of the model.",
          "b": "To be used for evaluating and tuning the model during development.",
          "c": "To be used for the final and independent testing of the model, after it has been selected and tuned.",
          "d": "To be used in production for making operational predictions."
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>Correct! b) To be used for evaluating and tuning the model during development</strong> because the <strong>validation dataset</strong> is used for evaluating the model during training cycles and to guide hyperparameter tuning. This allows selecting the best model without 'cheating' by using the test set, according to section 4.2.<br><br><strong>Incorrect a) To be used for the initial training of the model</strong> because this is the purpose of the <strong>training</strong> dataset.<br><strong>Incorrect c) To be used for the final and independent testing of the model, after it has been selected and tuned</strong> because this is the purpose of the <strong>test</strong> dataset (holdout dataset).<br><strong>Incorrect d) To be used in production for making operational predictions</strong> because production data are new, real data that the final model uses after deployment."
      }
    ]
  },
  {
    "LearningObjective": "AI-4.5.1",
    "LearningObjectiveDescription": "Recall the different approaches to the labelling of data in datasets for supervised learning.",
    "KnowledgeType": "K1",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q100",
        "QuestionText": "Which of the following is a data labeling approach where this activity is performed by a large group of people, with results consolidated to ensure quality?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Internal Labeling",
          "b": "Outsourced Labeling",
          "c": "Crowdsourced Labeling",
          "d": "AI-Assisted Labeling"
        },
        "CorrectAnswer": "c",
        "Explanation": "<strong>Correct! c) Crowdsourced Labeling</strong> because <strong>crowdsourced labeling</strong> involves using a crowd of people, and to manage quality, several annotators may be asked to label the same data, after which a decision is made on the final label, according to section 4.5.1.<br><br><strong>Incorrect a) Internal Labeling</strong> because internal labeling is done by employees of the organization.<br><strong>Incorrect b) Outsourced Labeling</strong> because outsourced labeling is done by a specialized organization, but not necessarily through a crowd of people.<br><strong>Incorrect d) AI-Assisted Labeling</strong> because AI-assisted labeling uses an AI-based tool to pre-label data, which is then verified by a human."
      }
    ]
  },
  {
    "LearningObjective": "AI-5.1.1",
    "LearningObjectiveDescription": "Calculate the ML functional performance metrics from a given set of confusion matrix data.",
    "KnowledgeType": "K3",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q101",
        "QuestionText": "For a binary classification model, the following values are known: True Positive (TP) = 80, True Negative (TN) = 50, False Positive (FP) = 20, False Negative (FN) = 10. What is the precision of the model?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "76.9%",
          "b": "80.0%",
          "c": "88.9%",
          "d": "72.7%"
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>Correct! b) 80.0%</strong> because <strong>precision</strong> is calculated as TP / (TP + FP). Calculation: 80 / (80 + 20) = 80 / 100 = 0.8 = 80.0%<br><br><strong>Incorrect a) 76.9%</strong> because this is the accuracy (Accuracy): (TP + TN) / Total = (80+50) / (80+50+20+10) = 130 / 160 ≈ 0.769.<br><strong>Incorrect c) 88.9%</strong> because this is the recall (Recall): TP / (TP + FN) = 80 / (80+10) = 80/90 ≈ 0.889.<br><strong>Incorrect d) 72.7%</strong> because this does not correspond to any standard metric from the confusion matrix for this data."
      }
    ]
  },
  {
    "LearningObjective": "AI-5.2.1",
    "LearningObjectiveDescription": "Contrast and compare the concepts behind the ML functional performance metrics for classification, regression and clustering methods.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q102",
        "QuestionText": "Which of the following metrics is specific to clustering problems in unsupervised learning?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Accuracy",
          "b": "Precision",
          "c": "Silhouette Coefficient",
          "d": "Mean Square Error (MSE)"
        },
        "CorrectAnswer": "c",
        "Explanation": "<strong>Correct! c) Silhouette Coefficient</strong> because the <strong>Silhouette Coefficient</strong> is a metric used to evaluate the quality of clustering in unsupervised learning. It measures how well each cluster is separated from others, with values between -1 and +1, according to section 5.2 of the syllabus.<br><br><strong>Incorrect a) Accuracy</strong> because accuracy is a metric for supervised classification problems.<br><strong>Incorrect b) Precision</strong> because precision is also a metric for supervised classification, derived from the confusion matrix.<br><strong>Incorrect d) Mean Square Error (MSE)</strong> because Mean Square Error (MSE) is a metric for supervised regression problems."
      }
    ]
  },
  {
    "LearningObjective": "AI-5.2.1",
    "LearningObjectiveDescription": "Contrast and compare the concepts behind the ML functional performance metrics for classification, regression and clustering methods.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q103",
        "QuestionText": "For a regression model predicting a company's sales, which metric measures the average of the squares of the differences between actual and predicted values?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "AUC (Area Under Curve)",
          "b": "R-squared",
          "c": "MSE (Mean Square Error)",
          "d": "F1-score"
        },
        "CorrectAnswer": "c",
        "Explanation": "<strong>Correct! c) MSE (Mean Square Error)</strong> because <strong>MSE</strong> is defined as the average of the squares of the differences between actual and predicted values, being a common metric for regression, according to section 5.2.<br><br><strong>Incorrect a) AUC (Area Under Curve)</strong> because AUC is used for binary classification and refers to the area under the ROC curve.<br><strong>Incorrect b) R-squared</strong> because R-squared measures the proportion of the variance in the dependent variable that is explained by the model, but it is not the average of squared errors.<br><strong>Incorrect d) F1-score</strong> because F1-score is a metric for classification that combines precision and recall."
      }
    ]
  },
  {
    "LearningObjective": "AI-5.4.1",
    "LearningObjectiveDescription": "Select appropriate ML functional performance metrics and/or their values for a given ML model and scenario.",
    "KnowledgeType": "K4",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q104",
        "QuestionText": "A model is being developed to detect disease X in a population. It is critical not to miss patients who have the disease (i.e., no false negatives). Which metric should be maximized to ensure this?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Precision",
          "b": "Accuracy",
          "c": "Recall",
          "d": "F1-score"
        },
        "CorrectAnswer": "c",
        "Explanation": "<strong>Correct! c) Recall</strong> because <strong>recall</strong> measures the proportion of true positives correctly detected among all actual positive cases. In this scenario, maximizing recall ensures that the number of false negatives is minimized, allowing the detection of as many real disease cases as possible, according to discussions in section 5.4.<br><br><strong>Incorrect a) Precision</strong> because precision refers to the proportion of correct positive predictions among all positive predictions, which is important to avoid false positives, but does not directly address false negatives.<br><strong>Incorrect b) Accuracy</strong> because accuracy measures overall performance, but can be misleading if classes are imbalanced and does not specifically focus on false negatives.<br><strong>Incorrect d) F1-score</strong> because the F1-score is the harmonic mean of precision and recall, but does not specifically maximize recall."
      }
    ]
  },
  {
    "LearningObjective": "AI-5.4.1",
    "LearningObjectiveDescription": "Select appropriate ML functional performance metrics and/or their values for a given ML model and scenario.",
    "KnowledgeType": "K4",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q105",
        "QuestionText": "A spam filtering system is designed to minimize the number of legitimate emails that are mistakenly marked as spam. Which metric should be prioritized in this case?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Recall",
          "b": "Precision",
          "c": "Accuracy",
          "d": "F1-score"
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>Correct! b) Precision</strong> because <strong>precision</strong> measures the proportion of emails correctly identified as spam among all emails marked as spam. High precision means that few legitimate emails (false positives) are misclassified as spam, which is crucial in this scenario, according to section 5.4.<br><br><strong>Incorrect a) Recall</strong> because recall would measure the proportion of actual spam detected, but this could increase the number of false positives if set too high.<br><strong>Incorrect c) Accuracy</strong> because accuracy could be high even if there are false positives, if the number of false negatives is low.<br><strong>Incorrect d) F1-score</strong> because the F1-score would balance precision and recall, but does not focus solely on minimizing false positives."
      }
    ]
  },
  {
    "LearningObjective": "AI-7.4.1",
    "LearningObjectiveDescription": "Explain automation bias and how this affects testing.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q106",
        "QuestionText": "Which of the following best describes \"automation bias\" in the context of AI-based systems?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "The tendency of an algorithm to favor certain groups due to training data.",
          "b": "The tendency of people to over-trust the recommendations of an automated system and ignore other sources of information.",
          "c": "The bias introduced by engineers during the development of the ML model.",
          "d": "The systematic error of sensors that feed the system with data."
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>Correct! b) The tendency of people to over-trust the recommendations of an automated system and ignore other sources of information</strong> because <strong>automation bias</strong> is defined in section 7.4 of the syllabus as people's tendency to rely excessively on automated systems, which can lead to errors in decisions when the system provides incorrect recommendations.<br><br><strong>Incorrect a) The tendency of an algorithm to favor certain groups due to training data</strong> because this describes sample or algorithmic bias, not automation bias.<br><strong>Incorrect c) The bias introduced by engineers during the development of the ML model</strong> because this refers to bias introduced by developers, which is a different concept.<br><strong>Incorrect d) The systematic error of sensors that feed the system with data</strong> because this refers to hardware or data errors, not users' cognitive bias."
      }
    ]
  },
  {
    "LearningObjective": "AI-7.4.1",
    "LearningObjectiveDescription": "Explain automation bias and how this affects testing.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q107",
        "QuestionText": "How should testers approach \"automation bias\" when testing an AI-based system that assists people in decision-making?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Test only the accuracy of the system's recommendations.",
          "b": "Ensure that the system can operate completely without human intervention.",
          "c": "Test both the quality of the system's recommendations and the quality of human input provided by representative users.",
          "d": "Focus exclusively on the system's performance under ideal conditions."
        },
        "CorrectAnswer": "c",
        "Explanation": "<strong>Correct! c) Test both the quality of the system's recommendations and the quality of human input provided by representative users</strong> because to manage <strong>automation bias</strong>, testers need to evaluate how people interact with the system, including whether they accept recommendations without sufficient verification or fail to adequately monitor the system, according to section 7.4.<br><br><strong>Incorrect a) Test only the accuracy of the system's recommendations</strong> because testing only the system's accuracy does not address how users' cognitive bias affects final decisions.<br><strong>Incorrect b) Ensure that the system can operate completely without human intervention</strong> because this refers to autonomy, not automation bias.<br><strong>Incorrect d) Focus exclusively on the system's performance under ideal conditions</strong> because testing under ideal conditions does not capture real scenarios where automation bias intervenes."
      }
    ]
  },
  {
    "LearningObjective": "AI-8.3.1",
    "LearningObjectiveDescription": "Explain how to test for bias in an AI-based system.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q108",
        "QuestionText": "Which of the following is an effective method to test for the presence of sample bias in an ML system?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Analyzing the hyperparameters of the algorithm used.",
          "b": "Reviewing the source of the training data and the acquisition processes.",
          "c": "Testing the model's accuracy on a test dataset identical to the training set.",
          "d": "Using neuron coverage on neural networks."
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>Correct! b) Reviewing the source of the training data and the acquisition processes</strong> because <strong>sample bias</strong> occurs when training data is not representative of the application domain. Reviewing the sources and methods of data collection can identify potential representativeness issues, according to section 8.3.<br><br><strong>Incorrect a) Analyzing the hyperparameters of the algorithm used</strong> because analyzing hyperparameters refers to algorithmic bias, not sample bias.<br><strong>Incorrect c) Testing the model's accuracy on a test dataset identical to the training set</strong> because testing on the same dataset as training will not detect sample bias; on the contrary, it may mask problems.<br><strong>Incorrect d) Using neuron coverage on neural networks</strong> because neuron coverage is a testing metric for neural networks, not for sample bias."
      }
    ]
  },
  {
    "LearningObjective": "AI-8.3.1",
    "LearningObjectiveDescription": "Explain how to test for bias in an AI-based system.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q109",
        "QuestionText": "To test for inappropriate bias in an ML-based recruitment system, which approach would be most relevant?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Measuring the impact of input changes on outputs for different demographic groups.",
          "b": "Calculating the model's accuracy on a validation dataset.",
          "c": "Optimizing the model's inference speed.",
          "d": "Testing the model on a randomly generated synthetic dataset."
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>Correct! a) Measuring the impact of input changes on outputs for different demographic groups</strong> because this approach, similar to methods like LIME, allows identifying how the system responds differently to various groups, detecting inappropriate biases related to characteristics such as gender or race, according to section 8.3.<br><br><strong>Incorrect b) Calculating the model's accuracy on a validation dataset</strong> because calculating overall accuracy does not reveal inappropriate biases if the dataset is biased.<br><strong>Incorrect c) Optimizing the model's inference speed</strong> because optimizing speed is a performance concern, not a bias one.<br><strong>Incorrect d) Testing the model on a randomly generated synthetic dataset</strong> because random synthetic data cannot reproduce real-world biases and may be uninterpretable."
      }
    ]
  },
  {
    "LearningObjective": "AI-8.6.1",
    "LearningObjectiveDescription": "Describe how the transparency, interpretability and explainability of AI-based systems can be tested.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q110",
        "QuestionText": "Which of the following methods is model-agnostic (applicable regardless of model type) and can be used to test the explainability of an ML model?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Tree analysis of a decision tree model.",
          "b": "LIME method (Local Interpretable Model-Agnostic Explanations)",
          "c": "Inspecting the weights in a neural network.",
          "d": "Using a linear regression model for interpretation."
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>Correct! b) LIME method (Local Interpretable Model-Agnostic Explanations)</strong> because <strong>LIME</strong> is a model-agnostic method that perturbs inputs and analyzes outputs to provide local explanations, being suitable for testing explainability, according to section 8.6.<br><br><strong>Incorrect a) Tree analysis of a decision tree model</strong> because tree analysis is specific to decision tree models and is not model-agnostic.<br><strong>Incorrect c) Inspecting the weights in a neural network</strong> because inspecting weights is specific to neural networks and can be complex to interpret.<br><strong>Incorrect d) Using a linear regression model for interpretation</strong> because linear regression is a specific model and is not necessarily applicable to other types of models."
      }
    ]
  },
  {
    "LearningObjective": "AI-8.6.1",
    "LearningObjectiveDescription": "Describe how the transparency, interpretability and explainability of AI-based systems can be tested.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q111",
        "QuestionText": "How can the \"transparency\" of an AI-based system be tested?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "By surveying users to understand how they perceive the system's decisions.",
          "b": "By comparing documented information about the algorithm and data with the actual implementation.",
          "c": "By measuring the speed at which the system provides responses.",
          "d": "By testing the system with perturbed input data."
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>Correct! b) By comparing documented information about the algorithm and data with the actual implementation</strong> because <strong>transparency</strong> refers to the ease with which the algorithm and training data can be determined. Testing involves checking whether the documentation matches the implementation, according to section 8.6.<br><br><strong>Incorrect a) By surveying users to understand how they perceive the system's decisions</strong> because this tests explainability or interpretability from the users' perspective, not transparency.<br><strong>Incorrect c) By measuring the speed at which the system provides responses</strong> because response speed is a performance metric, not a transparency one.<br><strong>Incorrect d) By testing the system with perturbed input data</strong> because testing with perturbed data is used for robustness or explainability (as in LIME), not directly for transparency."
      }
    ]
  },
  {
    "LearningObjective": "AI-1.4.1",
    "LearningObjectiveDescription": "Recognize the different technologies used to implement AI.",
    "KnowledgeType": "K1",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q112",
        "QuestionText": "Scenario: You are a solution architect at TechCorp, designing an AI system to automatically categorize customer support tickets. The system needs to learn from historical ticket data and improve over time. Which AI technology would be most appropriate for this learning capability?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Fuzzy Logic system",
          "b": "Rule Engine with predefined categories",
          "c": "Machine Learning classification algorithm",
          "d": "Case-based reasoning system"
        },
        "CorrectAnswer": "c",
        "Explanation": "<strong>Correct! c) Machine Learning classification algorithm</strong> because <strong>ML classification algorithms</strong> can learn patterns from historical data to automatically categorize new tickets, unlike rule-based systems that require manual rule creation.<br><br><strong>Incorrect a) Fuzzy Logic system</strong> because fuzzy logic handles uncertainty but doesn't learn from historical data.<br><strong>Incorrect b) Rule Engine with predefined categories</strong> because rule engines require explicit programming of rules and don't improve over time through learning.<br><strong>Incorrect d) Case-based reasoning system</strong> because while case-based reasoning can adapt, it's not primarily designed for continuous learning from large historical datasets."
      }
    ]
  },
  {
    "LearningObjective": "AI-1.4.1",
    "LearningObjectiveDescription": "Recognize the different technologies used to implement AI.",
    "KnowledgeType": "K1",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q113",
        "QuestionText": "Scenario: As an AI consultant for an automotive company, you're asked to recommend technology for real-time object detection in self-driving cars that can handle uncertain conditions like fog or rain. Which AI technology is specifically designed to handle such uncertainty?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Neural Networks",
          "b": "Fuzzy Logic",
          "c": "Decision Trees",
          "d": "Search Algorithms"
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>Correct! b) Fuzzy Logic</strong> because <strong>Fuzzy Logic</strong> is specifically designed to handle uncertainty and partial truth values, making it suitable for conditions where sensor data may be ambiguous like fog or rain.<br><br><strong>Incorrect a) Neural Networks</strong> because while neural networks can handle complex patterns, they're not specifically optimized for uncertainty handling like fuzzy logic.<br><strong>Incorrect c) Decision Trees</strong> because decision trees make binary decisions and aren't designed for handling continuous uncertainty.<br><strong>Incorrect d) Search Algorithms</strong> because search algorithms find optimal paths but don't specifically handle sensor uncertainty."
      }
    ]
  },
  {
    "LearningObjective": "AI-2.1.1",
    "LearningObjectiveDescription": "Explain the importance of flexibility and adaptability as characteristics of AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q114",
        "QuestionText": "Scenario: You're the Test Manager for a financial institution implementing an AI fraud detection system. The regulatory environment frequently changes, requiring the system to adapt to new fraud patterns. Which quality characteristic should be your primary testing focus?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Autonomy",
          "b": "Flexibility",
          "c": "Adaptability",
          "d": "Transparency"
        },
        "CorrectAnswer": "c",
        "Explanation": "<strong>Correct! c) Adaptability</strong> because <strong>adaptability</strong> refers to the ease with which the system can be modified for new situations, which is crucial for handling changing regulatory requirements and fraud patterns.<br><br><strong>Incorrect a) Autonomy</strong> because autonomy refers to working independently without human oversight, not adapting to changes.<br><strong>Incorrect b) Flexibility</strong> because flexibility refers to working in unanticipated contexts, not modifying the system for new situations.<br><strong>Incorrect d) Transparency</strong> because transparency relates to understanding how the system works, not adapting to changes."
      }
    ]
  },
  {
    "LearningObjective": "AI-2.1.1",
    "LearningObjectiveDescription": "Explain the importance of flexibility and adaptability as characteristics of AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q115",
        "QuestionText": "Scenario: As Project Lead for a smart home system, you need the AI to handle unexpected scenarios like new smart devices or unusual user behaviors not covered in initial specifications. Which characteristic ensures the system can operate in these unanticipated contexts?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Evolution",
          "b": "Flexibility",
          "c": "Autonomy",
          "d": "Safety"
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>Correct! b) Flexibility</strong> because <strong>flexibility</strong> is the ability to work in contexts outside initial specifications, making it essential for handling unexpected scenarios in smart home environments.<br><br><strong>Incorrect a) Evolution</strong> because evolution refers to self-improvement in response to changing constraints.<br><strong>Incorrect c) Autonomy</strong> because autonomy refers to independent operation without human intervention.<br><strong>Incorrect d) Safety</strong> because safety is about preventing harm, not handling unexpected scenarios."
      }
    ]
  },
  {
    "LearningObjective": "AI-2.4.1",
    "LearningObjectiveDescription": "Describe the different causes and types of bias found in AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q116",
        "QuestionText": "Scenario: You're testing a loan approval AI system and discover it was trained primarily on data from high-income urban areas. The system now shows poor performance for rural and low-income applicants. What type of bias is this?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Algorithmic Bias",
          "b": "Sample Bias",
          "c": "Measurement Bias",
          "d": "Automation Bias"
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>Correct! b) Sample Bias</strong> because this is classic <strong>sample bias</strong> where the training data doesn't represent the entire population the system will serve.<br><br><strong>Incorrect a) Algorithmic Bias</strong> because algorithmic bias comes from the algorithm itself, not from unrepresentative training data.<br><strong>Incorrect c) Measurement Bias</strong> because measurement bias occurs during data collection, not from geographic underrepresentation.<br><strong>Incorrect d) Automation Bias</strong> because automation bias is human cognitive bias, not data representation issues."
      }
    ]
  },
  {
    "LearningObjective": "AI-8.3.1",
    "LearningObjectiveDescription": "Explain how to test for bias in an AI-based system.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q117",
        "QuestionText": "Scenario: As an Ethics Officer reviewing a hiring AI, you find it penalizes resumes with gender-neutral pronouns and favors traditionally male-dominated field experience. What testing approach would best identify this inappropriate bias?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Testing model accuracy on validation data",
          "b": "Analyzing hyperparameter configurations",
          "c": "Correlation analysis between outputs and protected attributes",
          "d": "Performance testing under load"
        },
        "CorrectAnswer": "c",
        "Explanation": "<strong>Correct! c) Correlation analysis between outputs and protected attributes</strong> because correlating system outputs with <strong>protected attributes</strong> helps identify inappropriate bias, even when these attributes aren't explicit inputs.<br><br><strong>Incorrect a) Testing model accuracy on validation data</strong> because overall accuracy doesn't reveal bias against specific groups.<br><strong>Incorrect b) Analyzing hyperparameter configurations</strong> because hyperparameters relate to model tuning, not bias detection.<br><strong>Incorrect d) Performance testing under load</strong> because performance testing measures system speed, not fairness."
      }
    ]
  },
  {
    "LearningObjective": "AI-2.7.1",
    "LearningObjectiveDescription": "Explain how transparency, interpretability and explainability apply to AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q118",
        "QuestionText": "Scenario: You're a regulatory compliance officer for a healthcare AI that diagnoses diseases. Doctors need to understand why specific diagnoses are made for medical validation. Which characteristic is most critical?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Transparency",
          "b": "Interpretability",
          "c": "Explainability",
          "d": "Robustness"
        },
        "CorrectAnswer": "c",
        "Explanation": "<strong>Correct! c) Explainability</strong> because <strong>explainability</strong> focuses on how easily users can determine how the system reached a particular result, which is crucial for medical diagnosis validation.<br><br><strong>Incorrect a) Transparency</strong> because transparency relates to visibility of algorithms and data, not understanding specific decisions.<br><strong>Incorrect b) Interpretability</strong> because interpretability refers to understanding the technology at a conceptual level.<br><strong>Incorrect d) Robustness</strong> because robustness relates to performance under adverse conditions."
      }
    ]
  },
  {
    "LearningObjective": "AI-2.7.1",
    "LearningObjectiveDescription": "Explain how transparency, interpretability and explainability apply to AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q119",
        "QuestionText": "Scenario: As a Data Scientist presenting to non-technical stakeholders, you need them to understand your AI's basic functioning without technical details. Which characteristic should you emphasize?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Transparency of algorithms",
          "b": "Interpretability of the technology",
          "c": "Explainability of specific decisions",
          "d": "Accuracy of predictions"
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>Correct! b) Interpretability of the technology</strong> because <strong>interpretability</strong> refers to stakeholders' understanding of the underlying AI technology at an appropriate level of detail.<br><br><strong>Incorrect a) Transparency of algorithms</strong> because transparency provides technical details that may be too complex for non-technical stakeholders.<br><strong>Incorrect c) Explainability of specific decisions</strong> because explainability focuses on individual decisions, not overall system understanding.<br><strong>Incorrect d) Accuracy of predictions</strong> because accuracy metrics don't help stakeholders understand how the system works."
      }
    ]
  },
  {
    "LearningObjective": "AI-3.1.1",
    "LearningObjectiveDescription": "Describe classification and regression as part of supervised learning.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q120",
        "QuestionText": "Scenario: You're an ML engineer at an e-commerce company tasked with predicting customer churn probability. What type of supervised learning problem is this?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Classification",
          "b": "Regression",
          "c": "Clustering",
          "d": "Association"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>Correct! a) Classification</strong> because predicting churn probability (likely as high/medium/low risk) is a <strong>classification</strong> problem where outputs are discrete categories.<br><br><strong>Incorrect b) Regression</strong> because regression predicts continuous numerical values, not probability categories.<br><strong>Incorrect c) Clustering</strong> because clustering is unsupervised learning without predefined labels.<br><strong>Incorrect d) Association</strong> because association finds relationships between items, not prediction of outcomes."
      }
    ]
  },
  {
    "LearningObjective": "AI-3.1.1",
    "LearningObjectiveDescription": "Describe classification and regression as part of supervised learning.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q121",
        "QuestionText": "Scenario: As a data analyst for a real estate platform, you're asked to build a model that estimates property prices based on features like location, size, and amenities. Which ML approach is most suitable?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Classification",
          "b": "Regression",
          "c": "Clustering",
          "d": "Reinforcement Learning"
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>Correct! b) Regression</strong> because predicting continuous numerical values like property prices is a <strong>regression</strong> problem.<br><br><strong>Incorrect a) Classification</strong> because classification predicts discrete categories, not continuous values.<br><strong>Incorrect c) Clustering</strong> because clustering groups similar items without prediction.<br><strong>Incorrect d) Reinforcement Learning</strong> because reinforcement learning involves learning through trial and error with environmental feedback."
      }
    ]
  },
  {
    "LearningObjective": "AI-3.1.2",
    "LearningObjectiveDescription": "Describe clustering and association as part of unsupervised learning.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": null,
        "QuestionId": "q122",
        "QuestionText": "Scenario: You're an AI consultant for a retail chain wanting to identify natural customer segments for targeted marketing without predefined categories. Which ML approach should you recommend?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Classification",
          "b": "Regression",
          "c": "Clustering",
          "d": "Reinforcement Learning"
        },
        "CorrectAnswer": "c",
        "Explanation": "<strong>Correct! c) Clustering</strong> because <strong>clustering</strong> identifies natural groupings in data without predefined labels, perfect for discovering customer segments.<br><br><strong>Incorrect a) Classification</strong> because classification requires predefined categories and labeled data.<br><strong>Incorrect b) Regression</strong> because regression predicts numerical values, not groups.<br><strong>Incorrect d) Reinforcement Learning</strong> because reinforcement learning involves reward-based learning from interactions."
      }
    ]
  },
  {
    "LearningObjective": "AI-3.1.3",
    "LearningObjectiveDescription": "Describe reinforcement learning.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q123",
        "QuestionText": "Scenario: As a robotics engineer, you're designing an AI that learns optimal movements through trial and error with environmental feedback. Which learning paradigm is most appropriate?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Supervised Learning",
          "b": "Unsupervised Learning",
          "c": "Reinforcement Learning",
          "d": "Transfer Learning"
        },
        "CorrectAnswer": "c",
        "Explanation": "<strong>Correct! c) Reinforcement Learning</strong> because <strong>Reinforcement Learning</strong> is designed for scenarios where an agent learns through environmental interaction and feedback.<br><br><strong>Incorrect a) Supervised Learning</strong> because supervised learning requires labeled training data.<br><strong>Incorrect b) Unsupervised Learning</strong> because unsupervised learning finds patterns without explicit feedback.<br><strong>Incorrect d) Transfer Learning</strong> because transfer learning applies knowledge from one domain to another."
      }
    ]
  },
  {
    "LearningObjective": "AI-3.5.1",
    "LearningObjectiveDescription": "Summarize the concepts of underfitting and overfitting.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q124",
        "QuestionText": "Scenario: You're a Data Scientist noticing your model achieves 98% training accuracy but only 65% on new production data. Colleagues suggest adding more model complexity. What risk are they proposing?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Underfitting",
          "b": "Overfitting",
          "c": "Sample Bias",
          "d": "Concept Drift"
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>Correct! b) Overfitting</strong> because high training accuracy with poor generalization to new data indicates <strong>overfitting</strong>, and increasing complexity would worsen this.<br><br><strong>Incorrect a) Underfitting</strong> because underfitting shows poor performance on both training and test data.<br><strong>Incorrect c) Sample Bias</strong> because sample bias relates to unrepresentative training data, not model complexity.<br><strong>Incorrect d) Concept Drift</strong> because concept drift occurs when relationships change over time, not from model complexity."
      }
    ]
  },
  {
    "LearningObjective": "AI-3.5.1",
    "LearningObjectiveDescription": "Summarize the concepts of underfitting and overfitting.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q125",
        "QuestionText": "Scenario: As an ML trainer, you observe a student's model showing consistently poor performance on both training and test data with simple linear patterns. What guidance should you provide?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "The model is overfitting - reduce complexity",
          "b": "The model is underfitting - increase complexity",
          "c": "There's sample bias - collect more diverse data",
          "d": "There's concept drift - update training data"
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>Correct! b) The model is underfitting - increase complexity</strong> because poor performance on both training and test data suggests <strong>underfitting</strong>, where the model is too simple to capture underlying patterns.<br><br><strong>Incorrect a) The model is overfitting - reduce complexity</strong> because overfitting shows high training accuracy but poor test performance.<br><strong>Incorrect c) There's sample bias - collect more diverse data</strong> because sample bias affects specific groups, not overall poor performance.<br><strong>Incorrect d) There's concept drift - update training data</strong> because concept drift shows performance degradation over time, not consistent poor performance."
      }
    ]
  },
  {
    "LearningObjective": "AI-4.2.1",
    "LearningObjectiveDescription": "Contrast the use of training, validation and test datasets in the development of an ML model.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q126",
        "QuestionText": "Scenario: You're leading an ML team where data scientists keep tweaking models based on test set performance, compromising its independence. What process should you implement?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Use only training and test datasets",
          "b": "Introduce a separate validation dataset for tuning",
          "c": "Combine all data into one dataset",
          "d": "Use production data for model evaluation"
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>Correct! b) Introduce a separate validation dataset for tuning</strong> because a <strong>validation dataset</strong> prevents data leakage by providing a separate set for model tuning while preserving the test set for final evaluation.<br><br><strong>Incorrect a) Use only training and test datasets</strong> because this would lead to overfitting to the test set.<br><strong>Incorrect c) Combine all data into one dataset</strong> because this eliminates the ability to properly evaluate generalization.<br><strong>Incorrect d) Use production data for model evaluation</strong> because production data should be used for monitoring, not development tuning."
      }
    ]
  },
  {
    "LearningObjective": "AI-4.2.1",
    "LearningObjectiveDescription": "Contrast the use of training, validation and test datasets in the development of an ML model.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q127",
        "QuestionText": "Scenario: As an ML Ops engineer with limited data, you need to maximize both model training and reliable evaluation. Which data splitting strategy is most appropriate?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "80% training, 20% test",
          "b": "60% training, 20% validation, 20% test",
          "c": "K-fold cross-validation",
          "d": "100% training with synthetic test data"
        },
        "CorrectAnswer": "c",
        "Explanation": "<strong>Correct! c) K-fold cross-validation</strong> because with limited data, <strong>K-fold cross-validation</strong> maximizes data usage for both training and validation while providing reliable performance estimates.<br><br><strong>Incorrect a) 80% training, 20% test</strong> because this wastes valuable data that could be used for training.<br><strong>Incorrect b) 60% training, 20% validation, 20% test</strong> because this still leaves significant data unused for training.<br><strong>Incorrect d) 100% training with synthetic test data</strong> because synthetic data may not represent real-world patterns accurately."
      }
    ]
  },
  {
    "LearningObjective": "AI-4.5.1",
    "LearningObjectiveDescription": "Recall the different approaches to the labelling of data in datasets for supervised learning.",
    "KnowledgeType": "K1",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q128",
        "QuestionText": "Scenario: You're managing a medical imaging project requiring annotation of 100,000 X-ray images. Budget is limited but quality is critical. Which labeling approach offers the best balance?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Internal medical staff labeling",
          "b": "Outsourced specialized company",
          "c": "Crowdsourced platform with medical qualification checks",
          "d": "AI-assisted labeling with radiologist verification"
        },
        "CorrectAnswer": "d",
        "Explanation": "<strong>Correct! d) AI-assisted labeling with radiologist verification</strong> because <strong>AI-assisted labeling with expert verification</strong> provides cost efficiency while maintaining the required medical accuracy.<br><br><strong>Incorrect a) Internal medical staff labeling</strong> because this would be too expensive and time-consuming for 100,000 images.<br><strong>Incorrect b) Outsourced specialized company</strong> because while specialized, this may still be expensive for the budget.<br><strong>Incorrect c) Crowdsourced platform with medical qualification checks</strong> because crowdsourcing may not provide the required medical expertise consistently."
      }
    ]
  },
  {
    "LearningObjective": "AI-4.5.1",
    "LearningObjectiveDescription": "Recall the different approaches to the labelling of data in datasets for supervised learning.",
    "KnowledgeType": "K1",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q129",
        "QuestionText": "Scenario: As a startup founder with limited resources, you need to label customer feedback data quickly and cost-effectively for sentiment analysis. Which approach is most feasible?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Hire internal data annotators",
          "b": "Contract a specialized labeling company",
          "c": "Use crowdsourcing platforms",
          "d": "Implement fully automated AI labeling"
        },
        "CorrectAnswer": "c",
        "Explanation": "<strong>Correct! c) Use crowdsourcing platforms</strong> because <strong>crowdsourcing</strong> offers the best balance of speed and cost-effectiveness for non-critical labeling tasks like sentiment analysis.<br><br><strong>Incorrect a) Hire internal data annotators</strong> because this is expensive for a startup with limited resources.<br><strong>Incorrect b) Contract a specialized labeling company</strong> because this is typically more expensive than crowdsourcing.<br><strong>Incorrect d) Implement fully automated AI labeling</strong> because fully automated labeling may not be accurate enough without human verification."
      }
    ]
  },
  {
    "LearningObjective": "AI-5.4.1",
    "LearningObjectiveDescription": "Select appropriate ML functional performance metrics and/or their values for a given ML model and scenario.",
    "KnowledgeType": "K4",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q130",
        "QuestionText": "Scenario: You're a Test Analyst evaluating a cancer detection AI. The confusion matrix shows: TP=95, FN=5, FP=15, TN=885. What is the most critical metric for patient safety in this context?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Accuracy = 98%",
          "b": "Precision = 86.4%",
          "c": "Recall = 95%",
          "d": "F1-score = 90.5%"
        },
        "CorrectAnswer": "c",
        "Explanation": "<strong>Correct! c) Recall = 95%</strong> because in medical diagnosis, <strong>recall</strong> (minimizing false negatives) is most critical to avoid missing actual cancer cases.<br><br><strong>Incorrect a) Accuracy = 98%</strong> because accuracy can be misleading and doesn't specifically address false negatives.<br><strong>Incorrect b) Precision = 86.4%</strong> because precision focuses on avoiding false positives, which is less critical than missing cancer cases.<br><strong>Incorrect d) F1-score = 90.5%</strong> because F1-score balances precision and recall, but doesn't specifically maximize recall for patient safety."
      }
    ]
  },
  {
    "LearningObjective": "AI-5.4.1",
    "LearningObjectiveDescription": "Select appropriate ML functional performance metrics and/or their values for a given ML model and scenario.",
    "KnowledgeType": "K4",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q131",
        "QuestionText": "Scenario: As a Quality Manager for a spam filter, your priority is minimizing legitimate emails marked as spam. Given: TP=800, FN=50, FP=20, TN=4130. Which metric reflects this priority?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Accuracy = 98.6%",
          "b": "Precision = 97.6%",
          "c": "Recall = 94.1%",
          "d": "Specificity = 99.5%"
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>Correct! b) Precision = 97.6%</strong> because <strong>precision</strong> measures the proportion of correctly identified spam, directly reflecting the goal of minimizing false positives (legitimate emails marked as spam).<br><br><strong>Incorrect a) Accuracy = 98.6%</strong> because accuracy includes true negatives and doesn't specifically measure false positive reduction.<br><strong>Incorrect c) Recall = 94.1%</strong> because recall measures spam detection rate, not false positive minimization.<br><strong>Incorrect d) Specificity = 99.5%</strong> because specificity measures true negative rate, but precision directly addresses the business requirement."
      }
    ]
  },
  {
    "LearningObjective": "AI-5.2.1",
    "LearningObjectiveDescription": "Contrast and compare the concepts behind the ML functional performance metrics for classification, regression and clustering methods.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q132",
        "QuestionText": "Scenario: You are a data scientist at a healthcare company developing a model to predict patient readmission risk. The model outputs probabilities, and you need to evaluate its performance across different thresholds. You generate the following ROC curve data: Threshold: 0.1 - TPR: 0.95, FPR: 0.85 Threshold: 0.3 - TPR: 0.85, FPR: 0.45 Threshold: 0.5 - TPR: 0.70, FPR: 0.20 Threshold: 0.7 - TPR: 0.50, FPR: 0.10 Threshold: 0.9 - TPR: 0.20, FPR: 0.02 What is the AUC (Area Under the Curve) approximately, and what does it indicate about the model?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "0.50 - The model is no better than random guessing",
          "b": "0.65 - The model has poor discriminatory power",
          "c": "0.75 - The model has reasonable discriminatory power",
          "d": "0.85 - The model has good discriminatory power"
        },
        "CorrectAnswer": "c",
        "Explanation": "<strong>Correct! c) 0.75 - The model has reasonable discriminatory power</strong> because AUC can be estimated by integrating the TPR over FPR. The area under these points is roughly 0.75, indicating reasonable but not excellent performance. AUC of 0.75 means the model can distinguish between positive and negative classes reasonably well.<br><br><strong>Incorrect a) 0.50 - The model is no better than random guessing</strong> because 0.50 indicates random performance, but the data shows better discrimination.<br><strong>Incorrect b) 0.65 - The model has poor discriminatory power</strong> because 0.65 is below the reasonable threshold and doesn't match the calculated area.<br><strong>Incorrect d) 0.85 - The model has good discriminatory power</strong> because the data points don't support an area of 0.85."
      }
    ]
  },
  {
    "LearningObjective": "AI-5.4.1",
    "LearningObjectiveDescription": "Select appropriate ML functional performance metrics and/or their values for a given ML model and scenario.",
    "KnowledgeType": "K4",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q133",
        "QuestionText": "Scenario: As a ML engineer for a fraud detection system, you have to choose the primary metric for model selection. The business context requires that: - False positives (legitimate transactions flagged as fraud) are costly due to customer inconvenience and support calls - False negatives (missed fraud) are very costly due to financial losses - The dataset has 99% legitimate transactions and 1% fraudulent transactions Which metric should be prioritized to balance these concerns?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Accuracy",
          "b": "Precision",
          "c": "Recall",
          "d": "F1-score"
        },
        "CorrectAnswer": "d",
        "Explanation": "<strong>Correct! d) F1-score</strong> because in imbalanced datasets where both false positives and false negatives are costly, <strong>F1-score</strong> (the harmonic mean of precision and recall) provides a balance. Accuracy would be misleading due to class imbalance, and prioritizing only precision or recall would neglect one type of error.<br><br><strong>Incorrect a) Accuracy</strong> because accuracy would be misleading due to class imbalance (99% legitimate vs 1% fraudulent).<br><strong>Incorrect b) Precision</strong> because focusing only on precision would neglect the cost of false negatives (missed fraud).<br><strong>Incorrect c) Recall</strong> because focusing only on recall would neglect the cost of false positives (customer inconvenience)."
      }
    ]
  },
  {
    "LearningObjective": "AI-5.4.1",
    "LearningObjectiveDescription": "Select appropriate ML functional performance metrics and/or their values for a given ML model and scenario.",
    "KnowledgeType": "K4",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q134",
        "QuestionText": "Scenario: You are evaluating two models for a cancer screening test. The cost of a false negative (missing cancer) is 100 times higher than a false positive (false alarm). The models have the following performance: Model A: Precision = 0.85, Recall = 0.70 Model B: Precision = 0.75, Recall = 0.90 Which model should you choose based on cost sensitivity?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Model A because of higher precision",
          "b": "Model B because of higher recall",
          "c": "Model A because of higher F1-score",
          "d": "Model B because recall is more important given costs"
        },
        "CorrectAnswer": "d",
        "Explanation": "<strong>Correct! d) Model B because recall is more important given costs</strong> because since false negatives are 100 times more costly, <strong>recall</strong> (which minimizes false negatives) should be prioritized. Model B has higher recall (0.90 vs. 0.70), making it more suitable despite lower precision.<br><br><strong>Incorrect a) Model A because of higher precision</strong> because precision reduces false positives, but false negatives are much more costly in this scenario.<br><strong>Incorrect b) Model B because of higher recall</strong> because while correct, this doesn't explicitly reference the cost sensitivity which is crucial.<br><strong>Incorrect c) Model A because of higher F1-score</strong> because F1-score doesn't account for the 100:1 cost ratio between false negatives and false positives."
      }
    ]
  },
  {
    "LearningObjective": "AI-7.4.1",
    "LearningObjectiveDescription": "Explain automation bias and how this affects testing.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q135",
        "QuestionText": "Scenario: You are testing an AI-based financial advisory system that recommends investment portfolios. During user testing, you observe that users consistently accept the AI's recommendations without reviewing alternative options, even when the AI's confidence is low. What is this phenomenon called, and how should you address it in testing?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Automation bias; test the system's ability to flag low-confidence recommendations and ensure users are alerted",
          "b": "Sample bias; increase the diversity of training data",
          "c": "Algorithmic bias; retrain the model with more data",
          "d": "Confirmation bias; add more explanatory features"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>Correct! a) Automation bias; test the system's ability to flag low-confidence recommendations and ensure users are alerted</strong> because <strong>automation bias</strong> occurs when users over-rely on automated systems. Testing should include scenarios where the system has low confidence, and verify that users are prompted to review decisions or seek additional information.<br><br><strong>Incorrect b) Sample bias; increase the diversity of training data</strong> because sample bias relates to unrepresentative training data, not user over-reliance.<br><strong>Incorrect c) Algorithmic bias; retrain the model with more data</strong> because algorithmic bias comes from the model itself, not user behavior.<br><strong>Incorrect d) Confirmation bias; add more explanatory features</strong> because confirmation bias is a human cognitive bias, not specifically related to automation over-reliance."
      }
    ]
  },
  {
    "LearningObjective": "AI-7.4.1",
    "LearningObjectiveDescription": "Explain automation bias and how this affects testing.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q136",
        "QuestionText": "Scenario: As a test manager for an autonomous vehicle company, you are concerned about drivers becoming complacent and not monitoring the system adequately during long trips. What type of testing should you implement to evaluate this risk?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Performance testing under heavy load",
          "b": "Usability testing with simulated system failures",
          "c": "Security testing for adversarial attacks",
          "d": "Accuracy testing on validation data"
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>Correct! b) Usability testing with simulated system failures</strong> because to test for automation bias in the form of complacency, <strong>simulated failures</strong> can assess whether users remain vigilant and can take control when needed. This directly addresses the risk of over-reliance.<br><br><strong>Incorrect a) Performance testing under heavy load</strong> because performance testing measures system capabilities, not user vigilance.<br><strong>Incorrect c) Security testing for adversarial attacks</strong> because security testing focuses on system vulnerabilities, not user behavior.<br><strong>Incorrect d) Accuracy testing on validation data</strong> because accuracy testing validates model performance, not user complacency."
      }
    ]
  },
  {
    "LearningObjective": "AI-8.3.1",
    "LearningObjectiveDescription": "Explain how to test for bias in an AI-based system.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q137",
        "QuestionText": "Scenario: You are auditing a loan approval AI system for bias. The training data includes historical loan applications with outcomes, but demographic data was not collected. You suspect the model might be biased based on zip codes, which correlate with race. How can you test for inappropriate bias without direct demographic data?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Use proxy variables like zip code and income to analyze approval rates across groups",
          "b": "Ignore bias testing since demographic data is missing",
          "c": "Test only for algorithmic bias in the model code",
          "d": "Retrain the model with synthetic demographic data"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>Correct! a) Use proxy variables like zip code and income to analyze approval rates across groups</strong> because even without direct demographic data, <strong>proxy variables</strong> (e.g., zip code, which can correlate with race) can be used to identify potential disparate impact. Statistical analysis of approval rates across these proxies can reveal bias.<br><br><strong>Incorrect b) Ignore bias testing since demographic data is missing</strong> because regulatory requirements often mandate bias testing even without direct demographic data.<br><strong>Incorrect c) Test only for algorithmic bias in the model code</strong> because algorithmic bias analysis alone doesn't address disparate impact through proxy variables.<br><strong>Incorrect d) Retrain the model with synthetic demographic data</strong> because synthetic data may not accurately represent real-world correlations and patterns."
      }
    ]
  },
  {
    "LearningObjective": "AI-8.3.1",
    "LearningObjectiveDescription": "Explain how to test for bias in an AI-based system.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q138",
        "QuestionText": "Scenario: You are testing a facial recognition system for a security company. The system was trained on a dataset primarily composed of light-skinned individuals. You want to evaluate its performance across different skin tones. What is the best approach to test for sample bias?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Measure overall accuracy on a held-out test set",
          "b": "Use a diverse test dataset with balanced representation of skin tones",
          "c": "Analyze the model's confusion matrix for the training set",
          "d": "Inspect the model's hyperparameters for fairness constraints"
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>Correct! b) Use a diverse test dataset with balanced representation of skin tones</strong> because to test for <strong>sample bias</strong>, the test dataset must be representative of the population the system will face in production. A diverse test set with balanced skin tones can reveal performance disparities across groups.<br><br><strong>Incorrect a) Measure overall accuracy on a held-out test set</strong> because overall accuracy doesn't reveal performance differences across demographic groups.<br><strong>Incorrect c) Analyze the model's confusion matrix for the training set</strong> because training set performance doesn't indicate generalization to underrepresented groups.<br><strong>Incorrect d) Inspect the model's hyperparameters for fairness constraints</strong> because hyperparameters control model training, not bias detection in deployment."
      }
    ]
  },
  {
    "LearningObjective": "AI-8.6.1",
    "LearningObjectiveDescription": "Describe how the transparency, interpretability and explainability of AI-based systems can be tested.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q139",
        "QuestionText": "Scenario: You are working on a credit scoring AI that uses a complex ensemble model. Regulators require explanations for individual credit decisions. You decide to use LIME (Local Interpretable Model-Agnostic Explanations). How does LIME help in testing explainability?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "It provides global feature importance for the entire model",
          "b": "It trains a simpler, interpretable model that approximates the complex model locally",
          "c": "It reduces model complexity to improve transparency",
          "d": "It generates synthetic data to test model robustness"
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>Correct! b) It trains a simpler, interpretable model that approximates the complex model locally</strong> because <strong>LIME</strong> works by perturbing input data and observing changes in predictions to build a local, interpretable model (e.g., linear model) that explains individual predictions. This is model-agnostic and useful for testing explainability.<br><br><strong>Incorrect a) It provides global feature importance for the entire model</strong> because LIME provides local explanations, not global feature importance.<br><strong>Incorrect c) It reduces model complexity to improve transparency</strong> because LIME doesn't modify the original model's complexity.<br><strong>Incorrect d) It generates synthetic data to test model robustness</strong> because LIME uses perturbations for explanation, not robustness testing."
      }
    ]
  },
  {
    "LearningObjective": "AI-8.6.1",
    "LearningObjectiveDescription": "Describe how the transparency, interpretability and explainability of AI-based systems can be tested.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q140",
        "QuestionText": "Scenario: You are evaluating the transparency of a third-party AI model used for medical diagnosis. The vendor provides documentation including the algorithm type, training data sources, and hyperparameters. What should you verify to assess transparency?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "That the model achieves high accuracy on test data",
          "b": "That the documentation matches the actual implementation and data used",
          "c": "That the model can be explained to patients in simple terms",
          "d": "That the model has low computational costs"
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>Correct! b) That the documentation matches the actual implementation and data used</strong> because <strong>transparency</strong> refers to the visibility of the algorithm and data. Testing involves verifying that the documented information (algorithm, data sources, etc.) accurately reflects what was used in development and deployment.<br><br><strong>Incorrect a) That the model achieves high accuracy on test data</strong> because accuracy relates to performance, not transparency.<br><strong>Incorrect c) That the model can be explained to patients in simple terms</strong> because this relates to explainability, not transparency.<br><strong>Incorrect d) That the model has low computational costs</strong> because computational costs relate to performance efficiency, not transparency."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.2.1",
    "LearningObjectiveDescription": "Explain how pairwise testing is used for AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q141",
        "QuestionText": "Scenario: You are testing an AI-based autonomous drone system that must navigate urban environments. The system has multiple parameters: weather conditions (sunny, rainy, foggy), time of day (day, night), obstacle density (low, medium, high), and drone speed (slow, medium, fast). Using pairwise testing, what is the minimum number of test cases required to cover all pairs of parameters?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "6 test cases",
          "b": "9 test cases",
          "c": "12 test cases",
          "d": "18 test cases"
        },
        "CorrectAnswer": "c",
        "Explanation": "<strong>Correct! c) 12 test cases</strong> because with parameters having 3, 2, 3, and 3 values respectively, the number of possible combinations is 3×2×3×3=54. <strong>Pairwise testing</strong> reduces this significantly. Using a pairwise tool, the minimum number of test cases to cover all pairs is typically around 12 for such parameters.<br><br><strong>Incorrect a) 6 test cases</strong> because 6 test cases would be insufficient to cover all pairwise combinations for these parameters.<br><strong>Incorrect b) 9 test cases</strong> because 9 test cases would still miss some important pairwise combinations.<br><strong>Incorrect d) 18 test cases</strong> because 18 test cases would be more than necessary for pairwise coverage."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.2.1",
    "LearningObjectiveDescription": "Explain how pairwise testing is used for AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q142",
        "QuestionText": "Scenario: You are using pairwise testing for a system with three parameters: A (values A1, A2), B (B1, B2), C (C1, C2, C3). What is the minimum number of test cases required to cover all pairs?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "4 test cases",
          "b": "6 test cases",
          "c": "8 test cases",
          "d": "12 test cases"
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>Correct! b) 6 test cases</strong> because for parameters with 2,2,3 values, the number of possible combinations is 2×2×3=12. <strong>Pairwise testing</strong> can cover all pairs with 6 test cases. For example, using a pairwise tool, we can generate test cases that cover all pairs between A-B, A-C, and B-C.<br><br><strong>Incorrect a) 4 test cases</strong> because 4 test cases would be insufficient to cover all pairwise combinations.<br><strong>Incorrect c) 8 test cases</strong> because 8 test cases would be more than necessary for pairwise coverage.<br><strong>Incorrect d) 12 test cases</strong> because 12 test cases would be exhaustive testing, not pairwise optimized."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.2.1",
    "LearningObjectiveDescription": "Explain how pairwise testing is used for AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q143",
        "QuestionText": "Scenario: You are testing an autonomous vehicle perception system with the following parameters and values: - Weather: Sunny, Rainy, Foggy - Time of Day: Day, Night - Road Type: Highway, Urban, Rural - Traffic Density: Low, Medium, High - Vehicle Speed: Slow (0-30 mph), Medium (31-60 mph), Fast (61+ mph) Question: Using the formula for pairwise testing, what is the theoretical minimum number of test cases needed to cover all pairs of parameters?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "15 test cases",
          "b": "25 test cases",
          "c": "35 test cases",
          "d": "50 test cases"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>Correct! a) 15 test cases</strong> because for pairwise testing with parameters having 3, 2, 3, 3, and 3 values respectively, the minimum number of test cases can be calculated using combinatorial testing algorithms. The maximum number of values for any parameter is 3, and with 5 parameters, the theoretical minimum is typically around 9-15 test cases. Using pairwise testing tools, 15 test cases would be sufficient to cover all pairs.<br><br><strong>Incorrect b) 25 test cases</strong> because 25 would be excessive for pairwise coverage of these parameters.<br><strong>Incorrect c) 35 test cases</strong> because 35 approaches exhaustive testing and is unnecessary for pairwise coverage.<br><strong>Incorrect d) 50 test cases</strong> because 50 would be complete exhaustive testing, not optimized pairwise testing."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.5.1",
    "LearningObjectiveDescription": "Apply metamorphic testing for the testing of AI-based systems.",
    "KnowledgeType": "K3",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q144",
        "QuestionText": "Scenario: You are testing a financial fraud detection system. For a source test case where transaction amount = $1,000 is correctly flagged as suspicious, you apply the following metamorphic relations: MR1: If amount is doubled, suspicion score should at least double MR2: If amount is halved, suspicion score should at most halve MR3: If transaction time is changed from business hours to midnight, suspicion score should increase Question: Which MR is testing for consistency with business rules about transaction timing?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "MR1 only",
          "b": "MR2 only",
          "c": "MR3 only",
          "d": "MR1 and MR2"
        },
        "CorrectAnswer": "c",
        "Explanation": "<strong>Correct! c) MR3 only</strong> because <strong>MR3</strong> specifically tests the business rule that transactions at unusual hours (midnight) should be more suspicious, regardless of the amount. MR1 and MR2 test amount-based relationships.<br><br><strong>Incorrect a) MR1 only</strong> because MR1 tests amount doubling relationships, not timing-based business rules.<br><strong>Incorrect b) MR2 only</strong> because MR2 tests amount halving relationships, not timing-based suspiciousness.<br><strong>Incorrect d) MR1 and MR2</strong> because both MR1 and MR2 focus on amount relationships, not transaction timing."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.5.1",
    "LearningObjectiveDescription": "Apply metamorphic testing for the testing of AI-based systems.",
    "KnowledgeType": "K3",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q145",
        "QuestionText": "Scenario: You are testing a route optimization AI for delivery trucks. For a source test case with delivery points A→B→C taking 45 minutes, you apply MR: \"If point D is added between B and C, the total time should be greater than or equal to 45 minutes.\" Question: What is the expected result when you add point D, and the system reports total time = 40 minutes?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Test passed - the system found a more efficient route",
          "b": "Test failed - violation of metamorphic relation",
          "c": "Test inconclusive - need more data",
          "d": "Test passed - within acceptable margin"
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>Correct! b) Test failed - violation of metamorphic relation</strong> because adding an additional delivery point should never decrease the total travel time (unless there's a fundamentally different route). The MR states time should be ≥ 45 minutes, so 40 minutes violates this relation, indicating a potential defect.<br><br><strong>Incorrect a) Test passed - the system found a more efficient route</strong> because adding a point cannot logically decrease travel time without indicating an error in route optimization.<br><strong>Incorrect c) Test inconclusive - need more data</strong> because the MR violation is clear and indicates a potential defect.<br><strong>Incorrect d) Test passed - within acceptable margin</strong> because a 5-minute decrease when adding a point is not acceptable and violates the fundamental MR."
      }
    ]
  },
  {
    "LearningObjective": "HO-9.6.1",
    "LearningObjectiveDescription": "Apply exploratory testing to an AI-based system.",
    "KnowledgeType": "H2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q146",
        "QuestionText": "Scenario: During exploratory testing of a recommendation system, you use the \"money tour\" strategy focusing on premium users and high-value transactions. Question: What is the primary goal of a \"money tour\" in exploratory testing?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "To test the system's payment processing capabilities",
          "b": "To focus on high-risk, high-value functionality",
          "c": "To verify financial calculations are accurate",
          "d": "To test the system with large monetary values"
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>Correct! b) To focus on high-risk, high-value functionality</strong> because the <strong>'money tour'</strong> in exploratory testing focuses on functionality that has the highest business value or poses the highest risk if it fails, ensuring critical business processes work correctly.<br><br><strong>Incorrect a) To test the system's payment processing capabilities</strong> because while payment processing might be part of it, the money tour encompasses all high-value functionality.<br><strong>Incorrect c) To verify financial calculations are accurate</strong> because this is too specific - the money tour covers all critical business functions.<br><strong>Incorrect d) To test the system with large monetary values</strong> because the money tour focuses on business-critical functions, not necessarily large transaction values."
      }
    ]
  },
  {
    "LearningObjective": "AI-4.3.1",
    "LearningObjectiveDescription": "Describe typical dataset quality issues.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q147",
        "QuestionText": "Scenario: You are performing exploratory data analysis on a customer churn dataset and discover the following distribution of customer tenure before churning: Tenure (months) | % of Churned Customers 0-3 | 45% 4-6 | 25% 7-12 | 15% 13-24 | 10% 25+ | 5% Question: What key insight does this provide for testing the churn prediction model?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "The model should be optimized for long-term customers",
          "b": "Early detection (0-6 months) is most critical",
          "c": "The dataset is balanced across tenure periods",
          "d": "Long-term customers have higher churn risk"
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>Correct! b) Early detection (0-6 months) is most critical</strong> because 70% of churn occurs within the first 6 months, so testing should focus on the model's ability to detect early churn signals, and the model should be optimized for recall in this critical period.<br><br><strong>Incorrect a) The model should be optimized for long-term customers</strong> because only 15% of churn occurs after 12 months, making this less critical.<br><strong>Incorrect c) The dataset is balanced across tenure periods</strong> because the data shows clear imbalance with 70% concentration in early months.<br><strong>Incorrect d) Long-term customers have higher churn risk</strong> because the data shows the opposite - early tenure has much higher churn risk."
      }
    ]
  },
  {
    "LearningObjective": "AI-5.1.1",
    "LearningObjectiveDescription": "Calculate the ML functional performance metrics from a given set of confusion matrix data.",
    "KnowledgeType": "K3",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q148",
        "QuestionText": "Scenario: You are evaluating a multi-class classification model for sentiment analysis with the following confusion matrix: Actual Positive predicted Positive: 150 Actual Positive predicted Neutral: 20 Actual Positive predicted Negative: 10 Actual Neutral predicted Positive: 15 Actual Neutral predicted Neutral: 200 Actual Neutral predicted Negative: 25 Actual Negative predicted Positive: 5 Actual Negative predicted Neutral: 15 Actual Negative predicted Negative: 180 Question: What is the precision for the \"Negative\" class?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "0.84",
          "b": "0.86",
          "c": "0.88",
          "d": "0.90"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>Correct! a) 0.84</strong> because for Negative class: TP = 180, FP = (10 + 25) = 35 (from Positive and Neutral predicted as Negative). Precision = TP / (TP + FP) = 180 / (180 + 35) = 180/215 ≈ 0.837 ≈ 0.84<br><br><strong>Incorrect b) 0.86</strong> because this doesn't match the calculated precision value.<br><strong>Incorrect c) 0.88</strong> because this overestimates the precision based on the confusion matrix.<br><strong>Incorrect d) 0.90</strong> because this would require fewer false positives than shown in the matrix."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.2.1",
    "LearningObjectiveDescription": "Explain how pairwise testing is used for AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q149",
        "QuestionText": "Scenario: You are testing a financial trading algorithm with the following parameters: - Market Condition: Bull, Bear, Sideways - Volatility: Low, Medium, High - Time of Day: Open, Midday, Close - Trade Size: Small, Medium, Large Question: If you've already executed 12 pairwise test cases, what testing strategy should you use for additional coverage?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Execute all remaining 78 possible combinations",
          "b": "Use orthogonal arrays for 3-way combinations",
          "c": "Focus on boundary value analysis for numeric parameters",
          "d": "Use error guessing for high-risk scenarios"
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>Correct! b) Use orthogonal arrays for 3-way combinations</strong> because after achieving pairwise coverage, moving to <strong>3-way combinations</strong> using orthogonal arrays can provide additional defect detection for higher-order interactions while still being more efficient than exhaustive testing.<br><br><strong>Incorrect a) Execute all remaining 78 possible combinations</strong> because exhaustive testing is inefficient and unnecessary after pairwise coverage.<br><strong>Incorrect c) Focus on boundary value analysis for numeric parameters</strong> because boundary testing doesn't address combinatorial interactions.<br><strong>Incorrect d) Use error guessing for high-risk scenarios</strong> because error guessing is unstructured and may miss systematic issues."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.5.1",
    "LearningObjectiveDescription": "Apply metamorphic testing for the testing of AI-based systems.",
    "KnowledgeType": "K3",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q150",
        "QuestionText": "Scenario: You are testing a recommendation system using metamorphic testing. For a user who likes movies A, B, and C, the system recommends X, Y, and Z. You apply MR: \"If the user's preference for movie A increases, movie X should move higher in the recommendations.\" Question: What is the expected result when you increase the rating for movie A from 4 to 5 stars, and movie X moves from position 3 to position 1?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Test passed - the MR is satisfied",
          "b": "Test failed - the MR is violated",
          "c": "Test inconclusive - need more users",
          "d": "Test passed - but only if statistical significance is achieved"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>Correct! a) Test passed - the MR is satisfied</strong> because the MR states that increased preference for A should increase the ranking of X. Since X moved from position 3 to 1 (higher ranking), the MR is satisfied.<br><br><strong>Incorrect b) Test failed - the MR is violated</strong> because the MR is clearly satisfied with the improved ranking.<br><strong>Incorrect c) Test inconclusive - need more users</strong> because the MR test result is definitive for this specific case.<br><strong>Incorrect d) Test passed - but only if statistical significance is achieved</strong> because MR testing doesn't require statistical significance for individual test cases."
      }
    ]
  },
  {
    "LearningObjective": "AI-4.3.1",
    "LearningObjectiveDescription": "Describe typical dataset quality issues.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q151",
        "QuestionText": "Scenario: You are performing exploratory data analysis on a dataset for a credit scoring model and create the following correlation matrix: Feature | Income | Age | Credit History | Default Rate Income | 1.00 | 0.25 | 0.40 | -0.35 Age | 0.25 | 1.00 | 0.15 | -0.20 Credit History | 0.40 | 0.15 | 1.00 | -0.45 Default Rate | -0.35 | -0.20 | -0.45 | 1.00 Question: What key insight does this provide for testing?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Credit History is the strongest predictor of default",
          "b": "Income and Age are highly correlated, suggesting multicollinearity",
          "c": "All features have weak correlations with default",
          "d": "The model will likely overfit due to high correlations"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>Correct! a) Credit History is the strongest predictor of default</strong> because Credit History has the strongest negative correlation with Default Rate (-0.45), indicating it's the most important feature. Testing should focus on how the model uses this feature and whether it properly weights its importance.<br><br><strong>Incorrect b) Income and Age are highly correlated, suggesting multicollinearity</strong> because the correlation between Income and Age (0.25) is moderate, not indicating severe multicollinearity.<br><strong>Incorrect c) All features have weak correlations with default</strong> because Credit History has a strong correlation (-0.45).<br><strong>Incorrect d) The model will likely overfit due to high correlations</strong> because the correlations are not high enough to guarantee overfitting."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.7.1",
    "LearningObjectiveDescription": "For a given scenario, select appropriate test techniques when testing an AI-based system.",
    "KnowledgeType": "K4",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q152",
        "QuestionText": "Scenario: You are selecting test techniques for a self-learning customer service chatbot that handles sensitive financial information. Question: Which combination of techniques addresses the highest risks?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "A/B testing for conversation quality and security testing for data protection",
          "b": "Performance testing for response time and usability testing for user experience",
          "c": "Metamorphic testing for conversation consistency and bias testing for fair treatment",
          "d": "Back-to-back testing with human agents and exploratory testing for edge cases"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>Correct! a) A/B testing for conversation quality and security testing for data protection</strong> because for a financial chatbot, conversation quality (A/B testing) and data security (security testing) are the highest risks, addressing both functional quality and protection of sensitive information.<br><br><strong>Incorrect b) Performance testing for response time and usability testing for user experience</strong> because while important, these don't address the critical financial data protection risks.<br><strong>Incorrect c) Metamorphic testing for conversation consistency and bias testing for fair treatment</strong> because these address quality aspects but not the paramount security concerns.<br><strong>Incorrect d) Back-to-back testing with human agents and exploratory testing for edge cases</strong> because these don't specifically address security and data protection."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.7.1",
    "LearningObjectiveDescription": "For a given scenario, select appropriate test techniques when testing an AI-based system.",
    "KnowledgeType": "K4",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q153",
        "QuestionText": "Scenario: You are designing a test strategy for an AI-based medical diagnosis system that will be deployed in multiple countries with different healthcare regulations. Question: Which test techniques are most important for ensuring global compliance?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Localization testing and regulatory compliance testing",
          "b": "Performance testing and load testing",
          "c": "Unit testing and integration testing",
          "d": "Security testing and penetration testing"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>Correct! a) Localization testing and regulatory compliance testing</strong> because for global deployment, <strong>localization testing</strong> ensures proper functioning in different regions, and <strong>regulatory compliance testing</strong> ensures adherence to varying healthcare regulations across countries.<br><br><strong>Incorrect b) Performance testing and load testing</strong> because these address technical performance, not international compliance.<br><strong>Incorrect c) Unit testing and integration testing</strong> because these are development-phase tests, not deployment-phase international compliance tests.<br><strong>Incorrect d) Security testing and penetration testing</strong> because while security is important, it doesn't address localization and regulatory variations."
      }
    ]
  },
  {
    "LearningObjective": "AI-5.1.1",
    "LearningObjectiveDescription": "Calculate the ML functional performance metrics from a given set of confusion matrix data.",
    "KnowledgeType": "K3",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q154",
        "QuestionText": "Scenario: You are evaluating a multi-class classification model with the following normalized confusion matrix: Actual \\\\ Predicted | Class A | Class B | Class C Class A | 0.85 | 0.10 | 0.05 Class B | 0.15 | 0.75 | 0.10 Class C | 0.05 | 0.15 | 0.80 Question: What is the overall accuracy of the model?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "0.75",
          "b": "0.80",
          "c": "0.85",
          "d": "0.90"
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>Correct! b) 0.80</strong> because overall accuracy is the sum of the diagonal divided by total: (0.85 + 0.75 + 0.80) / 3 = 2.40 / 3 = 0.80<br><br><strong>Incorrect a) 0.75</strong> because this underestimates the calculated accuracy.<br><strong>Incorrect c) 0.85</strong> because this would be the accuracy if only considering Class A performance.<br><strong>Incorrect d) 0.90</strong> because this overestimates the overall model performance."
      }
    ]
  },
  {
    "LearningObjective": "AI-5.4.1",
    "LearningObjectiveDescription": "Select appropriate ML functional performance metrics and/or their values for a given ML model and scenario.",
    "KnowledgeType": "K4",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q155",
        "QuestionText": "Scenario: You have a binary classifier with the following cost matrix: Actual \\\\ Predicted | Positive | Negative Positive | 0 (TP) | 50 (FN) Negative | 10 (FP) | 0 (TN) Question: If the model has Precision = 0.80 and Recall = 0.90, what is the expected cost per 1000 predictions assuming a positive prevalence of 20%?",
        "AnswerOption": "Select ONE option. (1 out of 5)",
        "Answers": {
          "a": "1,000",
          "b": "1,450",
          "c": "1,900",
          "d": "2,800",
          "e": "3,500"
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>Correct! b) 1,450</strong> because Total Cost = FN cost (20 instances × 50 cost/instance = 1,000) + FP cost (45 instances × 10 cost/instance = 450) = 1,450<br><br><strong>Incorrect a) 1,000</strong> because this only accounts for false negatives, ignoring false positive costs.<br><strong>Incorrect c) 1,900</strong> because this overestimates the total cost calculation.<br><strong>Incorrect d) 2,800</strong> because this significantly overestimates based on incorrect calculations.<br><strong>Incorrect e) 3,500</strong> because this is far beyond the actual calculated cost."
      }
    ]
  },
  {
    "LearningObjective": "AI-4.1.1",
    "LearningObjectiveDescription": "Describe the activities and challenges related to data preparation.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q156",
        "QuestionText": "Scenario: You're performing feature engineering on a dataset for house price prediction. The dataset contains: square_footage, num_bedrooms, num_bathrooms, zip_code, year_built. Question: Which feature engineering technique would be most beneficial for improving model performance?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "One-hot encoding for zip_code",
          "b": "Creating a new feature: \"price_per_square_foot\"",
          "c": "Normalizing all numerical features",
          "d": "Removing year_built as it's correlated with other features"
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>Correct! b) Creating a new feature: 'price_per_square_foot'</strong> because creating derived features like <strong>'price_per_square_foot'</strong> can capture important relationships that raw features might miss, often leading to better model performance than just encoding or normalizing existing features.<br><br><strong>Incorrect a) One-hot encoding for zip_code</strong> because while encoding is necessary, it doesn't create new informative features.<br><strong>Incorrect c) Normalizing all numerical features</strong> because normalization helps training but doesn't create new feature relationships.<br><strong>Incorrect d) Removing year_built as it's correlated with other features</strong> because removing features reduces information rather than creating new insights."
      }
    ]
  },
  {
    "LearningObjective": "HO-8.6.1",
    "LearningObjectiveDescription": "Use a tool to show how explainability can be used by testers.",
    "KnowledgeType": "H2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q157",
        "QuestionText": "Scenario: You're testing model explainability using SHAP on a medical diagnosis AI. The SHAP summary plot shows: - Age: high positive impact for patients > 60 - Blood Pressure: mixed impact depending on values - Cholesterol: consistently negative impact when very high - Exercise Frequency: moderate positive impact Question: For a 65-year-old patient with high cholesterol but normal blood pressure and regular exercise, what would the explanation likely emphasize?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Age and exercise as positive factors",
          "b": "Cholesterol as the main concern with age as secondary",
          "c": "Blood pressure as the deciding factor",
          "d": "Exercise as the primary positive factor"
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>Correct! b) Cholesterol as the main concern with age as secondary</strong> because high cholesterol has consistently negative impact, and age > 60 has high positive impact for risk. The combination would likely emphasize cholesterol as the main concern, with age as an additional risk factor.<br><br><strong>Incorrect a) Age and exercise as positive factors</strong> because exercise is positive but age is actually a risk factor (positive impact means higher risk).<br><strong>Incorrect c) Blood pressure as the deciding factor</strong> because blood pressure has mixed impact and is normal in this case.<br><strong>Incorrect d) Exercise as the primary positive factor</strong> because exercise has only moderate positive impact compared to the strong negative impact of cholesterol."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.5.1",
    "LearningObjectiveDescription": "Apply metamorphic testing for the testing of AI-based systems.",
    "KnowledgeType": "K3",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q158",
        "QuestionText": "Scenario: You're applying metamorphic testing to a sentiment analysis system. For the source test case \"The movie was absolutely fantastic!\" which is correctly classified as \"positive\", you apply these MRs: MR1: Add positive words → \"The movie was absolutely fantastic and wonderful!\" MR2: Add negative words → \"The movie was absolutely fantastic but the acting was terrible\" MR3: Change intensity → \"The movie was quite good\" Question: Which MR is most likely to reveal a defect in the sentiment analysis?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "MR1 only",
          "b": "MR2 only",
          "c": "MR3 only",
          "d": "MR1 and MR3"
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>Correct! b) MR2 only</strong> because MR2 tests the system's ability to handle mixed sentiments, which is a common weakness in sentiment analysis systems. If it only focuses on the last part or doesn't properly weigh conflicting sentiments, this MR would reveal the defect.<br><br><strong>Incorrect a) MR1 only</strong> because MR1 (adding positive words) should maintain positive sentiment in a robust system.<br><strong>Incorrect c) MR3 only</strong> because MR3 (reducing intensity) might change sentiment strength but shouldn't flip the category.<br><strong>Incorrect d) MR1 and MR3</strong> because these test simpler transformations that most systems handle well."
      }
    ]
  },
  {
    "LearningObjective": "AI-11.5.1",
    "LearningObjectiveDescription": "Explain how AI can assist in defect prediction.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q159",
        "QuestionText": "Scenario: You're building a defect prediction system using historical data. The dataset includes: - Lines of code per module - Cyclomatic complexity - Number of developers who worked on the module - Number of previous defects - Code churn (frequency of changes) Question: Which features are most likely to be strong predictors based on research?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Lines of code and cyclomatic complexity",
          "b": "Number of previous defects and code churn",
          "c": "Number of developers and lines of code",
          "d": "Cyclomatic complexity and number of developers"
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>Correct! b) Number of previous defects and code churn</strong> because research shows that historical defect data and code churn (frequency of changes) are typically the strongest predictors, often outperforming static code metrics like lines of code or complexity.<br><br><strong>Incorrect a) Lines of code and cyclomatic complexity</strong> because while these measure complexity, they're often less predictive than historical defect data.<br><strong>Incorrect c) Number of developers and lines of code</strong> because number of developers alone isn't a strong predictor compared to actual change history.<br><strong>Incorrect d) Cyclomatic complexity and number of developers</strong> because these don't capture the dynamic aspects of code evolution that predict defects."
      }
    ]
  },
  {
    "LearningObjective": "AI-5.1.1",
    "LearningObjectiveDescription": "Calculate the ML functional performance metrics from a given set of confusion matrix data.",
    "KnowledgeType": "K3",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q160",
        "QuestionText": "Scenario: You've implemented a defect prediction model that outputs probabilities between 0 and 1. The confusion matrix at threshold 0.5 shows: Actual \\\\ Predicted | Defective | Not Defective Defective | 40 | 10 Not Defective | 15 | 135 Question: What is the precision of defect prediction?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "0.73",
          "b": "0.80",
          "c": "0.85",
          "d": "0.90"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>Correct! a) 0.73</strong> because Precision = TP / (TP + FP) = 40 / (40 + 15) = 40 / 55 ≈ 0.727 ≈ 0.73<br><br><strong>Incorrect b) 0.80</strong> because this would require fewer false positives (FP = 10 instead of 15).<br><strong>Incorrect c) 0.85</strong> because this significantly overestimates the precision.<br><strong>Incorrect d) 0.90</strong> because this would require very few false positives (FP = 4-5)."
      }
    ]
  },
  {
    "LearningObjective": "AI-4.1.1",
    "LearningObjectiveDescription": "Describe the activities and challenges related to data preparation.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q161",
        "QuestionText": "Scenario: You're performing data augmentation for an image classification model. The original dataset has 1,000 images of cats and dogs. Question: Which augmentation techniques would be most appropriate?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Rotation, flipping, color adjustment, random cropping",
          "b": "Adding Gaussian noise, changing image format, resizing",
          "c": "Converting to grayscale, edge detection, histogram equalization",
          "d": "Image segmentation, object detection, feature extraction"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>Correct! a) Rotation, flipping, color adjustment, random cropping</strong> because these are standard image augmentation techniques that create realistic variations while preserving the essential features needed for classification.<br><br><strong>Incorrect b) Adding Gaussian noise, changing image format, resizing</strong> because changing image format isn't a standard augmentation technique.<br><strong>Incorrect c) Converting to grayscale, edge detection, histogram equalization</strong> because these are feature extraction techniques, not data augmentation.<br><strong>Incorrect d) Image segmentation, object detection, feature extraction</strong> because these are computer vision tasks, not data augmentation methods."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.1.1",
    "LearningObjectiveDescription": "Explain how the testing of ML systems can help prevent adversarial attacks and data poisoning.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q162",
        "QuestionText": "Scenario: You are testing an AI-based financial trading system. Which TWO of the following test techniques would be MOST appropriate for ensuring both performance and security?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Load testing to verify system performance under high trading volumes",
          "b": "Adversarial testing to identify vulnerabilities to manipulation attacks",
          "c": "Usability testing to ensure intuitive user interface",
          "d": "A/B testing to compare different algorithm versions"
        },
        "CorrectAnswer": [
          "a",
          "b"
        ],
        "Explanation": "<strong>a) Is correct</strong>. Load testing is essential for financial trading systems to ensure they can handle high trading volumes without performance degradation.<br><br><strong>b) Is correct</strong>. Adversarial testing is crucial for security to identify vulnerabilities to manipulation attacks in AI-based systems.<br><br>c) Is not correct. While usability is important, it's not the primary concern for performance and security in financial trading systems.<br><br>d) Is not correct. A/B testing compares algorithm versions but doesn't directly address performance under load or security vulnerabilities."
      }
    ]
  },
  {
    "LearningObjective": "AI-8.6.1",
    "LearningObjectiveDescription": "Describe how the transparency, interpretability and explainability of AI-based systems can be tested.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q163",
        "QuestionText": "Scenario: You are evaluating a medical diagnosis AI system. Which TWO characteristics are MOST critical for regulatory compliance and patient safety?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "High recall to minimize false negatives in disease detection",
          "b": "Explainability to provide reasoning for diagnosis decisions",
          "c": "Fast inference speed for real-time diagnosis",
          "d": "Low computational resource requirements"
        },
        "CorrectAnswer": [
          "a",
          "b"
        ],
        "Explanation": "<strong>a) Is correct</strong>. High recall minimizes false negatives, which is critical in medical diagnosis to avoid missing diseases.<br><br><strong>b) Is correct</strong>. Explainability is essential for regulatory compliance and patient safety, allowing doctors to understand and trust AI diagnoses.<br><br>c) Is not correct. While fast inference is useful, it's not as critical as accuracy and explainability for patient safety.<br><br>d) Is not correct. Low computational requirements are practical but not the most critical factors for regulatory compliance and patient safety."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.1.1",
    "LearningObjectiveDescription": "Explain how the testing of ML systems can help prevent adversarial attacks and data poisoning.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q164",
        "QuestionText": "Scenario: You discover that adversarial examples created for one facial recognition model also successfully attack other models trained on similar data, even with different architectures. What property of adversarial examples does this demonstrate?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Transferability",
          "b": "Robustness",
          "c": "Generalization",
          "d": "Interpretability"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>a) Is correct</strong>. Transferability refers to the property where adversarial examples created for one model successfully attack other models with different architectures.<br><br>b) Is not correct. Robustness refers to a model's resistance to attacks, not the cross-model effectiveness of attacks.<br><br>c) Is not correct. Generalization refers to a model's performance on unseen data, not adversarial example properties.<br><br>d) Is not correct. Interpretability concerns understanding model decisions, not adversarial attack properties."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.6.1",
    "LearningObjectiveDescription": "Explain how experience-based testing can be applied to the testing of AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q165",
        "QuestionText": "Scenario: You are designing test cases for an autonomous vehicle system. Which TWO of the following testing approaches would be MOST effective for identifying edge cases?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Boundary value analysis for sensor input ranges",
          "b": "Exploratory testing with unusual weather conditions",
          "c": "Unit testing for individual software components",
          "d": "Performance testing under normal operating conditions"
        },
        "CorrectAnswer": [
          "a",
          "b"
        ],
        "Explanation": "<strong>a) Is correct</strong>. Boundary value analysis helps identify edge cases at the limits of sensor input ranges.<br><br><strong>b) Is correct</strong>. Exploratory testing with unusual conditions like bad weather helps discover unexpected edge cases.<br><br>c) Is not correct. Unit testing focuses on individual components but may miss system-level edge cases.<br><br>d) Is not correct. Performance testing under normal conditions doesn't specifically target edge cases."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.1.1",
    "LearningObjectiveDescription": "Explain how the testing of ML systems can help prevent adversarial attacks and data poisoning.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q166",
        "QuestionText": "Scenario: You are testing an image classification model for security cameras. You create adversarial examples by adding subtle noise patterns that are invisible to humans but cause the model to misclassify people as objects. What type of adversarial attack is this?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Data poisoning attack",
          "b": "Black-box attack",
          "c": "Evasion attack",
          "d": "Model inversion attack"
        },
        "CorrectAnswer": "c",
        "Explanation": "<strong>c) Is correct</strong>. This is an evasion attack where subtle perturbations cause misclassification during inference.<br><br>a) Is not correct. Data poisoning occurs during training, not inference.<br><br>b) Is not correct. Black-box refers to attack knowledge, not the attack type.<br><br>d) Is not correct. Model inversion aims to extract training data, not cause misclassification."
      }
    ]
  },
  {
    "LearningObjective": "AI-4.1.1",
    "LearningObjectiveDescription": "Describe the activities and challenges related to data preparation.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q167",
        "QuestionText": "Scenario: You are preparing data for a customer churn prediction model. Which TWO data preprocessing steps are MOST critical for ensuring model quality?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Handling missing values in customer demographic data",
          "b": "Feature scaling for numerical variables like age and income",
          "c": "One-hot encoding for categorical variables like subscription type",
          "d": "Removing all outliers from the dataset"
        },
        "CorrectAnswer": [
          "a",
          "c"
        ],
        "Explanation": "<strong>a) Is correct</strong>. Handling missing values is critical for data quality and model performance.<br><br><strong>c) Is correct</strong>. One-hot encoding categorical variables is essential for most ML algorithms.<br><br>b) Is not correct. Feature scaling is important but can be applied after basic preprocessing.<br><br>d) Is not correct. Removing all outliers can remove valuable information and should be done carefully."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.1.1",
    "LearningObjectiveDescription": "Explain how the testing of ML systems can help prevent adversarial attacks and data poisoning.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q168",
        "QuestionText": "Scenario: You discover that an attacker has been injecting mislabeled data into your training pipeline over several weeks, gradually reducing your model's accuracy. What type of attack is this, and how can you detect it?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Evasion attack; monitor model performance on test data",
          "b": "Data poisoning attack; use statistical analysis to detect anomalies in training data",
          "c": "Model stealing attack; monitor API access patterns",
          "d": "Membership inference attack; audit data access logs"
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>b) Is correct</strong>. This describes a data poisoning attack, detected through statistical analysis of training data anomalies.<br><br>a) Is not correct. Evasion attacks occur during inference, not training.<br><br>c) Is not correct. Model stealing involves copying the model, not corrupting training data.<br><br>d) Is not correct. Membership inference determines if specific data was in training set."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.2.1",
    "LearningObjectiveDescription": "Explain how pairwise testing is used for AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q169",
        "QuestionText": "Scenario: You are using pairwise testing for a complex configuration system with 8 parameters, each with 3 possible values. The total possible combinations are 3⁸ = 6,561. Using pairwise testing, approximately what percentage of test cases can you eliminate while still maintaining good defect detection?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "50-60%",
          "b": "70-80%",
          "c": "85-95%",
          "d": "95-99%"
        },
        "CorrectAnswer": "c",
        "Explanation": "<strong>c) Is correct</strong>. Pairwise testing typically eliminates 85-95% of test cases while maintaining good defect detection for combinatorial testing.<br><br>a) Is not correct. 50-60% reduction is too conservative for pairwise testing.<br><br>b) Is not correct. 70-80% is possible but less typical than 85-95%.<br><br>d) Is not correct. 95-99% reduction would likely miss important interactions."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.5.1",
    "LearningObjectiveDescription": "Apply metamorphic testing for the testing of AI-based systems.",
    "KnowledgeType": "K3",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q170",
        "QuestionText": "Scenario: You are testing a price prediction AI using metamorphic relations. For a car with features: age=3 years, mileage=30,000, brand=Toyota, predicted price=$15,000. Which TWO metamorphic relations would be MOST useful for testing consistency?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Increase age to 4 years → price should decrease",
          "b": "Change brand to Honda → price should be similar",
          "c": "Set mileage to 0 → price should be maximum",
          "d": "Change color from red to blue → price should be identical"
        },
        "CorrectAnswer": [
          "a",
          "c"
        ],
        "Explanation": "<strong>a) Is correct</strong>. Increasing age should decrease price, testing monotonic relationships.<br><br><strong>c) Is correct</strong>. Setting mileage to 0 should give maximum price, testing boundary conditions.<br><br>b) Is not correct. Different brands may have different price points, so prices shouldn't necessarily be similar.<br><br>d) Is not correct. Color changes shouldn't affect price if the model is properly trained on relevant features."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.1.1",
    "LearningObjectiveDescription": "Explain how the testing of ML systems can help prevent adversarial attacks and data poisoning.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q171",
        "QuestionText": "Scenario: You are testing a speech recognition system's robustness. You create adversarial examples by adding background noise that doesn't affect human perception but causes transcription errors. What testing technique should you use to improve robustness against such attacks?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Data augmentation with noisy samples during training",
          "b": "Increasing model complexity",
          "c": "Adding more training data from clean sources",
          "d": "Implementing ensemble methods"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>a) Is correct</strong>. Data augmentation with noisy samples during training improves robustness against adversarial noise attacks.<br><br>b) Is not correct. Increasing model complexity may lead to overfitting without addressing robustness.<br><br>c) Is not correct. More clean data doesn't help with noise robustness.<br><br>d) Is not correct. Ensemble methods provide limited protection against adversarial attacks."
      }
    ]
  },
  {
    "LearningObjective": "AI-5.4.1",
    "LearningObjectiveDescription": "Select appropriate ML functional performance metrics and/or their values for a given ML model and scenario.",
    "KnowledgeType": "K4",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q172",
        "QuestionText": "Scenario: You are evaluating two credit scoring models with the following performance at the same threshold: Model A: Precision = 0.75, Recall = 0.85, F1-score = 0.80, AUC = 0.88 Model B: Precision = 0.85, Recall = 0.75, F1-score = 0.80, AUC = 0.86 Which TWO statements are CORRECT based on this data?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Model A is better for minimizing false negatives",
          "b": "Model B is better for minimizing false positives",
          "c": "Model A has better overall discriminatory power",
          "d": "Both models have identical F1-scores"
        },
        "CorrectAnswer": [
          "a",
          "b"
        ],
        "Explanation": "<strong>a) Is correct</strong>. Model A has higher recall (0.85), meaning fewer false negatives.<br><br><strong>b) Is correct</strong>. Model B has higher precision (0.85), meaning fewer false positives.<br><br>c) Is not correct. Model A has slightly higher AUC (0.88 vs 0.86), but both have identical F1-scores.<br><br>d) Is not correct. While both have F1=0.80, their precision-recall tradeoffs differ."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.5.1",
    "LearningObjectiveDescription": "Apply metamorphic testing for the testing of AI-based systems.",
    "KnowledgeType": "K3",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q173",
        "QuestionText": "Scenario: You are testing a natural language processing system that summarizes long documents. For a source test case where a 10-page document is correctly summarized in 200 words, you apply MR: \"If the document is split into two 5-page documents, the combined summaries should be equivalent to the original summary.\" What type of metamorphic relation is this?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Equivalence relation",
          "b": "Conservation relation",
          "c": "Associative relation",
          "d": "Inclusive relation"
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>b) Is correct</strong>. This is a conservation relation where the total information/content should be preserved when splitting and recombining.<br><br>a) Is not correct. Equivalence would require identical outputs, which may not be possible with split documents.<br><br>c) Is not correct. Associative relations involve order changes, not content splitting.<br><br>d) Is not correct. Inclusive relations involve subset/superset relationships."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.5.1",
    "LearningObjectiveDescription": "Apply metamorphic testing for the testing of AI-based systems.",
    "KnowledgeType": "K3",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q174",
        "QuestionText": "Scenario: You are testing a route optimization system using metamorphic testing. For a source route A→B→C taking 30 minutes, you apply: MR1: Reverse the route C→B→A MR2: Add a point D between B and C: A→B→D→C MR3: Remove point B: A→C Which TWO of the following expected results are CORRECT?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "MR1: Same time (assuming symmetric travel times)",
          "b": "MR2: Longer time than original route",
          "c": "MR3: Shorter time than original route",
          "d": "All MRs should result in identical travel times"
        },
        "CorrectAnswer": [
          "a",
          "b"
        ],
        "Explanation": "<strong>a) Is correct</strong>. MR1: Reversing route should take same time with symmetric travel.<br><br><strong>b) Is correct</strong>. MR2: Adding a point should increase travel time.<br><br>c) Is not correct. MR3: Removing point B might not necessarily give shorter time if A→C is longer than A→B→C.<br><br>d) Is not correct. Different MRs test different properties and won't give identical times."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.6.1",
    "LearningObjectiveDescription": "Explain how experience-based testing can be applied to the testing of AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q175",
        "QuestionText": "Scenario: During exploratory testing of a predictive maintenance system, you use the \"supermodel tour\" focusing on the most complex and critical components. What is the primary focus of a \"supermodel tour\" in exploratory testing?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Testing the system with superusers and their workflows",
          "b": "Focusing on the most complex algorithms and data flows",
          "c": "Testing the system's performance under extreme conditions",
          "d": "Validating the model against superhuman performance benchmarks"
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>b) Is correct</strong>. The supermodel tour focuses on testing the most complex algorithms and critical data flows in the system.<br><br>a) Is not correct. Supermodel tour refers to system complexity, not user types.<br><br>c) Is not correct. Performance under extreme conditions is stress testing, not supermodel tour.<br><br>d) Is not correct. It doesn't involve benchmarking against superhuman performance."
      }
    ]
  },
  {
    "LearningObjective": "AI-8.3.1",
    "LearningObjectiveDescription": "Explain how to test for bias in an AI-based system.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q176",
        "QuestionText": "Scenario: You are performing exploratory data analysis on a dataset for a credit scoring model and create a correlation matrix. Which TWO insights would be MOST concerning for model fairness?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "High correlation between zip code and loan approval rate",
          "b": "Strong negative correlation between credit history length and default rate",
          "c": "Moderate correlation between income and employment type",
          "d": "Significant correlation between gender-neutral names and rejection rates"
        },
        "CorrectAnswer": [
          "a",
          "d"
        ],
        "Explanation": "<strong>a) Is correct</strong>. High correlation between zip code and approval rates may indicate geographic/racial bias.<br><br><strong>d) Is correct</strong>. Correlation between gender-neutral names and rejection may indicate gender bias.<br><br>b) Is not correct. Negative correlation between credit history and default is expected and legitimate.<br><br>c) Is not correct. Moderate correlation between income and employment type is normal."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.1.1",
    "LearningObjectiveDescription": "Explain how the testing of ML systems can help prevent adversarial attacks and data poisoning.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q177",
        "QuestionText": "Scenario: You are selecting test techniques for a real-time stock trading AI that makes millisecond decisions. The system must be highly robust and secure against manipulation. Which combination of test techniques is most appropriate?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Performance testing for latency and adversarial testing for security",
          "b": "A/B testing for accuracy and exploratory testing for usability",
          "c": "Metamorphic testing for functionality and pairwise testing for input combinations",
          "d": "Back-to-back testing with legacy systems and user acceptance testing"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>a) Is correct</strong>. Performance testing for latency and adversarial testing address the critical risks of speed and security manipulation.<br><br>b) Is not correct. A/B testing and usability don't address core security and performance risks.<br><br>c) Is not correct. Metamorphic and pairwise testing don't specifically address security and high-frequency requirements.<br><br>d) Is not correct. Back-to-back and UAT don't focus on the critical performance and security aspects."
      }
    ]
  },
  {
    "LearningObjective": "AI-8.1.1",
    "LearningObjectiveDescription": "Explain the challenges in testing created by the self-learning of AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q178",
        "QuestionText": "Scenario: You are testing a medical image analysis AI that detects tumors in MRI scans. The system must be highly accurate, and false negatives could have serious consequences. Which TWO testing strategies provide the best risk coverage?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Focus on high recall testing with medical expert validation",
          "b": "Prioritize precision testing to minimize false alarms",
          "c": "Concentrate on performance testing for fast results",
          "d": "Use diverse test datasets representing different patient demographics"
        },
        "CorrectAnswer": [
          "a",
          "d"
        ],
        "Explanation": "<strong>a) Is correct</strong>. High recall testing minimizes false negatives, crucial for medical diagnosis.<br><br><strong>d) Is correct</strong>. Diverse test datasets ensure the model works across different patient demographics.<br><br>b) Is not correct. Prioritizing precision would increase false negatives, which is dangerous in medical contexts.<br><br>c) Is not correct. Performance testing for speed is secondary to accuracy in medical diagnosis."
      }
    ]
  },
  {
    "LearningObjective": "AI-5.2.1",
    "LearningObjectiveDescription": "Contrast and compare the concepts behind the ML functional performance metrics for classification, regression and clustering methods.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q179",
        "QuestionText": "Scenario: You are evaluating a multi-class classification model for sentiment analysis with the following confusion matrix: Actual Positive predicted Positive: 150 Actual Positive predicted Neutral: 20 Actual Positive predicted Negative: 10 Actual Neutral predicted Positive: 15 Actual Neutral predicted Neutral: 200 Actual Neutral predicted Negative: 25 Actual Negative predicted Positive: 5 Actual Negative predicted Neutral: 15 Actual Negative predicted Negative: 180 Which TWO metrics would be MOST useful for evaluating model performance across all classes?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Macro-averaged F1-score",
          "b": "Overall accuracy",
          "c": "Weighted precision",
          "d": "Class-specific recall for the majority class only"
        },
        "CorrectAnswer": [
          "a",
          "c"
        ],
        "Explanation": "<strong>a) Is correct</strong>. Macro-averaged F1-score gives equal weight to all classes, important for multi-class imbalance.<br><br><strong>c) Is correct</strong>. Weighted precision accounts for class imbalance in multi-class scenarios.<br><br>b) Is not correct. Overall accuracy can be misleading with class imbalance.<br><br>d) Is not correct. Focusing only on majority class ignores performance on minority classes."
      }
    ]
  },
  {
    "LearningObjective": "AI-5.4.1",
    "LearningObjectiveDescription": "Select appropriate ML functional performance metrics and/or their values for a given ML model and scenario.",
    "KnowledgeType": "K4",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q180",
        "QuestionText": "Scenario: You have a binary classifier with the following performance metrics: Accuracy: 0.85, Precision: 0.80, Recall: 0.90, F1-score: 0.85, Specificity: 0.70 If the prevalence of the positive class in the population is only 5%, what is the most likely issue with this model?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "The model has too many false positives",
          "b": "The model has too many false negatives",
          "c": "The model is overfitting to the training data",
          "d": "The model is underfitting to the training data"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>a) Is correct</strong>. With low prevalence (5%), high recall (0.90) and moderate specificity (0.70) suggest many false positives relative to true positives.<br><br>b) Is not correct. High recall means few false negatives, not many.<br><br>c) Is not correct. The metrics don't indicate overfitting specifically.<br><br>d) Is not correct. Good performance metrics don't suggest underfitting."
      }
    ]
  },
  {
    "LearningObjective": "AI-5.2.1",
    "LearningObjectiveDescription": "Contrast and compare the concepts behind the ML functional performance metrics for classification, regression and clustering methods.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q181",
        "QuestionText": "Scenario: You are comparing two regression models for predicting house prices. The results show: Model A: MSE = 50,000, R-squared = 0.85, MAE = 175 Model B: MSE = 45,000, R-squared = 0.88, MAE = 165 Which TWO statements are CORRECT about these models?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Model B has lower prediction errors than Model A",
          "b": "Model B explains more variance in house prices than Model A",
          "c": "Model A is better because it has higher R-squared",
          "d": "The MAE values indicate both models have similar performance"
        },
        "CorrectAnswer": [
          "a",
          "b"
        ],
        "Explanation": "<strong>a) Is correct</strong>. Model B has lower MSE and MAE, indicating smaller prediction errors.<br><br><strong>b) Is correct</strong>. Model B has higher R-squared (0.88), explaining more variance in house prices.<br><br>c) Is not correct. Model A has lower R-squared (0.85), not higher.<br><br>d) Is not correct. MAE values differ (175 vs 165), indicating different performance."
      }
    ]
  },
  {
    "LearningObjective": "AI-5.4.1",
    "LearningObjectiveDescription": "Select appropriate ML functional performance metrics and/or their values for a given ML model and scenario.",
    "KnowledgeType": "K4",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q182",
        "QuestionText": "Scenario: You are designing test criteria for a content moderation AI that flags inappropriate content. The business requirements state: - False positives (flagging appropriate content) damage user trust and engagement - False negatives (missing inappropriate content) risk regulatory fines and brand damage - The cost of a false negative is estimated to be 5x the cost of a false positive Which metric should be prioritized in model selection?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Precision",
          "b": "Recall",
          "c": "Accuracy",
          "d": "F2-score (which weights recall higher than precision)"
        },
        "CorrectAnswer": "d",
        "Explanation": "<strong>d) Is correct</strong>. F2-score weights recall higher than precision, appropriate when false negatives are 5x more costly than false positives.<br><br>a) Is not correct. Precision minimizes false positives, but false negatives are more costly.<br><br>b) Is not correct. Pure recall doesn't balance the costs appropriately.<br><br>c) Is not correct. Accuracy doesn't account for different misclassification costs."
      }
    ]
  },
  {
    "LearningObjective": "AI-7.4.1",
    "LearningObjectiveDescription": "Explain automation bias and how this affects testing.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q183",
        "QuestionText": "Scenario: You are testing an AI-powered medical diagnosis system where doctors use AI recommendations to make final decisions. During testing, you observe that doctors accept AI recommendations 95% of the time, even when the AI's confidence is below 50%. Which TWO actions should you take to address this issue?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Add tests for system behavior at low confidence levels",
          "b": "Implement confidence thresholds that require human review",
          "c": "Increase the diversity of training data",
          "d": "Focus only on improving model accuracy"
        },
        "CorrectAnswer": [
          "a",
          "b"
        ],
        "Explanation": "<strong>a) Is correct</strong>. Testing system behavior at low confidence levels identifies when recommendations should be questioned.<br><br><strong>b) Is correct</strong>. Confidence thresholds with human review prevent over-reliance on low-confidence AI recommendations.<br><br>c) Is not correct. More diverse data doesn't address the human-AI interaction problem.<br><br>d) Is not correct. Improving accuracy alone doesn't solve the automation bias issue."
      }
    ]
  },
  {
    "LearningObjective": "AI-7.4.1",
    "LearningObjectiveDescription": "Explain automation bias and how this affects testing.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q184",
        "QuestionText": "Scenario: You are designing tests for an autonomous vehicle where human drivers must take over in emergencies. You're concerned about \"complacency bias\" where drivers become inattentive during long autonomous periods. What testing approach best addresses this risk?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Performance testing for quick response times",
          "b": "Usability testing with simulated emergency takeovers",
          "c": "Accuracy testing for object detection",
          "d": "Security testing for system integrity"
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>b) Is correct</strong>. Usability testing with simulated emergency takeovers directly addresses complacency bias in human-AI interaction.<br><br>a) Is not correct. Performance testing doesn't address human attentiveness issues.<br><br>c) Is not correct. Accuracy testing focuses on AI performance, not human factors.<br><br>d) Is not correct. Security testing addresses different risks."
      }
    ]
  },
  {
    "LearningObjective": "AI-8.3.1",
    "LearningObjectiveDescription": "Explain how to test for bias in an AI-based system.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q185",
        "QuestionText": "Scenario: You are testing a hiring AI for gender bias. The training data shows historical hiring patterns where male candidates were preferred 2:1 over female candidates for technical roles. Which TWO statistical approaches would be MOST appropriate to detect if the AI is perpetuating this bias?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "loan approval AI uses zipuare test of independence between gender and hiring decisions",
          "b": "T-test of mean scores between male and female candidates",
          "c": "ANOVA across multiple demographic groups",
          "d": "Disparate impact analysis using the 80% rule"
        },
        "CorrectAnswer": [
          "a",
          "d"
        ],
        "Explanation": "<strong>a) Is correct</strong>. Chi-square test detects statistical dependence between gender and hiring decisions.<br><br><strong>d) Is correct</strong>. Disparate impact analysis using the 80% rule is a standard legal test for discrimination.<br><br>b) Is not correct. T-test compares means but may not capture hiring decision patterns effectively.<br><br>c) Is not correct. ANOVA is for multiple groups but less specific for bias detection."
      }
    ]
  },
  {
    "LearningObjective": "AI-8.3.1",
    "LearningObjectiveDescription": "Explain how to test for bias in an AI-based system.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q186",
        "QuestionText": "Scenario: You discover that a loan approval AI uses zip code as an input feature, and zip codes in your city are highly correlated with race (R² = 0.85). The model shows significantly lower approval rates for certain zip codes. What type of bias testing should you perform?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Test for disparate impact using the 80% rule across zip codes",
          "b": "Test for algorithmic bias by examining feature weights",
          "c": "Test for sample bias by comparing training and population distributions",
          "d": "Test for measurement bias in the data collection process"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>a) Is correct</strong>. Disparate impact testing using the 80% rule across zip codes detects potential proxy discrimination.<br><br>b) Is not correct. Feature weight analysis may not reveal proxy discrimination through zip codes.<br><br>c) Is not correct. Sample bias testing compares distributions, not proxy discrimination.<br><br>d) Is not correct. Measurement bias concerns data collection, not proxy variables."
      }
    ]
  },
  {
    "LearningObjective": "AI-8.6.1",
    "LearningObjectiveDescription": "Describe how the transparency, interpretability and explainability of AI-based systems can be tested.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q187",
        "QuestionText": "Scenario: You are evaluating the transparency of a third-party AI model. The vendor provides: algorithm type, training data sources, hyperparameters, and performance metrics. What additional information is needed for adequate transparency assessment?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "The specific hardware used for training",
          "b": "The names of the data scientists who built the model",
          "c": "Access to the actual training code and data preprocessing steps",
          "d": "The business goals of the company"
        },
        "CorrectAnswer": "c",
        "Explanation": "<strong>c) Is correct</strong>. Access to training code and data preprocessing steps is essential for transparency and reproducibility.<br><br>a) Is not correct. Hardware details are not critical for transparency assessment.<br><br>b) Is not correct. Names of data scientists don't contribute to technical transparency.<br><br>d) Is not correct. Business goals are separate from technical transparency."
      }
    ]
  },
  {
    "LearningObjective": "AI-5.4.1",
    "LearningObjectiveDescription": "Select appropriate ML functional performance metrics and/or their values for a given ML model and scenario.",
    "KnowledgeType": "K4",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q188",
        "QuestionText": "Scenario: You are testing a binary classifier with the following cost matrix: True Positive cost: 0 False Negative cost: 50 False Positive cost: 10 True Negative cost: 0 The model has Precision = 0.80 and Recall = 0.90. Assuming a positive prevalence of 20%, which TWO calculations are CORRECT for 1000 predictions?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Number of False Negatives = 20",
          "b": "Number of False Positives = 45",
          "c": "Total cost = 1,450",
          "d": "Precision is more important than recall for cost minimization"
        },
        "CorrectAnswer": [
          "a",
          "b"
        ],
        "Explanation": "<strong>a) Is correct</strong>. With 20% prevalence in 1000 samples = 200 positives. Recall 0.90 means 180 TP, so FN = 20.<br><br><strong>b) Is correct</strong>. Precision 0.80 with 180 TP means 225 predicted positives, so FP = 45.<br><br>c) Is not correct. Total cost = (20*50) + (45*10) = 1000 + 450 = 1450, not 1450.<br><br>d) Is not correct. Recall is more important since FN cost (50) > FP cost (10)."
      }
    ]
  },
  {
    "LearningObjective": "AI-5.2.1",
    "LearningObjectiveDescription": "Contrast and compare the concepts behind the ML functional performance metrics for classification, regression and clustering methods.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q189",
        "QuestionText": "Scenario: You are comparing clustering algorithms for customer segmentation. You have the following results: K-Means: Silhouette Score = 0.65, Davies-Bouldin Index = 0.80, Calinski-Harabasz Index = 350 DBSCAN: Silhouette Score = 0.72, Davies-Bouldin Index = 0.65, Calinski-Harabasz Index = 420 Gaussian Mixture: Silhouette Score = 0.68, Davies-Bouldin Index = 0.70, Calinski-Harabasz Index = 380 Which algorithm provides the best clustering quality?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "K-Means",
          "b": "DBSCAN",
          "c": "Gaussian Mixture",
          "d": "Cannot determine from given metrics"
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>b) Is correct</strong>. DBSCAN has best scores: highest Silhouette (0.72), lowest Davies-Bouldin (0.65), highest Calinski-Harabasz (420).<br><br>a) Is not correct. K-Means has inferior scores on all metrics.<br><br>c) Is not correct. Gaussian Mixture is intermediate, not best.<br><br>d) Is not correct. The metrics clearly show DBSCAN is best."
      }
    ]
  },
  {
    "LearningObjective": "AI-5.4.1",
    "LearningObjectiveDescription": "Select appropriate ML functional performance metrics and/or their values for a given ML model and scenario.",
    "KnowledgeType": "K4",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q190",
        "QuestionText": "Scenario: You are tuning a model for a COVID-19 detection system. The public health strategy requires identifying as many positive cases as possible, even if it means more false positives that can be resolved with follow-up testing. Which TWO strategies should guide your model selection and testing?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Maximize recall to minimize false negatives",
          "b": "Use a low classification threshold to catch more positives",
          "c": "Prioritize precision to minimize false alarms",
          "d": "Focus on overall accuracy as the primary metric"
        },
        "CorrectAnswer": [
          "a",
          "b"
        ],
        "Explanation": "<strong>a) Is correct</strong>. Maximizing recall minimizes false negatives, critical for disease detection.<br><br><strong>b) Is correct</strong>. Low classification threshold catches more positives, accepting more false positives.<br><br>c) Is not correct. Prioritizing precision would increase false negatives, contrary to requirements.<br><br>d) Is not correct. Accuracy is misleading with class imbalance and different error costs."
      }
    ]
  },
  {
    "LearningObjective": "AI-7.4.1",
    "LearningObjectiveDescription": "Explain automation bias and how this affects testing.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q191",
        "QuestionText": "Scenario: You are testing an AI-based investment advisory system. You observe that financial advisors consistently override the AI's recommendations when they disagree, but rarely when they lack expertise in that investment area. What type of bias does this demonstrate?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Automation bias",
          "b": "Confirmation bias",
          "c": "Expertise-based automation bias",
          "d": "Algorithmic bias"
        },
        "CorrectAnswer": "c",
        "Explanation": "<strong>c) Is correct</strong>. Expertise-based automation bias occurs when humans defer to automation more in areas where they lack expertise.<br><br>a) Is not correct. Automation bias is general over-reliance, not expertise-dependent.<br><br>b) Is not correct. Confirmation bias is seeking confirming evidence, not expertise-based.<br><br>d) Is not correct. Algorithmic bias refers to model fairness, not human-AI interaction."
      }
    ]
  },
  {
    "LearningObjective": "AI-8.3.1",
    "LearningObjectiveDescription": "Explain how to test for bias in an AI-based system.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q192",
        "QuestionText": "Scenario: You are testing a resume screening AI for gender bias. The training data shows historical hiring where male candidates were preferred, but the AI uses features like \"coding bootcamp attendance\" and \"programming language proficiency\" that correlate with gender in your industry. Which TWO approaches can detect this indirect bias?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Compare hiring rates by gender directly",
          "b": "Use propensity score matching to create balanced groups",
          "c": "Analyze feature importance for gender-correlated features",
          "d": "Calculate disparate impact ratio using proxy variables"
        },
        "CorrectAnswer": [
          "c",
          "d"
        ],
        "Explanation": "<strong>c) Is correct</strong>. Analyzing feature importance for gender-correlated features detects indirect bias through proxies.<br><br><strong>d) Is correct</strong>. Disparate impact analysis using proxy variables identifies indirect discrimination.<br><br>a) Is not correct. Direct gender comparison may not reveal indirect bias through correlated features.<br><br>b) Is not correct. Propensity score matching balances groups but doesn't specifically detect proxy bias."
      }
    ]
  },
  {
    "LearningObjective": "AI-8.6.1",
    "LearningObjectiveDescription": "Describe how the transparency, interpretability and explainability of AI-based systems can be tested.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q193",
        "QuestionText": "Scenario: You are testing an AI credit scoring system where loan officers need to understand why applications are rejected. The model uses hundreds of features in a complex ensemble. Which explainability method provides the most actionable insights for loan officers?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Global feature importance rankings",
          "b": "Local explanations for individual applications",
          "c": "Model-agnostic summary statistics",
          "d": "Confidence intervals for predictions"
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>b) Is correct</strong>. Local explanations for individual applications provide specific, actionable reasons for rejections.<br><br>a) Is not correct. Global feature importance gives overall trends, not specific application reasons.<br><br>c) Is not correct. Summary statistics don't provide actionable insights for individual cases.<br><br>d) Is not correct. Confidence intervals don't explain why specific decisions were made."
      }
    ]
  },
  {
    "LearningObjective": "AI-4.1.1",
    "LearningObjectiveDescription": "Describe the activities and challenges related to data preparation.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q194",
        "QuestionText": "Scenario: You are preparing data for a customer churn prediction model. You're given a raw dataset with the following issues: - Missing values in the \"income\" column (30% missing) - Categorical data in \"subscription_type\" (Basic, Premium, Enterprise) - Numerical outliers in \"monthly_usage_hours\" - Date format inconsistency in \"signup_date\" Which TWO data preprocessing steps should be performed FIRST?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Handle missing values in the income column",
          "b": "One-hot encode the subscription_type variable",
          "c": "Detect and handle outliers in monthly_usage_hours",
          "d": "Standardize the date format in signup_date"
        },
        "CorrectAnswer": [
          "a",
          "d"
        ],
        "Explanation": "<strong>a) Is correct</strong>. Handling missing values is fundamental and affects all subsequent processing.<br><br><strong>d) Is correct</strong>. Standardizing date format is basic data cleaning that should be done early.<br><br>b) Is not correct. One-hot encoding can be done after handling missing values and basic cleaning.<br><br>c) Is not correct. Outlier detection typically comes after basic data cleaning."
      }
    ]
  },
  {
    "LearningObjective": "HO-8.6.1",
    "LearningObjectiveDescription": "Use a tool to show how explainability can be used by testers.",
    "KnowledgeType": "H2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q195",
        "QuestionText": "Scenario: You're using LIME to explain a loan application rejection. The model predicted \"reject\" with 85% confidence. LIME provides the following feature contributions: Credit Score: -0.45 Debt-to-Income: -0.35 Employment Duration: +0.15 Loan Amount: -0.20 Which TWO statements are CORRECT based on this LIME explanation?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Credit Score is the most influential factor for rejection",
          "b": "Employment Duration had a positive impact on the decision",
          "c": "All features contributed negatively to the rejection",
          "d": "The model's confidence level affects the LIME explanation"
        },
        "CorrectAnswer": [
          "a",
          "b"
        ],
        "Explanation": "<strong>a) Is correct</strong>. Credit Score has the largest negative contribution (-0.45), making it most influential for rejection.<br><br><strong>b) Is correct</strong>. Employment Duration has positive contribution (+0.15), meaning it favored approval.<br><br>c) Is not correct. Employment Duration contributed positively, not all features were negative.<br><br>d) Is not correct. LIME explanations are local and don't depend on the model's confidence level."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.5.1",
    "LearningObjectiveDescription": "Apply metamorphic testing for the testing of AI-based systems.",
    "KnowledgeType": "K3",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q196",
        "QuestionText": "Scenario: You are testing a speech recognition system that converts speech to text. You have a source test case with the sentence \"The quick brown fox jumps over the lazy dog\" which is correctly transcribed. You apply a metamorphic relation (MR) that changes the input by adding background noise and expects the output to be the same or similar. What type of MR is this?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "MR based on equality: output should be identical",
          "b": "MR based on change in input: output should be consistent despite noise",
          "c": "MR based on performance: output should be within time constraints",
          "d": "MR based on robustness: output should degrade gracefully"
        },
        "CorrectAnswer": "d",
        "Explanation": "<strong>d) Is correct</strong>. This MR tests robustness - the system should degrade gracefully with added noise, not necessarily maintain identical output.<br><br>a) Is not correct. Equality would require identical output, which may not be realistic with noise.<br><br>b) Is not correct. Change in input doesn't specify the expected relationship clearly.<br><br>c) Is not correct. Performance relations concern timing, not output quality."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.5.1",
    "LearningObjectiveDescription": "Apply metamorphic testing for the testing of AI-based systems.",
    "KnowledgeType": "K3",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q197",
        "QuestionText": "Scenario: You are testing an image classification AI that identifies animals. For a source test case with an image of a cat that is correctly classified, you apply an MR that rotates the image by 90 degrees. What should the expected output be for the follow-up test case?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "The output should be \"cat\" because rotation should not change the classification",
          "b": "The output should be \"dog\" because rotation may confuse the model",
          "c": "The output should be uncertain because rotation alters pixels",
          "d": "The output should be checked for consistency with the source test case"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>a) Is correct</strong>. Image classification should be rotation-invariant; rotating a cat image should still be classified as cat.<br><br>b) Is not correct. Rotation shouldn't change the fundamental classification to a different animal.<br><br>c) Is not correct. Well-trained models should handle simple rotations without uncertainty.<br><br>d) Is not correct. While consistency is checked, the expected output is specifically 'cat'."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.6.1",
    "LearningObjectiveDescription": "Explain how experience-based testing can be applied to the testing of AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q198",
        "QuestionText": "Scenario: You are performing exploratory testing on a recommendation system for an e-commerce site. You decide to use a \"data tour\" to explore how the system handles different user profiles. Which TWO goals are primary for a data tour in exploratory testing?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "To understand the data types, distributions, and variations used by the system",
          "b": "To verify the system's compliance with data privacy regulations",
          "c": "To measure the system's performance under load",
          "d": "To identify how different data characteristics affect system behavior"
        },
        "CorrectAnswer": [
          "a",
          "d"
        ],
        "Explanation": "<strong>a) Is correct</strong>. Understanding data types, distributions and variations is a primary goal of data tours.<br><br><strong>d) Is correct</strong>. Identifying how data characteristics affect system behavior is key in data tours.<br><br>b) Is not correct. Privacy compliance is important but not the primary focus of data tours.<br><br>c) Is not correct. Performance under load is addressed in performance testing, not data tours."
      }
    ]
  },
  {
    "LearningObjective": "AI-4.1.1",
    "LearningObjectiveDescription": "Describe the activities and challenges related to data preparation.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q199",
        "QuestionText": "Scenario: You are using exploratory data analysis (EDA) on a dataset used to train a credit scoring model. You discover that the dataset has missing values for income in 30% of the records, and the missing values are concentrated in low-income brackets. What risk does this pose, and how should it be addressed in testing?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Sample bias; test the model with a complete dataset to compare performance",
          "b": "Algorithmic bias; adjust the model's hyperparameters",
          "c": "Data quality issue; test the model's handling of missing values and consider imputation methods",
          "d": "Overfitting; reduce model complexity"
        },
        "CorrectAnswer": "c",
        "Explanation": "<strong>c) Is correct</strong>. This is a data quality issue with missing values concentrated in specific groups, requiring testing of missing value handling and imputation methods.<br><br>a) Is not correct. Sample bias refers to unrepresentative sampling, not missing data patterns.<br><br>b) Is not correct. Algorithmic bias concerns model decisions, not data quality issues.<br><br>d) Is not correct. Overfitting is a modeling issue, not a data quality problem."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.5.1",
    "LearningObjectiveDescription": "Apply metamorphic testing for the testing of AI-based systems.",
    "KnowledgeType": "K3",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q200",
        "QuestionText": "Scenario: You are selecting test techniques for a medical diagnostic AI that uses a deep neural network. The system must be highly accurate, and explanations for diagnoses are required for regulatory approval. Which TWO combinations of test techniques are most appropriate?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Back-to-back testing with a simpler model, and metamorphic testing for robustness",
          "b": "A/B testing with previous versions, and performance testing for speed",
          "c": "Pairwise testing for input parameters, and explainability testing using LIME",
          "d": "Experience-based testing for bias, and unit testing for code coverage"
        },
        "CorrectAnswer": [
          "a",
          "c"
        ],
        "Explanation": "<strong>a) Is correct</strong>. Back-to-back testing with simpler models and metamorphic testing address accuracy and robustness requirements.<br><br><strong>c) Is correct</strong>. Pairwise testing and explainability testing using LIME address input combinations and regulatory explanation needs.<br><br>b) Is not correct. A/B testing and performance testing don't specifically address accuracy and explainability.<br><br>d) Is not correct. Experience-based testing and unit testing are too limited for comprehensive medical AI testing."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.5.1",
    "LearningObjectiveDescription": "Apply metamorphic testing for the testing of AI-based systems.",
    "KnowledgeType": "K3",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q201",
        "QuestionText": "Scenario: You are testing a self-learning chatbot that adapts to user conversations. The chatbot can change its responses based on new interactions, and you are concerned about it learning inappropriate behavior. Which TWO test techniques should you use to continuously monitor the system?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Static code analysis and code reviews",
          "b": "A/B testing and metamorphic testing with conversational invariants",
          "c": "Performance testing and load testing",
          "d": "Security testing and penetration testing"
        },
        "CorrectAnswer": [
          "b",
          "d"
        ],
        "Explanation": "<strong>b) Is correct</strong>. A/B testing and metamorphic testing with conversational invariants monitor learning behavior and consistency.<br><br><strong>d) Is correct</strong>. Security testing and penetration testing protect against malicious manipulation of learning.<br><br>a) Is not correct. Static analysis and code reviews don't address runtime learning behavior.<br><br>c) Is not correct. Performance and load testing don't monitor content appropriateness."
      }
    ]
  },
  {
    "LearningObjective": "AI-5.1.1",
    "LearningObjectiveDescription": "Calculate the ML functional performance metrics from a given set of confusion matrix data.",
    "KnowledgeType": "K3",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q202",
        "QuestionText": "Scenario: You are evaluating a binary classification model for email spam detection. The confusion matrix for a test set of 1000 emails is: Actual Spam predicted Spam: 97 Actual Spam predicted Not Spam: 3 Actual Not Spam predicted Spam: 3 Actual Not Spam predicted Not Spam: 897 What is the F1-score for the spam class?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "0.90",
          "b": "0.95",
          "c": "0.97",
          "d": "0.99"
        },
        "CorrectAnswer": "c",
        "Explanation": "<strong>c) Is correct</strong>. Precision = 97/(97+3) = 0.97, Recall = 97/(97+3) = 0.97, F1 = 2*(0.97*0.97)/(0.97+0.97) = 0.97.<br><br>a) Is not correct. 0.90 is too low given high precision and recall.<br><br>b) Is not correct. 0.95 underestimates the actual F1-score.<br><br>d) Is not correct. 0.99 overestimates the actual F1-score."
      }
    ]
  },
  {
    "LearningObjective": "AI-5.2.1",
    "LearningObjectiveDescription": "Contrast and compare the concepts behind the ML functional performance metrics for classification, regression and clustering methods.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q203",
        "QuestionText": "Scenario: You are comparing two clustering algorithms on customer data. You have the following metrics: Algorithm A: Silhouette Coefficient = 0.75, Intra-cluster Distance = 0.5, Inter-cluster Distance = 2.0 Algorithm B: Silhouette Coefficient = 0.65, Intra-cluster Distance = 0.6, Inter-cluster Distance = 1.8 Which TWO statements are CORRECT based on these metrics?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Algorithm A has better-defined clusters than Algorithm B",
          "b": "Algorithm B has tighter clusters than Algorithm A",
          "c": "Algorithm A has better separation between clusters",
          "d": "Both algorithms have similar clustering quality"
        },
        "CorrectAnswer": [
          "a",
          "c"
        ],
        "Explanation": "<strong>a) Is correct</strong>. Algorithm A has higher Silhouette Coefficient (0.75), indicating better-defined clusters.<br><br><strong>c) Is correct</strong>. Algorithm A has higher inter-cluster distance (2.0 vs 1.8), indicating better separation.<br><br>b) Is not correct. Algorithm A has lower intra-cluster distance (0.5 vs 0.6), meaning tighter clusters.<br><br>d) Is not correct. The metrics clearly show Algorithm A performs better."
      }
    ]
  },
  {
    "LearningObjective": "AI-5.4.1",
    "LearningObjectiveDescription": "Select appropriate ML functional performance metrics and/or their values for a given ML model and scenario.",
    "KnowledgeType": "K4",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q204",
        "QuestionText": "Scenario: You are working on a model to predict equipment failure in a manufacturing plant. False negatives (missing a failure) are very costly because they lead to unplanned downtime. False positives (false alarms) are less costly but can lead to unnecessary maintenance. Which metric should you optimize during model selection?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Precision",
          "b": "Recall",
          "c": "Accuracy",
          "d": "Specificity"
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>b) Is correct</strong>. Recall minimizes false negatives, which are costly due to unplanned downtime from missed failures.<br><br>a) Is not correct. Precision minimizes false positives, which are less costly.<br><br>c) Is not correct. Accuracy doesn't account for different misclassification costs.<br><br>d) Is not correct. Specificity concerns true negative rate, not failure detection."
      }
    ]
  },
  {
    "LearningObjective": "HO-6.1.1",
    "LearningObjectiveDescription": "Experience the implementation of a perceptron.",
    "KnowledgeType": "H1",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q205",
        "QuestionText": "Scenario: You are implementing a simple perceptron for AND gate logic. The training data is: Input: (0,0) → Output: 0 Input: (0,1) → Output: 0 Input: (1,0) → Output: 0 Input: (1,1) → Output: 1 After training, the perceptron converges with weights w1 = 0.7, w2 = 0.7, and bias = -1.0. Which TWO statements are CORRECT about this perceptron?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "For input (1,1), z = 0.4 and output = 1",
          "b": "For input (0,0), z = -1.0 and output = 0",
          "c": "The perceptron correctly implements AND logic for all inputs",
          "d": "The bias value should be positive for correct AND logic"
        },
        "CorrectAnswer": [
          "a",
          "b"
        ],
        "Explanation": "<strong>a) Is correct</strong>. For input (1,1): z = (1*0.7 + 1*0.7) - 1.0 = 0.4 > 0, output = 1.<br><br><strong>b) Is correct</strong>. For input (0,0): z = (0*0.7 + 0*0.7) - 1.0 = -1.0 < 0, output = 0.<br><br>c) Is not correct. The perceptron correctly implements AND logic for all inputs.<br><br>d) Is not correct. Negative bias is correct for AND logic implementation."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.5.1",
    "LearningObjectiveDescription": "Apply metamorphic testing for the testing of AI-based systems.",
    "KnowledgeType": "K3",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q206",
        "QuestionText": "Scenario: You are testing a time series forecasting model using metamorphic testing. For a source test case predicting sales for the next 30 days, you apply MR: \"If historical data is shifted forward by 7 days, the forecast should shift forward by 7 days.\" What property of time series models does this MR test?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Stationarity",
          "b": "Temporal invariance",
          "c": "Seasonality",
          "d": "Trend consistency"
        },
        "CorrectAnswer": "b",
        "Explanation": "<strong>b) Is correct</strong>. This tests temporal invariance - the model should be invariant to time shifts in the data.<br><br>a) Is not correct. Stationarity is a data property, not what the MR specifically tests.<br><br>c) Is not correct. Seasonality involves periodic patterns, not simple time shifts.<br><br>d) Is not correct. Trend consistency would test if trends are maintained across shifts."
      }
    ]
  },
  {
    "LearningObjective": "AI-7.7.1",
    "LearningObjectiveDescription": "For a given scenario determine a test approach to be followed when developing an ML system.",
    "KnowledgeType": "K4",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q207",
        "QuestionText": "Scenario: You are designing a test strategy for an AI-based autonomous trading system that must handle high-frequency trading with strict regulatory requirements. Which TWO combinations of test techniques address the highest-priority risks?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Performance testing for latency, and adversarial testing for security",
          "b": "Regulatory compliance testing, and back-to-back testing with legacy systems",
          "c": "Usability testing for trader experience, and A/B testing for strategy optimization",
          "d": "Metamorphic testing for functionality, and pairwise testing for parameter combinations"
        },
        "CorrectAnswer": [
          "a",
          "b"
        ],
        "Explanation": "<strong>a) Is correct</strong>. Performance testing for latency and adversarial testing address critical speed and security risks.<br><br><strong>b) Is correct</strong>. Regulatory compliance testing and back-to-back testing address legal requirements and consistency.<br><br>c) Is not correct. Usability and A/B testing don't address high-priority regulatory and security risks.<br><br>d) Is not correct. Metamorphic and pairwise testing miss critical performance and regulatory aspects."
      }
    ]
  },
  {
    "LearningObjective": "AI-4.5.2",
    "LearningObjectiveDescription": "Recall reasons for the data in datasets being mislabelled.",
    "KnowledgeType": "K1",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q208",
        "QuestionText": "A sentiment analysis model overestimates positive reviews because training data contained 80% positive comments. Which approach aligns best with AI-4.5.2?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Use oversampling of negative examples",
          "b": "Lower the model's accuracy threshold",
          "c": "Remove all positive reviews from training data",
          "d": "Apply cross-validation with balanced folds"
        },
        "CorrectAnswer": [
          "a",
          "d"
        ],
        "Explanation": "<strong>a) Correct</strong> – oversampling balances class frequency.<br><br>b) Incorrect – changes evaluation, not learning.<br><br>c) Incorrect – introduces reverse bias.<br><br><strong>d) Correct</strong> – stratified cross-validation prevents imbalance during testing."
      }
    ]
  },
  {
    "LearningObjective": "AI-8.3.1",
    "LearningObjectiveDescription": "Explain how to test for bias in an AI-based system.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q209",
        "QuestionText": "A recruitment AI rejects significantly more female applicants. Which corrective test action addresses this best?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Apply fairness testing using equal opportunity metrics",
          "b": "Increase dropout rate in neural network",
          "c": "Add synthetic female candidate data for retraining",
          "d": "Mask the gender column in training data"
        },
        "CorrectAnswer": [
          "a",
          "c"
        ],
        "Explanation": "<strong>a) Correct</strong> – fairness metrics quantify discrimination.<br><br>b) Incorrect – dropout prevents overfitting, not social bias.<br><br><strong>c) Correct</strong> – data augmentation restores class balance.<br><br>d) Partially correct but incomplete – removing gender doesn't eliminate indirect bias from correlated attributes."
      }
    ]
  },
  {
    "LearningObjective": "AI-5.4.1",
    "LearningObjectiveDescription": "Select appropriate ML functional performance metrics and/or their values for a given ML model and scenario.",
    "KnowledgeType": "K4",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q210",
        "QuestionText": "You are comparing two medical classifiers. Model A has high recall but lower precision; Model B has balanced precision and recall. Which is more suitable for early disease detection?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Model A",
          "b": "Model B",
          "c": "Either, since accuracy is identical",
          "d": "None; F1-score must be compared"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>a) Correct</strong> – high recall minimizes false negatives, critical for screening.<br><br>b) Incorrect – balanced model may miss critical cases.<br><br>c) Incorrect – accuracy hides imbalance effects.<br><br>d) F1 is useful, but recall is the priority metric in early detection."
      }
    ]
  },
  {
    "LearningObjective": "AI-5.1.1",
    "LearningObjectiveDescription": "Calculate the ML functional performance metrics from a given set of confusion matrix data.",
    "KnowledgeType": "K3",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q211",
        "QuestionText": "A model's confusion matrix shows 900 TN, 50 FP, 40 FN, and 10 TP. Which TWO statements are correct?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Recall = 10/(10+40) = 0.2",
          "b": "Precision = 10/(10+50) = 0.17",
          "c": "Accuracy = (900+10)/1000 = 0.91",
          "d": "F1-score > 0.5"
        },
        "CorrectAnswer": [
          "a",
          "b"
        ],
        "Explanation": "<strong>a) True</strong> – recall measures sensitivity.<br><br><strong>b) True</strong> – precision calculation is correct.<br><br>c) Correct – but misleading due to imbalance.<br><br>d) Incorrect – F1 is low (~0.18)."
      }
    ]
  },
  {
    "LearningObjective": "AI-5.3.1",
    "LearningObjectiveDescription": "Summarize the limitations of using ML functional performance metrics to determine the quality of the ML system.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q212",
        "QuestionText": "In model evaluation, why is AUC-ROC preferred over simple accuracy in AI-based fraud detection?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "AUC-ROC accounts for class imbalance",
          "b": "It measures model performance across thresholds",
          "c": "It ignores false positives",
          "d": "It's less computationally expensive"
        },
        "CorrectAnswer": [
          "a",
          "b"
        ],
        "Explanation": "<strong>a) Correct</strong> – imbalance tolerance is key.<br><br><strong>b) Correct</strong> – threshold-independent metric.<br><br>c) Incorrect – FP affect ROC curve.<br><br>d) Incorrect – AUC requires multiple computations."
      }
    ]
  },
  {
    "LearningObjective": "AI-7.7.1",
    "LearningObjectiveDescription": "For a given scenario determine a test approach to be followed when developing an ML system.",
    "KnowledgeType": "K4",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q213",
        "QuestionText": "Your model is periodically retrained with new data. Which regression testing approach ensures stability across versions?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Fixed test datasets and metric comparison",
          "b": "Retraining from scratch with identical hyperparameters",
          "c": "Continuous integration with full re-evaluation suite",
          "d": "Manual spot checks on random samples"
        },
        "CorrectAnswer": [
          "a",
          "c"
        ],
        "Explanation": "<strong>a) Correct</strong> – reference dataset ensures comparability.<br><br>b) Helps reproducibility but not testing itself.<br><br><strong>c) Correct</strong> – automation validates every model iteration.<br><br>d) Unsystematic, prone to oversight."
      }
    ]
  },
  {
    "LearningObjective": "AI-7.7.1",
    "LearningObjectiveDescription": "For a given scenario determine a test approach to be followed when developing an ML system.",
    "KnowledgeType": "K4",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q214",
        "QuestionText": "A recommender system's retrained version improved accuracy but degraded diversity. What testing strategy aligns with AI-7.7.1?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Introduce regression tests for secondary KPIs",
          "b": "Focus solely on F1-score improvements",
          "c": "Use delta analysis on top-N recommendations",
          "d": "Freeze diversity metric weighting during training"
        },
        "CorrectAnswer": [
          "a",
          "c"
        ],
        "Explanation": "<strong>a) Correct</strong> – secondary metric regression avoids performance drift.<br><br>b) Incorrect – single metric bias.<br><br><strong>c) Correct</strong> – comparative evaluation across versions.<br><br>d) Design-level, not test-level action."
      }
    ]
  },
  {
    "LearningObjective": "AI-8.7.1",
    "LearningObjectiveDescription": "Explain the challenges in creating test oracles resulting from the specific characteristics of AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q215",
        "QuestionText": "Which TWO regression test principles apply uniquely to AI-based systems?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Static expected results are replaced by tolerance ranges",
          "b": "Test oracles depend on stochastic model outputs",
          "c": "The goal is zero deviation in predictions",
          "d": "Retesting only occurs when data changes"
        },
        "CorrectAnswer": [
          "a",
          "b"
        ],
        "Explanation": "<strong>a) Correct</strong> – AI outputs vary probabilistically.<br><br><strong>b) Correct</strong> – oracles reflect distributional similarity.<br><br>c) Incorrect – minor variation is expected.<br><br>d) Retraining can cause change without data updates."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.1.1",
    "LearningObjectiveDescription": "Explain how the testing of ML systems can help prevent adversarial attacks and data poisoning.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q216",
        "QuestionText": "You inject Gaussian noise into input data for an autonomous drone vision system. Which TWO observations indicate poor robustness?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Significant performance drop (>30%)",
          "b": "Minor fluctuation in accuracy (<2%)",
          "c": "Model output changes drastically for similar inputs",
          "d": "ROC-AUC improves after noise injection"
        },
        "CorrectAnswer": [
          "a",
          "c"
        ],
        "Explanation": "<strong>a) Correct</strong> – significant performance drop indicates poor robustness.<br><br>b) Minor fluctuation is normal and acceptable.<br><br><strong>c) Correct</strong> – drastic output changes indicate instability.<br><br>d) Improvement doesn't indicate poor robustness."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.1.1",
    "LearningObjectiveDescription": "Explain how the testing of ML systems can help prevent adversarial attacks and data poisoning.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q217",
        "QuestionText": "A speech recognition AI misinterprets commands in noisy environments. Which robustness enhancement is most testable?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Adversarial data augmentation",
          "b": "Simplifying vocabulary",
          "c": "Increasing model depth",
          "d": "Disabling dropout"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>a) Correct</strong> – exposes and strengthens noise resistance.<br><br>b) Design choice, not robustness test.<br><br>c) Increases capacity but not necessarily robustness.<br><br>d) Reduces regularization, may worsen generalization."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.5.1",
    "LearningObjectiveDescription": "Apply metamorphic testing for the testing of AI-based systems.",
    "KnowledgeType": "K3",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q218",
        "QuestionText": "Why is metamorphic testing suitable for robustness evaluation?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "It verifies consistent behavior under predictable transformations",
          "b": "It eliminates the need for ground truth data",
          "c": "It relies on adversarial labeling",
          "d": "It measures internal neuron activation consistency"
        },
        "CorrectAnswer": [
          "a",
          "b"
        ],
        "Explanation": "<strong>a) Correct</strong> – verifies consistent behavior under predictable transformations.<br><br><strong>b) Correct</strong> – eliminates the need for ground truth data.<br><br>c) Incorrect – doesn't rely on adversarial labeling.<br><br>d) Incorrect – doesn't measure internal neuron activations."
      }
    ]
  },
  {
    "LearningObjective": "AI-8.8.1",
    "LearningObjectiveDescription": "Select appropriate test objectives and acceptance criteria for the AI-specific quality characteristics of a given AI-based system.",
    "KnowledgeType": "K4",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q219",
        "QuestionText": "When testing an AI-based medical diagnosis system for robustness, what scenario demonstrates a fail-safe design?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Returns \"uncertain\" when confidence < threshold",
          "b": "Produces random output to continue operation",
          "c": "Retrains itself on uncertain cases automatically",
          "d": "Ignores outliers to maintain speed"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>a) Correct</strong> – safety prioritized over false confidence.<br><br>b) Incorrect – random output is unsafe.<br><br>c) Incorrect – automatic retraining on uncertain cases could be dangerous.<br><br>d) Incorrect – ignoring outliers compromises safety."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.1.1",
    "LearningObjectiveDescription": "Explain how the testing of ML systems can help prevent adversarial attacks and data poisoning.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q220",
        "QuestionText": "An autonomous car's object detector fails under heavy rain. Which TWO mitigations should robustness testing verify?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Sensor fusion fallback",
          "b": "Model retraining with rain-augmented data",
          "c": "Manual override triggering",
          "d": "Reduced frame-rate inference"
        },
        "CorrectAnswer": [
          "a",
          "b"
        ],
        "Explanation": "<strong>a) Correct</strong> – sensor fusion provides redundancy.<br><br><strong>b) Correct</strong> – retraining with augmented data improves robustness.<br><br>c) Manual override is a safety feature but not the primary robustness mitigation.<br><br>d) Reduced frame-rate may degrade performance further."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.1.1",
    "LearningObjectiveDescription": "Explain how the testing of ML systems can help prevent adversarial attacks and data poisoning.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q221",
        "QuestionText": "For AI-based control systems, robustness tests should:",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Simulate environmental perturbations and observe stability",
          "b": "Focus exclusively on algorithm explainability",
          "c": "Use fault injection to evaluate resilience",
          "d": "Disable redundant safety mechanisms"
        },
        "CorrectAnswer": [
          "a",
          "c"
        ],
        "Explanation": "<strong>a) Correct</strong> – simulating perturbations evaluates stability.<br><br>b) Incorrect – explainability is important but not the focus of robustness testing.<br><br><strong>c) Correct</strong> – fault injection evaluates resilience.<br><br>d) Incorrect – disabling safety mechanisms is dangerous."
      }
    ]
  },
  {
    "LearningObjective": "HO-8.6.1",
    "LearningObjectiveDescription": "Use a tool to show how explainability can be used by testers.",
    "KnowledgeType": "H2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q222",
        "QuestionText": "You use LIME to explain individual predictions in a loan approval model. What key aspect is evaluated?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Local interpretability",
          "b": "Global model transparency",
          "c": "Feature importance stability",
          "d": "Model calibration"
        },
        "CorrectAnswer": [
          "a",
          "c"
        ],
        "Explanation": "<strong>a) Correct</strong> – LIME evaluates local interpretability.<br><br>b) Incorrect – LIME doesn't provide global transparency.<br><br><strong>c) Correct</strong> – evaluates feature importance stability.<br><br>d) Incorrect – LIME doesn't measure model calibration."
      }
    ]
  },
  {
    "LearningObjective": "AI-8.6.1",
    "LearningObjectiveDescription": "Describe how the transparency, interpretability and explainability of AI-based systems can be tested.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q223",
        "QuestionText": "When evaluating explainability, why is feature correlation analysis important?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Identifies surrogate features causing indirect bias",
          "b": "Improves model convergence speed",
          "c": "Detects redundancy in explanations",
          "d": "Ensures transparency in feature influence"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>a) Correct</strong> – identifies surrogate features causing indirect bias.<br><br>b) Incorrect – doesn't improve convergence speed.<br><br>c) Detects redundancy but this is secondary.<br><br>d) Ensures transparency but identifying bias is primary."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.1.1",
    "LearningObjectiveDescription": "Explain how the testing of ML systems can help prevent adversarial attacks and data poisoning.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q224",
        "QuestionText": "You add imperceptible pixel noise to test images, causing misclassification. Which conclusion supports effective adversarial testing?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "The model lacks robustness to input perturbations",
          "b": "The test violates test oracle stability",
          "c": "Gradient masking prevents interpretability",
          "d": "Model underfitting has been detected"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>a) Correct</strong> – model lacks robustness to input perturbations.<br><br>b) Incorrect – test doesn't violate oracle stability.<br><br>c) Incorrect – gradient masking is a different issue.<br><br>d) Incorrect – underfitting is not indicated by adversarial vulnerability."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.1.1",
    "LearningObjectiveDescription": "Explain how the testing of ML systems can help prevent adversarial attacks and data poisoning.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q225",
        "QuestionText": "Which TWO practices characterize advanced adversarial testing?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Gradient-based input perturbation",
          "b": "Randomized data shuffling",
          "c": "Black-box query attacks",
          "d": "Accuracy tuning by learning rate"
        },
        "CorrectAnswer": [
          "a",
          "c"
        ],
        "Explanation": "<strong>a) Correct</strong> – gradient-based perturbation is advanced adversarial testing.<br><br>b) Incorrect – randomized shuffling is basic data preparation.<br><br><strong>c) Correct</strong> – black-box query attacks are advanced techniques.<br><br>d) Incorrect – accuracy tuning is model optimization, not adversarial testing."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.1.1",
    "LearningObjectiveDescription": "Explain how the testing of ML systems can help prevent adversarial attacks and data poisoning.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q226",
        "QuestionText": "Why are adversarial attacks critical in AI security testing?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "They expose vulnerabilities to small, intentional input changes",
          "b": "They replace standard regression tests",
          "c": "They guarantee model interpretability",
          "d": "They assess resilience under realistic attack conditions"
        },
        "CorrectAnswer": [
          "a",
          "d"
        ],
        "Explanation": "<strong>a) Correct</strong> – they expose vulnerabilities to small, intentional input changes.<br><br>b) Incorrect – they complement but don't replace standard regression tests.<br><br>c) Incorrect – they don't guarantee interpretability.<br><br><strong>d) Correct</strong> – they assess resilience under realistic attack conditions."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.5.1",
    "LearningObjectiveDescription": "Apply metamorphic testing for the testing of AI-based systems.",
    "KnowledgeType": "K3",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q227",
        "QuestionText": "In a translation AI, replacing \"He is a doctor\" with \"She is a doctor\" yields \"Elle est infirmière\". Which metamorphic relation fails?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Gender invariance",
          "b": "Lexical consistency",
          "c": "Label permutation",
          "d": "Data normalization"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>a) Correct</strong> – gender invariance relation fails (doctor → infirmière).<br><br>b) Incorrect – lexical consistency is not the primary issue.<br><br>c) Incorrect – label permutation doesn't apply here.<br><br>d) Incorrect – data normalization isn't relevant."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.5.1",
    "LearningObjectiveDescription": "Apply metamorphic testing for the testing of AI-based systems.",
    "KnowledgeType": "K3",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q228",
        "QuestionText": "What is the main advantage of metamorphic testing in AI?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "It validates relations instead of explicit expected outputs",
          "b": "It guarantees determinism",
          "c": "It simplifies hyperparameter tuning",
          "d": "It enforces identical predictions"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>a) Correct</strong> – validates relations instead of explicit expected outputs.<br><br>b) Incorrect – doesn't guarantee determinism.<br><br>c) Incorrect – doesn't simplify hyperparameter tuning.<br><br>d) Incorrect – doesn't enforce identical predictions."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.5.1",
    "LearningObjectiveDescription": "Apply metamorphic testing for the testing of AI-based systems.",
    "KnowledgeType": "K3",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q229",
        "QuestionText": "Which example demonstrates a valid metamorphic relation?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Rotating an image should not change its class",
          "b": "Doubling input noise should double output",
          "c": "Shuffling sentences without changing meaning should preserve sentiment",
          "d": "Removing punctuation must not alter classification"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>a) Correct</strong> – rotating an image should not change its class (valid metamorphic relation).<br><br>b) Incorrect – doubling noise shouldn't necessarily double output.<br><br>c) Valid but a is the best example of metamorphic relation.<br><br>d) Valid but a is the most fundamental example."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.5.1",
    "LearningObjectiveDescription": "Apply metamorphic testing for the testing of AI-based systems.",
    "KnowledgeType": "K3",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q230",
        "QuestionText": "Which metamorphic testing principle is most relevant for AI systems?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Testing properties rather than specific outputs",
          "b": "Requiring identical outputs for similar inputs",
          "c": "Eliminating all statistical variance",
          "d": "Focusing only on deterministic systems"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>a) Correct</strong> – testing properties rather than specific outputs is key for AI systems.<br><br>b) Incorrect – identical outputs aren't required for similar inputs in AI.<br><br>c) Incorrect – statistical variance is inherent in AI systems.<br><br>d) Incorrect – metamorphic testing works with probabilistic systems."
      }
    ]
  },
  {
    "LearningObjective": "AI-2.7.1",
    "LearningObjectiveDescription": "Explain how transparency, interpretability and explainability apply to AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q231",
        "QuestionText": "A healthcare organization is developing an AI-based diagnostic system that will use pre-trained models from medical imaging research. The testing team is concerned about the risks associated with using these pre-trained models. Which TWO of the following represent the most significant testing challenges that should be addressed in the test strategy?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "The pre-trained models may have been trained on datasets with different demographic distributions, potentially introducing sample bias that affects diagnostic accuracy for underrepresented populations",
          "b": "The computational requirements for running pre-trained models may exceed the available hardware in clinical settings",
          "c": "Lack of transparency in the original training data and methodology may make it difficult to validate the model's decisions and ensure regulatory compliance",
          "d": "Pre-trained models typically require extensive customization of the user interface to match hospital workflow requirements"
        },
        "CorrectAnswer": [
          "a",
          "c"
        ],
        "Explanation": "a) CORRECT - Sample bias from different demographic distributions is a significant risk when using pre-trained models, especially in healthcare where underrepresented populations may receive inaccurate diagnoses.<br>b) INCORRECT - While computational requirements are important, they are not among the most significant testing challenges for pre-trained models compared to bias and transparency issues.<br><strong>c) CORRECT</strong> - Lack of transparency in training data and methodology creates major validation and regulatory compliance challenges, particularly in regulated healthcare environments.<br>d) INCORRECT - UI customization is a general software implementation concern, not a core testing challenge specific to pre-trained AI models."
      }
    ]
  },
  {
    "LearningObjective": "AI-8.2.1",
    "LearningObjectiveDescription": "Describe how autonomous AI-based systems are tested.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q232",
        "QuestionText": "An autonomous vehicle company is testing their self-driving system's ability to handle unexpected road conditions. During testing, the vehicle encounters a construction zone with temporary lane markings that conflict with the permanent markings. The system becomes confused and requests human intervention. Which characteristic of autonomous AI-based systems is being tested in this scenario, and what is the primary testing objective?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Testing flexibility - to verify the system can adapt to new road layouts without prior training",
          "b": "Testing autonomy - to verify the system correctly identifies situations where it should relinquish control to human operators",
          "c": "Testing evolution - to assess how quickly the system can learn from this new scenario for future encounters",
          "d": "Testing explainability - to understand why the system became confused by the conflicting lane markings"
        },
        "CorrectAnswer": "b",
        "Explanation": "a) INCORRECT - While flexibility is important, the scenario specifically shows the system recognizing its limits and requesting human intervention, which is about autonomy management.<br><strong>b) CORRECT</strong> - The system correctly identifies a situation beyond its autonomous capabilities and properly relinquishes control to human operators, demonstrating appropriate autonomy behavior.<br>c) INCORRECT - Evolution testing would focus on learning from the scenario, but here the system is not learning - it's properly handing off control.<br>d) INCORRECT - While understanding why the system became confused is valuable, the primary objective here is verifying proper autonomy behavior, not explainability."
      }
    ]
  },
  {
    "LearningObjective": "AI-8.1.1",
    "LearningObjectiveDescription": "Explain the challenges in testing created by the self-learning of AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q233",
        "QuestionText": "A financial trading firm is implementing a self-learning AI system that continuously adapts to market conditions. The testing team needs to address several challenges specific to testing self-learning systems. Which TWO of the following present the most significant testing challenges that require specialized approaches?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "The system's behavior may change unexpectedly due to self-learning, making previously passing tests fail and requiring continuous test maintenance",
          "b": "Defining complex acceptance criteria that specify expected improvements when the system self-learns, rather than just maintaining current performance levels",
          "c": "The system requires significantly more storage space for logging self-learning activities compared to static systems",
          "d": "Testing may inadvertently influence the system's learning process, potentially causing it to develop undesirable behaviors optimized for test scenarios rather than real-world performance"
        },
        "CorrectAnswer": [
          "a",
          "d"
        ],
        "Explanation": "<strong>a) CORRECT</strong> - Self-learning systems require continuous test maintenance as their behavior evolves, representing a significant testing challenge.<br>b) INCORRECT - Defining acceptance criteria for improvements is challenging but less critical than the dynamic nature of the system and test interference issues.<br>c) INCORRECT - Storage requirements are an implementation concern, not a core testing challenge specific to self-learning systems.<br><strong>d) CORRECT</strong> - Test interference with the learning process can cause the system to optimize for test scenarios rather than real-world performance, a major testing challenge."
      }
    ]
  },
  {
    "LearningObjective": "AI-4.3.1",
    "LearningObjectiveDescription": "Describe typical dataset quality issues.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q234",
        "QuestionText": "During the testing of a medical diagnosis AI system, testers discover that the model performs exceptionally well on the validation dataset but shows significantly lower accuracy when deployed in real clinical settings. Further investigation reveals that the training data primarily consisted of high-quality digital images from research hospitals, while real-world data includes lower-quality images from various community clinics. Which specific data quality issue is most likely causing this performance discrepancy, and what testing approach should have been used to detect it earlier?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "Sample bias - Input data testing with statistical analysis to ensure training data represents operational data diversity",
          "b": "Mislabeled data - Enhanced data labelling verification processes with multiple annotators",
          "c": "Obsolete data - Regular updates to training datasets with current medical imaging standards",
          "d": "Insufficient data - Increasing the size of the training dataset with more diverse sources"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>a) CORRECT</strong> - The performance discrepancy is caused by sample bias where training data (high-quality research images) doesn't represent operational data (lower-quality clinic images). Input data testing with statistical analysis would have detected this.<br>b) INCORRECT - The issue is not mislabeled data but rather the difference in data quality and characteristics between training and operational environments.<br>c) INCORRECT - The data isn't obsolete; it's from a different context (research vs. community clinics) with different quality characteristics.<br>d) INCORRECT - The problem isn't insufficient data quantity but rather insufficient data diversity and representativeness."
      }
    ]
  },
  {
    "LearningObjective": "AI-8.6.1",
    "LearningObjectiveDescription": "Describe how the transparency, interpretability and explainability of AI-based systems can be tested.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q235",
        "QuestionText": "A testing team is evaluating the transparency, interpretability, and explainability of a loan approval AI system. The system uses a complex deep neural network that has demonstrated high accuracy but operates as a \"black box.\" Regulatory requirements mandate that the system must provide explanations for its decisions. Which TWO testing approaches would be most effective for verifying the system meets these explainability requirements?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Using model-agnostic explanation methods like LIME to generate local explanations for individual predictions and verify they align with domain expertise",
          "b": "Conducting white-box testing of the neural network architecture to achieve full neuron coverage and ensure all activation patterns are exercised",
          "c": "Performing exploratory testing with domain experts to assess whether the provided explanations are understandable and meaningful for different stakeholder groups",
          "d": "Implementing comprehensive performance testing to ensure explanation generation doesn't significantly impact system response times"
        },
        "CorrectAnswer": [
          "a",
          "c"
        ],
        "Explanation": "<strong>a) CORRECT</strong> - Model-agnostic explanation methods like LIME can generate local explanations for black-box models and allow verification against domain expertise.<br>b) INCORRECT - White-box testing of neural network architecture doesn't address explainability requirements for regulatory compliance and stakeholder understanding.<br><strong>c) CORRECT</strong> - Exploratory testing with domain experts assesses whether explanations are actually understandable and meaningful to different stakeholders.<br>d) INCORRECT - Performance testing is important but doesn't directly verify that explanations meet regulatory requirements for transparency."
      }
    ]
  },
  {
    "LearningObjective": "HO-5.4.1",
    "LearningObjectiveDescription": "Evaluate the created ML model using selected ML functional performance metrics.",
    "KnowledgeType": "H2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q236",
        "QuestionText": "A testing team needs to evaluate an ML model for a medical diagnosis system where missing actual positive cases (false negatives) could have severe consequences. Which TWO ML functional performance metrics should be prioritized, and what are their correct interpretations in this context?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "High Recall is critical because it measures the proportion of actual positive cases that are correctly identified, minimizing missed diagnoses",
          "b": "High Precision should be the primary focus to ensure that when the system flags a case as positive, it is highly likely to be correct",
          "c": "F1-score provides the optimal balance since both false positives and false negatives carry equal weight in medical contexts",
          "d": "The confusion matrix should show low False Negative rates, indicating the system rarely misses actual positive cases"
        },
        "CorrectAnswer": [
          "a",
          "d"
        ],
        "Explanation": "<strong>a) CORRECT</strong> - High recall minimizes false negatives, which is critical in medical diagnosis where missed cases have severe consequences.<br>b) INCORRECT - High precision minimizes false positives, which is less critical than minimizing false negatives in this life-threatening context.<br>c) INCORRECT - F1-score balances both error types, but in medical diagnosis with severe consequences for missed cases, recall should be prioritized over balance.<br><strong>d) CORRECT</strong> - Low false negative rates directly address the risk of missing actual positive cases, which is the primary concern."
      }
    ]
  },
  {
    "LearningObjective": "AI-7.1.1",
    "LearningObjectiveDescription": "Explain how system specifications for AI-based systems can create challenges in testing.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q237",
        "QuestionText": "An AI-based natural language processing system is being developed with specifications that primarily consist of high-level business goals rather than detailed functional requirements. The testing team is struggling to define precise expected results for test cases. What is the fundamental challenge in testing this AI-based system, and which testing technique would be most effective?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "The test oracle problem - using metamorphic testing to verify that certain relationships between inputs and outputs hold true",
          "b": "Insufficient computational resources - focusing on performance testing to optimize resource usage",
          "c": "Over-specification of requirements - applying model-based testing to generate precise test cases",
          "d": "Lack of training data - prioritizing data acquisition over testing activities"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>a) CORRECT</strong> - The test oracle problem occurs when expected results are hard to define, and metamorphic testing verifies relationships between inputs and outputs when exact results are unknown.<br>b) INCORRECT - Computational resources are not the fundamental challenge; the issue is defining expected results from high-level specifications.<br>c) INCORRECT - The problem is under-specification, not over-specification of requirements.<br>d) INCORRECT - While training data is important, the core challenge is the test oracle problem, not data acquisition."
      }
    ]
  },
  {
    "LearningObjective": "AI-7.2.1",
    "LearningObjectiveDescription": "Describe how AI-based systems are tested at each test level.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q238",
        "QuestionText": "A financial institution is implementing an AI-based fraud detection system and needs to establish appropriate test levels. The system includes data pipelines, ML models, and integration with existing banking infrastructure. Which TWO specialized test levels for AI-based systems are most critical and distinct from conventional software testing approaches?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Input Data Testing - focusing on data quality, bias detection, and statistical properties of training datasets",
          "b": "ML Model Testing - evaluating ML functional performance metrics and model-specific characteristics",
          "c": "Unit Testing - of individual software components, which follows identical methodologies for AI and non-AI systems",
          "d": "Database Testing - ensuring data integrity and performance, which is common to all database-driven applications"
        },
        "CorrectAnswer": [
          "a",
          "b"
        ],
        "Explanation": "<strong>a) CORRECT</strong> - Input Data Testing is a specialized AI test level focusing on data quality, bias, and statistical properties distinct from conventional testing.<br><strong>b) CORRECT</strong> - ML Model Testing evaluates ML-specific performance metrics and characteristics, representing a unique test level for AI systems.<br>c) INCORRECT - Unit Testing follows similar methodologies regardless of AI implementation and is not specialized for AI systems.<br>d) INCORRECT - Database Testing is common to all database-driven applications and not specific to AI-based systems."
      }
    ]
  },
  {
    "LearningObjective": "AI-8.2.1",
    "LearningObjectiveDescription": "Describe how autonomous AI-based systems are tested.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q239",
        "QuestionText": "An autonomous drone delivery system is being tested to verify its ability to operate independently while properly handling situations requiring human intervention. During testing, the drone encounters severe weather conditions beyond its operational parameters. What is the primary testing objective for autonomy verification in this scenario?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "To verify the system recognizes operational boundary conditions and appropriately requests human intervention when environmental conditions exceed safe autonomous operation limits",
          "b": "To ensure the system continues autonomous operation regardless of weather conditions to maintain delivery schedules",
          "c": "To test the system's ability to learn from new weather patterns and adapt its behavior without human input",
          "d": "To validate that the system's explainability features can clearly describe why weather conditions are challenging"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>a) CORRECT</strong> - The primary objective is verifying the system recognizes its operational boundaries and properly requests human intervention when conditions exceed safe autonomous operation limits.<br>b) INCORRECT - Continuing autonomous operation in unsafe conditions would be dangerous and contrary to proper autonomy behavior.<br>c) INCORRECT - Testing learning capability is separate from verifying proper autonomy boundary management.<br>d) INCORRECT - Explainability is valuable but not the primary objective for autonomy verification in boundary conditions."
      }
    ]
  },
  {
    "LearningObjective": "AI-8.3.1",
    "LearningObjectiveDescription": "Explain how to test for bias in an AI-based system.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q240",
        "QuestionText": "A recruitment AI system is suspected of showing gender bias in candidate screening. The testing team needs to identify and verify the presence of bias. Which TWO testing approaches would be most effective for detecting algorithmic bias in this system?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Analyzing system outputs across different demographic groups and measuring performance disparities that cannot be justified by job-relevant qualifications",
          "b": "Reviewing the training data sources and collection methods to identify potential sample bias in the original dataset",
          "c": "Focusing exclusively on overall accuracy metrics, as high accuracy indicates the system is unbiased",
          "d": "Testing only with synthetic data to ensure complete control over input variables"
        },
        "CorrectAnswer": [
          "a",
          "b"
        ],
        "Explanation": "<strong>a) CORRECT</strong> - Analyzing performance disparities across demographic groups that cannot be justified by job-relevant qualifications is a direct method for detecting algorithmic bias.<br><strong>b) CORRECT</strong> - Reviewing training data sources and collection methods helps identify sample bias that may lead to algorithmic bias.<br>c) INCORRECT - High overall accuracy can mask significant bias against specific demographic groups, making it an inadequate measure of fairness.<br>d) INCORRECT - Testing only with synthetic data may not reveal real-world bias patterns and could miss important bias manifestations."
      }
    ]
  },
  {
    "LearningObjective": "HO-5.4.1",
    "LearningObjectiveDescription": "Evaluate the created ML model using selected ML functional performance metrics.",
    "KnowledgeType": "H2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q241",
        "QuestionText": "A testing team is evaluating an ML model for cancer detection where false negatives (missed cancer cases) could have life-threatening consequences, while false positives lead to unnecessary follow-up tests. The team needs to select appropriate ML functional performance metrics. Which TWO metrics and their interpretations are most critical for this medical context?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "High Recall is essential because it measures the proportion of actual cancer cases correctly identified, directly addressing the risk of missed diagnoses",
          "b": "High Precision should be prioritized to minimize unnecessary anxiety and medical procedures from false alarms",
          "c": "F1-score provides the optimal balance since both error types have equal medical significance in cancer detection",
          "d": "The confusion matrix should show minimal False Negatives, indicating the system rarely misses actual positive cancer cases"
        },
        "CorrectAnswer": [
          "a",
          "d"
        ],
        "Explanation": "<strong>a) CORRECT</strong> - High recall is essential for cancer detection as it directly addresses the life-threatening risk of missed diagnoses (false negatives).<br>b) INCORRECT - While precision is important, in cancer detection the consequences of false negatives outweigh those of false positives.<br>c) INCORRECT - F1-score balances both error types, but in life-threatening cancer detection, minimizing false negatives should be prioritized.<br><strong>d) CORRECT</strong> - Minimal false negatives in the confusion matrix directly indicates the system rarely misses actual cancer cases, which is critical."
      }
    ]
  },
  {
    "LearningObjective": "AI-7.1.1",
    "LearningObjectiveDescription": "Explain how system specifications for AI-based systems can create challenges in testing.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q242",
        "QuestionText": "An AI startup is developing a recommendation system with specifications that primarily consist of high-level business objectives like \"increase user engagement\" and \"improve content discovery.\" The testing team struggles to create precise test cases with defined expected results. What is the fundamental challenge in testing AI-based systems that this scenario illustrates, and which testing approach is most appropriate?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "The exploratory nature of AI development often results in incomplete specifications, requiring experience-based and exploratory testing approaches",
          "b": "AI systems inherently lack any form of specification, making traditional testing approaches completely ineffective",
          "c": "The main issue is technical complexity; increasing unit test coverage would resolve the specification challenges",
          "d": "AI specifications are always complete but written in mathematical notation inaccessible to testers"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>a) CORRECT</strong> - The exploratory nature of AI development often leads to incomplete specifications, requiring experience-based and exploratory testing approaches that don't rely on precise expected results.<br>b) INCORRECT - AI systems do have specifications, though they may be high-level business objectives rather than detailed functional requirements.<br>c) INCORRECT - Technical complexity and unit testing don't address the fundamental challenge of incomplete specifications for defining test cases.<br>d) INCORRECT - AI specifications are not always complete nor exclusively in mathematical notation; the issue is their high-level, business-oriented nature."
      }
    ]
  },
  {
    "LearningObjective": "AI-7.2.1",
    "LearningObjectiveDescription": "Describe how AI-based systems are tested at each test level.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q243",
        "QuestionText": "A financial services company is implementing an AI-based risk assessment system and establishing comprehensive test levels. The system includes data pipelines, ML models, and integration with existing banking infrastructure. Which TWO specialized test levels for AI-based systems are most critical and distinct from conventional software testing approaches?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Input Data Testing - focusing on statistical analysis, bias detection, and quality assessment of training and operational datasets",
          "b": "ML Model Testing - evaluating ML functional performance metrics, model-specific characteristics, and algorithm selection adequacy",
          "c": "User Interface Testing - which follows identical methodologies regardless of whether the backend uses AI or conventional logic",
          "d": "Network Infrastructure Testing - ensuring proper connectivity and bandwidth, common to all distributed systems"
        },
        "CorrectAnswer": [
          "a",
          "b"
        ],
        "Explanation": "<strong>a) CORRECT</strong> - Input Data Testing focusing on statistical analysis, bias detection, and quality assessment is a specialized AI test level distinct from conventional testing.<br><strong>b) CORRECT</strong> - ML Model Testing evaluating ML performance metrics and algorithm selection is a unique test level specific to AI-based systems.<br>c) INCORRECT - User Interface Testing follows similar methodologies regardless of backend implementation and is not specialized for AI systems.<br>d) INCORRECT - Network Infrastructure Testing is common to all distributed systems and not specific to AI-based implementations."
      }
    ]
  },
  {
    "LearningObjective": "AI-8.2.1",
    "LearningObjectiveDescription": "Describe how autonomous AI-based systems are tested.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "1",
        "QuestionId": "q244",
        "QuestionText": "An autonomous industrial robot system is being tested to verify its ability to operate independently while properly handling situations requiring human intervention. During testing, the robot encounters an unexpected object obstruction that wasn't present during training. What is the primary testing objective for autonomy verification in this scenario?",
        "AnswerOption": "Select ONE option. (1 out of 4)",
        "Answers": {
          "a": "To verify the system recognizes its operational limits and appropriately requests human intervention when encountering unforeseen obstacles beyond its autonomous capabilities",
          "b": "To ensure the system continues autonomous operation and attempts to remove the obstruction without human assistance",
          "c": "To test the system's learning capability by allowing it to develop new strategies for handling similar obstructions in the future",
          "d": "To validate that the system's performance metrics remain unchanged despite the unexpected operational challenge"
        },
        "CorrectAnswer": "a",
        "Explanation": "<strong>a) CORRECT</strong> - The primary testing objective is verifying the system recognizes its operational limits and appropriately requests human intervention when encountering unforeseen obstacles beyond its autonomous capabilities.<br>b) INCORRECT - Continuing autonomous operation with unexpected obstructions could be dangerous and doesn't demonstrate proper autonomy boundary management.<br>c) INCORRECT - Testing learning capability is separate from verifying proper autonomy behavior in boundary situations.<br>d) INCORRECT - Maintaining performance metrics is not the primary concern; proper handling of operational limits is key for autonomy verification."
      }
    ]
  },
  {
    "LearningObjective": "AI-4.4.1",
    "LearningObjectiveDescription": "Recognize how poor data quality can cause problems with the resultant ML model.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q245",
        "QuestionText": "A financial institution developed a credit scoring AI model that consistently approves loans for applicants from urban areas while rejecting qualified applicants from rural regions, despite similar financial profiles. The training data primarily contained financial histories of urban customers with higher average incomes. Which TWO data quality issues most likely contributed to this problematic ML model outcome?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Unbalanced training data that underrepresented rural applicants, creating sample bias in the model's decision patterns",
          "b": "Irrelevant geographic features that were incorrectly weighted by the algorithm, despite not being meaningful predictors of creditworthiness",
          "c": "Insufficient data volume that prevented the model from learning complex patterns across different demographic groups",
          "d": "Obsolete income thresholds that didn't account for regional cost-of-living differences, making rural applicants appear less creditworthy"
        },
        "CorrectAnswer": [
          "a",
          "c"
        ],
        "Explanation": "a) CORRECT – Unbalanced training data that underrepresented rural applicants creates sample bias, causing the model to perform poorly on underrepresented groups. Compare CT-AI curriculum Section 4.4 on data quality issues.<br>b) INCORRECT – While geographic features might be relevant, the primary issue is data imbalance rather than poor feature selection.<br>c) CORRECT – Insufficient data volume for rural applicants is a key factor in the model's poor performance on this demographic. Compare CT-AI curriculum Section 4.4 on data quantity issues.<br>d) INCORRECT – Obsolete income thresholds relate to data currency issues, but the core problem is insufficient rural data volume and distribution."
      }
    ]
  },
  {
    "LearningObjective": "HO-3.5.1",
    "LearningObjectiveDescription": "Demonstrate underfitting and overfitting.",
    "KnowledgeType": "H0",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q246",
        "QuestionText": "During a machine learning workshop, participants are training a model to predict housing prices. One team's model achieves 98% accuracy on training data but only 65% on validation data. Another team's model shows 70% accuracy on both training and validation datasets but fails to capture clear seasonal price patterns visible in the data. Which TWO issues are being demonstrated in this scenario?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "The first team's model exhibits overfitting, memorizing training data noise rather than learning generalizable patterns",
          "b": "The second team's model suffers from underfitting, being too simplistic to capture the underlying data relationships",
          "c": "Both teams are experiencing data poisoning attacks that manipulated their training datasets",
          "d": "The validation datasets for both teams contain significant measurement errors affecting accuracy calculations"
        },
        "CorrectAnswer": [
          "a",
          "d"
        ],
        "Explanation": "a) CORRECT – The first team's high training accuracy but low validation accuracy demonstrates classic overfitting where the model memorizes training noise. Compare CT-AI curriculum Section 3.5 on overfitting.<br>b) INCORRECT – The second team's performance pattern doesn't clearly indicate underfitting; more context is needed.<br>c) INCORRECT – There's no evidence of data poisoning attacks in this scenario.<br>d) CORRECT – The accuracy discrepancies between training and validation datasets suggest potential validation data issues or improper splitting. Compare CT-AI curriculum Section 4.2 on dataset validation."
      }
    ]
  },
  {
    "LearningObjective": "HO-4.1.1",
    "LearningObjectiveDescription": "Perform data preparation in support of the creation of an ML model.",
    "KnowledgeType": "H2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q247",
        "QuestionText": "A data science team is preparing a customer churn prediction dataset. The raw data contains missing values in the \"customer_tenure\" field, categorical variables like \"subscription_type\" stored as text, and numerical features with different scales. Which TWO data preparation activities are most critical to perform before model training can begin?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Data cleaning to handle missing values through imputation or removal of incomplete records",
          "b": "Data transformation to convert categorical variables into numerical representations using one-hot encoding",
          "c": "Feature engineering to create entirely new synthetic features not present in the original dataset",
          "d": "Data augmentation by generating additional artificial customer records to increase dataset size"
        },
        "CorrectAnswer": [
          "b",
          "d"
        ],
        "Explanation": "a) INCORRECT – While data cleaning is important, it's not the most critical initial step compared to proper data transformation.<br><strong>b) CORRECT</strong> – Converting categorical variables to numerical representations is fundamental for most ML algorithms that require numerical input. Compare CT-AI curriculum Section 4.1 on data transformation.<br>c) INCORRECT – While feature engineering can be valuable, it's not the most critical initial step.<br><strong>d) CORRECT</strong> – Data augmentation can significantly improve model performance by creating additional training examples, especially for limited datasets. Compare CT-AI curriculum Section 4.1 on data augmentation techniques."
      }
    ]
  },
  {
    "LearningObjective": "HO-4.1.1",
    "LearningObjectiveDescription": "Perform data preparation in support of the creation of an ML model.",
    "KnowledgeType": "H2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q248",
        "QuestionText": "During a hands-on data preparation exercise, participants are working with a hotel booking dataset containing reservation dates, customer demographics, and booking channels. Which TWO practical data preparation steps would demonstrate proper handling of temporal and categorical data for ML model creation?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Extracting seasonal features from date fields (month, day_of_week, season) to capture temporal patterns",
          "b": "Applying min-max normalization to numerical features to bring all values into a consistent 0-1 range",
          "c": "Removing all categorical variables since ML algorithms cannot process non-numerical data",
          "d": "Using label encoding for high-cardinality categorical variables like customer IDs"
        },
        "CorrectAnswer": [
          "a",
          "d"
        ],
        "Explanation": "<strong>a) CORRECT</strong> – Extracting seasonal features from temporal data helps capture important patterns relevant to hotel booking behavior. Compare CT-AI curriculum Section 4.1 on feature engineering.<br>b) INCORRECT – Normalization is useful but not always necessary depending on the algorithm used.<br>c) INCORRECT – Categorical variables contain valuable information and should be properly encoded, not removed entirely.<br><strong>d) CORRECT</strong> – Avoiding label encoding for high-cardinality variables like IDs prevents introducing false ordinal relationships. Compare CT-AI curriculum Section 4.1 on categorical encoding best practices."
      }
    ]
  },
  {
    "LearningObjective": "HO-4.2.1",
    "LearningObjectiveDescription": "Identify training and test datasets and create an ML model.",
    "KnowledgeType": "H2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q249",
        "QuestionText": "A team is splitting their prepared e-commerce dataset for model development. The dataset contains 50,000 customer transactions with features like purchase amount, product categories, and customer demographics. Which TWO dataset splitting strategies would properly support the creation and evaluation of an ML model?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Using stratified sampling to maintain the same distribution of the target variable across training and validation sets",
          "b": "Implementing temporal splitting where recent data is used for testing to simulate real-world deployment conditions",
          "c": "Allocating 90% of data to training and 10% to testing without a separate validation set for hyperparameter tuning",
          "d": "Using the entire dataset for both training and testing to maximize model performance metrics"
        },
        "CorrectAnswer": [
          "a",
          "c"
        ],
        "Explanation": "<strong>a) CORRECT</strong> – Stratified sampling maintains target variable distribution across splits, ensuring representative datasets for model evaluation. Compare CT-AI curriculum Section 4.2 on dataset splitting.<br>b) INCORRECT – Temporal splitting may not be appropriate for all scenarios and can introduce time-based biases.<br><strong>c) CORRECT</strong> – Without a separate validation set, hyperparameter tuning would be performed on the test set, leading to overfitting and unreliable performance estimates. Compare CT-AI curriculum Section 4.2 on validation strategies.<br>d) INCORRECT – Using the same data for training and testing prevents proper evaluation of model generalization capability."
      }
    ]
  },
  {
    "LearningObjective": "HO-4.2.1",
    "LearningObjectiveDescription": "Identify training and test datasets and create an ML model.",
    "KnowledgeType": "H2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q250",
        "QuestionText": "During a practical ML exercise, participants have prepared a healthcare dataset for predicting patient readmission risk. They need to create both training and test datasets and build an initial model. Which TWO actions demonstrate proper understanding of dataset usage in ML workflow?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Training the model exclusively on the training dataset and evaluating final performance on the completely separate test dataset",
          "b": "Using k-fold cross-validation on the training data to tune hyperparameters while keeping test data untouched",
          "c": "Periodically evaluating model performance on the test dataset during training to monitor progress",
          "d": "Combining training and test datasets for final model training after satisfactory validation results"
        },
        "CorrectAnswer": [
          "b",
          "d"
        ],
        "Explanation": "a) INCORRECT – While test data separation is important, the described approach may be too rigid for iterative development.<br><strong>b) CORRECT</strong> – Using cross-validation for hyperparameter tuning while preserving test data integrity is a standard ML practice. Compare CT-AI curriculum Section 4.2 on validation techniques.<br>c) INCORRECT – Evaluating on test data during training contaminates the test set and leads to overfitting.<br><strong>d) CORRECT</strong> – Combining training and validation sets for final model training after hyperparameter tuning can improve model performance by utilizing more data. Compare CT-AI curriculum Section 4.2 on model training strategies."
      }
    ]
  },
  {
    "LearningObjective": "AI-7.1.1",
    "LearningObjectiveDescription": "Explain how system specifications for AI-based systems can create challenges in testing.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q251",
        "QuestionText": "A testing team is reviewing specifications for an AI-based credit scoring system. The requirements document states high-level goals like \"improve loan approval accuracy\" and \"reduce default rates\" but lacks detailed functional specifications. Which TWO challenges in testing AI-based systems are demonstrated by this specification approach?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Test oracle problem - difficulty in determining expected results for specific test cases",
          "b": "Inadequate acceptance criteria - vague requirements make it hard to determine when the system is acceptable",
          "c": "Over-specification - too many detailed requirements constraining the AI's learning capability",
          "d": "Excessive transparency - requirements reveal too much about the intended algorithm behavior"
        },
        "CorrectAnswer": [
          "a",
          "c"
        ],
        "Explanation": "<strong>a) CORRECT</strong> – Vague specifications create test oracle problems where expected results are difficult to determine. Compare CT-AI curriculum Section 7.1 on specification challenges.<br>b) INCORRECT – The scenario focuses more on specification clarity than acceptance criteria adequacy.<br><strong>c) CORRECT</strong> – The problem described is under-specification, where requirements lack necessary detail for effective testing. Compare CT-AI curriculum Section 7.1 on specification quality.<br>d) INCORRECT – The scenario demonstrates insufficient detail, not transparency issues."
      }
    ]
  },
  {
    "LearningObjective": "AI-7.1.1",
    "LearningObjectiveDescription": "Explain how system specifications for AI-based systems can create challenges in testing.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q252",
        "QuestionText": "During a workshop on AI system specifications, participants are discussing how to address the exploratory nature of AI development. The project involves creating a recommendation engine where the exact capabilities will emerge during development. Which TWO specification strategies would best support testability in this context?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Defining tolerances for quality requirements like prediction accuracy ranges rather than fixed values",
          "b": "Specifying comparative performance metrics against existing systems or human benchmarks",
          "c": "Creating detailed functional specifications for all possible user interactions before development begins",
          "d": "Requiring complete algorithm transparency with full disclosure of all model parameters"
        },
        "CorrectAnswer": [
          "b",
          "d"
        ],
        "Explanation": "a) INCORRECT – Tolerance ranges may not provide sufficient specificity for complex AI system testing.<br><strong>b) CORRECT</strong> – Comparative metrics against existing benchmarks provide concrete acceptance criteria for exploratory AI development. Compare CT-AI curriculum Section 7.1 on testability approaches.<br>c) INCORRECT – Detailed pre-specification contradicts the exploratory nature of AI development described in the scenario.<br><strong>d) CORRECT</strong> – Defining clear behavioral boundaries rather than exact outputs accommodates AI system variability while maintaining testability. Compare CT-AI curriculum Section 7.1 on AI testing approaches."
      }
    ]
  },
  {
    "LearningObjective": "AI-7.2.1",
    "LearningObjectiveDescription": "Describe how AI-based systems are tested at each test level.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q253",
        "QuestionText": "A testing team is planning test levels for an autonomous delivery robot system that includes AI components for navigation and conventional components for package handling. Which TWO specialized test levels for AI-based systems should be included beyond conventional test levels?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Input Data Testing - validating the quality and appropriateness of training and operational data",
          "b": "ML Model Testing - evaluating model-specific performance criteria and algorithm selection",
          "c": "User Interface Testing - focusing on conventional GUI elements and user interactions",
          "d": "Database Testing - verifying data storage and retrieval operations in backend systems"
        },
        "CorrectAnswer": [
          "a",
          "c"
        ],
        "Explanation": "<strong>a) CORRECT</strong> – Input Data Testing is a specialized AI test level focusing on data quality and bias detection. Compare CT-AI curriculum Section 7.2 on AI-specific test levels.<br>b) INCORRECT – ML Model Testing shares similarities with conventional component testing approaches.<br><strong>c) CORRECT</strong> – User Interface Testing for AI systems requires specialized approaches to handle probabilistic outputs and user interactions. Compare CT-AI curriculum Section 7.2 on AI-specific testing challenges.<br>d) INCORRECT – Database Testing is a conventional test level applicable to all systems with database components."
      }
    ]
  },
  {
    "LearningObjective": "AI-7.2.1",
    "LearningObjectiveDescription": "Describe how AI-based systems are tested at each test level.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q254",
        "QuestionText": "During a test planning session for a medical diagnosis AI system, the team is discussing how to test at different levels. The system includes data pipelines, ML models for image analysis, and integration with hospital record systems. Which TWO testing approaches demonstrate proper understanding of AI system test levels?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Conducting component integration testing to verify proper data flow between AI models and other system components",
          "b": "Performing ML model testing in isolation to establish baseline performance before system integration",
          "c": "Testing only the conventional components since AI components are too complex to test systematically",
          "d": "Combining all test levels into a single system testing phase to reduce testing overhead"
        },
        "CorrectAnswer": [
          "b",
          "d"
        ],
        "Explanation": "a) INCORRECT – Component integration testing follows conventional approaches and isn't AI-specific.<br><strong>b) CORRECT</strong> – Isolated ML model testing establishes baseline performance before system integration complexities. Compare CT-AI curriculum Section 7.2 on ML model testing.<br>c) INCORRECT – AI components require systematic testing just like conventional components.<br><strong>d) CORRECT</strong> – Separate AI-specific test levels address the unique characteristics and challenges of AI components. Compare CT-AI curriculum Section 7.2 on AI test level organization."
      }
    ]
  },
  {
    "LearningObjective": "AI-8.2.1",
    "LearningObjectiveDescription": "Describe how autonomous AI-based systems are tested.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q255",
        "QuestionText": "A testing team is evaluating an autonomous warehouse management system that can operate for extended periods without human intervention. Which TWO testing scenarios would effectively verify the system's autonomy capabilities?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Simulating equipment failures to verify the system appropriately requests human intervention",
          "b": "Testing boundary conditions where system capabilities reach operational limits",
          "c": "Verifying the system never requests human assistance under any circumstances",
          "d": "Testing only normal operating conditions to establish baseline autonomy performance"
        },
        "CorrectAnswer": [
          "a",
          "d"
        ],
        "Explanation": "<strong>a) CORRECT</strong> – Testing failure scenarios verifies the system appropriately requests human intervention when needed. Compare CT-AI curriculum Section 8.2 on testing autonomy.<br>b) INCORRECT – Boundary condition testing is important but not the primary focus for autonomy verification.<br>c) INCORRECT – Autonomous systems should request human intervention when exceeding their capabilities.<br><strong>d) CORRECT</strong> – Testing that the system maintains autonomy within its verified operational domain ensures reliable performance. Compare CT-AI curriculum Section 8.2 on autonomy boundary testing."
      }
    ]
  },
  {
    "LearningObjective": "AI-8.2.1",
    "LearningObjectiveDescription": "Describe how autonomous AI-based systems are tested.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q256",
        "QuestionText": "During a practical exercise on testing autonomous systems, participants are working with a self-driving car simulation. They need to create test cases that validate the car's decision-making about when to relinquish control to a human driver. Which TWO testing approaches would demonstrate proper testing of autonomy?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Creating scenarios with sudden weather deterioration beyond the system's sensor capabilities",
          "b": "Testing prolonged operation periods to verify sustained autonomous performance",
          "c": "Verifying the system maintains autonomy even when critical sensors fail",
          "d": "Testing unnecessary intervention requests during normal operating conditions"
        },
        "CorrectAnswer": [
          "b",
          "c"
        ],
        "Explanation": "a) INCORRECT – Testing beyond sensor capabilities should trigger intervention requests, not continued autonomy.<br><strong>b) CORRECT</strong> – Prolonged operation testing verifies sustained autonomous performance without degradation. Compare CT-AI curriculum Section 8.2 on testing autonomy duration.<br><strong>c) CORRECT</strong> – Critical sensor failures should trigger appropriate intervention requests and safe system behavior. Compare CT-AI curriculum Section 8.2 on failure scenario testing.<br>d) INCORRECT – Testing unnecessary interventions verifies the system doesn't relinquish control inappropriately."
      }
    ]
  },
  {
    "LearningObjective": "AI-8.3.1",
    "LearningObjectiveDescription": "Explain how to test for bias in an AI-based system.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q257",
        "QuestionText": "A testing team is evaluating a resume screening AI system for potential hiring bias. The system shows significantly different recommendation rates for candidates from different demographic groups, even with similar qualifications. Which TWO testing approaches would effectively identify the sources of bias?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Analyzing training data sources and acquisition processes for sample bias",
          "b": "Applying LIME-like methods to measure how input changes affect outputs across different groups",
          "c": "Testing only with synthetic data to eliminate real-world bias variables",
          "d": "Focusing exclusively on overall accuracy metrics without demographic analysis"
        },
        "CorrectAnswer": [
          "b",
          "c"
        ],
        "Explanation": "a) INCORRECT – Training data source analysis may not capture all bias patterns in complex models.<br><strong>b) CORRECT</strong> – LIME-like methods measure input-output relationships to detect algorithmic bias patterns. Compare CT-AI curriculum Section 8.3 on bias detection techniques.<br><strong>c) CORRECT</strong> – Synthetic data generation can help identify bias by testing model behavior on carefully constructed demographic variations. Compare CT-AI curriculum Section 8.3 on bias testing approaches.<br>d) INCORRECT – Overall accuracy can mask biased performance across different demographic groups."
      }
    ]
  },
  {
    "LearningObjective": "AI-8.3.1",
    "LearningObjectiveDescription": "Explain how to test for bias in an AI-based system.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q258",
        "QuestionText": "During a hands-on bias testing workshop, participants are working with a loan approval dataset. They need to identify both algorithmic and sample bias in the model. Which TWO practical testing techniques would demonstrate comprehensive bias detection?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Correlating model outcomes with protected demographic attributes not used as direct inputs",
          "b": "Reviewing data pre-processing steps for transformations that might introduce bias",
          "c": "Testing only with perfectly balanced datasets to avoid bias concerns",
          "d": "Assuming bias is eliminated if overall accuracy meets business requirements"
        },
        "CorrectAnswer": [
          "a",
          "d"
        ],
        "Explanation": "<strong>a) CORRECT</strong> – Correlating outcomes with protected attributes detects bias even when not explicitly used as inputs. Compare CT-AI curriculum Section 8.3 on hidden variable bias.<br>b) INCORRECT – Pre-processing steps review is important but may not reveal all bias sources.<br>c) INCORRECT – Perfectly balanced datasets may not represent real-world operational conditions.<br><strong>d) CORRECT</strong> – Testing model performance consistency across different data slices reveals biased behavior patterns. Compare CT-AI curriculum Section 8.3 on slice-based testing."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.1.1",
    "LearningObjectiveDescription": "Explain how the testing of ML systems can help prevent adversarial attacks and data poisoning.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q259",
        "QuestionText": "A security team is concerned about potential adversarial attacks on their image recognition system used for facial authentication. Which TWO testing approaches would help prevent successful adversarial attacks?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Incorporating adversarial examples into training data to improve model robustness",
          "b": "Conducting adversarial testing to identify vulnerabilities before deployment",
          "c": "Relying solely on traditional functional testing without specific adversarial checks",
          "d": "Implementing data augmentation only with clean, unperturbed images"
        },
        "CorrectAnswer": [
          "a",
          "d"
        ],
        "Explanation": "<strong>a) CORRECT</strong> – Incorporating adversarial examples during training improves model robustness against attacks. Compare CT-AI curriculum Section 9.1 on adversarial defense.<br>b) INCORRECT – Proactive adversarial testing is valuable but doesn't directly improve model training.<br>c) INCORRECT – Traditional testing alone may not detect subtle adversarial perturbations.<br><strong>d) CORRECT</strong> – Data augmentation with adversarial examples creates more robust models resistant to manipulation. Compare CT-AI curriculum Section 9.1 on robust training techniques."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.1.1",
    "LearningObjectiveDescription": "Explain how the testing of ML systems can help prevent adversarial attacks and data poisoning.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q260",
        "QuestionText": "During a security testing workshop for ML systems, participants are discussing protection against data poisoning. The system uses publicly available training data and user feedback for continuous learning. Which TWO testing strategies would effectively prevent data poisoning attacks?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Implementing EDA to detect outliers and anomalous patterns in training data",
          "b": "Establishing data provenance policies to verify training data sources",
          "c": "Assuming public datasets are inherently safe due to widespread usage",
          "d": "Using A/B testing to detect performance degradation from potential poisoning"
        },
        "CorrectAnswer": [
          "b",
          "c"
        ],
        "Explanation": "a) INCORRECT – EDA may not detect sophisticated poisoning attempts designed to evade detection.<br><strong>b) CORRECT</strong> – Data provenance policies verify training data sources to prevent malicious injections. Compare CT-AI curriculum Section 9.1 on data acquisition security.<br><strong>c) CORRECT</strong> – Monitoring data distributions for unexpected shifts can detect potential poisoning attempts. Compare CT-AI curriculum Section 9.1 on poisoning detection.<br>d) INCORRECT – A/B testing detects performance changes but may not prevent poisoning attacks."
      }
    ]
  },
  {
    "LearningObjective": "HO-9.2.1",
    "LearningObjectiveDescription": "Apply pairwise testing to derive and execute test cases for an AI-based system.",
    "KnowledgeType": "H2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q261",
        "QuestionText": "A testing team is applying pairwise testing to an AI-based automotive system with multiple parameters: road_type (urban, highway, rural), weather (clear, rain, snow), time_of_day (day, night), and sensor_accuracy (high, medium, low). Which TWO benefits does pairwise testing provide in this context?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Significant reduction in test cases while covering all pairs of parameter interactions",
          "b": "Effective detection of defects caused by interactions between two parameters",
          "c": "Guaranteed coverage of all possible multi-parameter combination defects",
          "d": "Elimination of the need for any other testing techniques"
        },
        "CorrectAnswer": [
          "b",
          "d"
        ],
        "Explanation": "a) INCORRECT – Pairwise testing reduces test cases but doesn't guarantee coverage of all critical interactions.<br><strong>b) CORRECT</strong> – Research shows most defects are caused by interactions between pairs of parameters. Compare CT-AI curriculum Section 9.2 on pairwise effectiveness.<br>c) INCORRECT – Pairwise testing doesn't guarantee coverage of all multi-parameter combination defects.<br><strong>d) CORRECT</strong> – Pairwise testing provides systematic coverage with manageable test suite sizes. Compare CT-AI curriculum Section 9.2 on combinatorial testing benefits."
      }
    ]
  },
  {
    "LearningObjective": "HO-9.2.1",
    "LearningObjectiveDescription": "Apply pairwise testing to derive and execute test cases for an AI-based system.",
    "KnowledgeType": "H2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q262",
        "QuestionText": "During a pairwise testing practical exercise, participants are generating test cases for a bike classification model with parameters: tire_width (23, 25, 28, 32, 38, 45mm), num_gears (1, 3, 7, 11, 18, 21), and chainrings (1, 2, 3). Which TWO approaches demonstrate proper application of pairwise testing?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Using a pairwise tool to generate test cases covering all pairs of parameter values",
          "b": "Manually selecting test cases that seem most representative of real-world usage",
          "c": "Executing the pairwise-generated test cases and analyzing model predictions",
          "d": "Testing all 108 possible combinations to ensure exhaustive coverage"
        },
        "CorrectAnswer": [
          "a",
          "d"
        ],
        "Explanation": "<strong>a) CORRECT</strong> – Using pairwise tools systematically generates test cases covering all parameter value pairs. Compare CT-AI curriculum Section 9.2 on pairwise testing implementation.<br>b) INCORRECT – Manual selection may miss important parameter interactions covered by systematic pairwise generation.<br>c) INCORRECT – Executing test cases is implementation, not necessarily demonstrating understanding of the technique.<br><strong>d) CORRECT</strong> – Comparing pairwise results with exhaustive testing validates the technique's effectiveness. Compare CT-AI curriculum Section 9.2 on pairwise testing validation."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.3.1",
    "LearningObjectiveDescription": "Explain how back-to-back testing is used for AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q263",
        "QuestionText": "A testing team is using back-to-back testing for an AI-based medical diagnosis system where expected results are difficult to determine. They have developed a pseudo-oracle using a different ML framework. Which TWO conditions are essential for effective back-to-back testing in this scenario?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Ensuring no common software components between the SUT and pseudo-oracle",
          "b": "The pseudo-oracle doesn't need to meet the same non-functional requirements as the SUT",
          "c": "Using identical training data and hyperparameters for both systems",
          "d": "Requiring the pseudo-oracle to be more accurate than the SUT"
        },
        "CorrectAnswer": [
          "a",
          "c"
        ],
        "Explanation": "<strong>a) CORRECT</strong> – Avoiding common software components prevents shared defects from going undetected. Compare CT-AI curriculum Section 9.3 on back-to-back testing requirements.<br>b) INCORRECT – Pseudo-oracles should have comparable non-functional characteristics for valid comparison.<br><strong>c) CORRECT</strong> – Using identical training data and parameters ensures fair comparison between implementations. Compare CT-AI curriculum Section 9.3 on back-to-back testing setup.<br>d) INCORRECT – The pseudo-oracle doesn't need superior accuracy, just independent implementation."
      }
    ]
  },
  {
    "LearningObjective": "AI-9.3.1",
    "LearningObjectiveDescription": "Explain how back-to-back testing is used for AI-based systems.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q264",
        "QuestionText": "During a back-to-back testing exercise, participants are comparing outputs from their main image classification system with a pseudo-oracle built using a different architecture. They notice consistent differences in specific edge cases. Which TWO responses demonstrate proper understanding of back-to-back testing analysis?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Investigating the discrepancies to determine if they reveal defects in either system",
          "b": "Assuming the main system is always correct and ignoring pseudo-oracle differences",
          "c": "Using domain expertise to assess which system's outputs are more appropriate",
          "d": "Discarding the pseudo-oracle if any differences are found"
        },
        "CorrectAnswer": [
          "b",
          "c"
        ],
        "Explanation": "a) INCORRECT – Investigating all discrepancies may be inefficient; focus should be on significant differences.<br><strong>b) CORRECT</strong> – Assuming the main system is correct initially helps prioritize investigation efforts. Compare CT-AI curriculum Section 9.3 on back-to-back analysis prioritization.<br><strong>c) CORRECT</strong> – Domain expertise assesses which system's behavior aligns better with expected outcomes. Compare CT-AI curriculum Section 9.3 on result interpretation.<br>d) INCORRECT – Differences are expected and valuable for defect detection, not reasons for discarding the oracle."
      }
    ]
  },
  {
    "LearningObjective": "HO-9.5.1",
    "LearningObjectiveDescription": "Apply metamorphic testing to derive test cases for a given scenario and execute them.",
    "KnowledgeType": "H2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q265",
        "QuestionText": "A financial institution is testing a loan default prediction model using metamorphic testing. The team has identified several potential metamorphic relations based on domain knowledge about credit risk. Which TWO of the following represent valid metamorphic relations for testing the loan default prediction system?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Increasing the bank balance should decrease default likelihood for employed applicants",
          "b": "Changing employment status from employed to unemployed should increase default probability",
          "c": "Reducing annual salary to zero should dramatically increase default risk",
          "d": "All metamorphic relations should produce identical predictions to the source test case"
        },
        "CorrectAnswer": [
          "b",
          "d"
        ],
        "Explanation": "a) INCORRECT – The scenario specifically notes that defaulters have higher balances, contradicting this expected relationship.<br><strong>b) CORRECT</strong> – Unemployment typically increases default risk, making this a valid metamorphic relation. Compare CT-AI curriculum Section 9.5 on metamorphic testing principles.<br>c) INCORRECT – Zero salary dramatically increases default probability, but the relation needs proper formulation.<br><strong>d) CORRECT</strong> – Consistent predictions for similar input profiles represent a valid metamorphic relation. Compare CT-AI curriculum Section 9.5 on consistency relations."
      }
    ]
  },
  {
    "LearningObjective": "HO-9.5.1",
    "LearningObjectiveDescription": "Apply metamorphic testing to derive test cases for a given scenario and execute them.",
    "KnowledgeType": "H2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q266",
        "QuestionText": "During a metamorphic testing workshop, participants are testing a complex AI-based actuarial system that predicts life expectancy. The system's calculations are too complex to verify manually. Which TWO metamorphic testing approaches would provide confidence in system correctness?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Testing that increasing risk factors (like smoking) decreases predicted life expectancy",
          "b": "Verifying that reordering input parameters doesn't change the output",
          "c": "Requiring exact numeric matches with manual calculations for validation",
          "d": "Testing only with previously verified historical cases"
        },
        "CorrectAnswer": [
          "a",
          "c"
        ],
        "Explanation": "<strong>a) CORRECT</strong> – Testing that increased risk factors decrease life expectancy validates fundamental domain knowledge. Compare CT-AI curriculum Section 9.5 on metamorphic testing applications.<br>b) INCORRECT – Input reordering may affect outputs in systems with state or sequence dependencies.<br><strong>c) CORRECT</strong> – Avoiding exact numeric matches accommodates the probabilistic nature of AI systems. Compare CT-AI curriculum Section 9.5 on metamorphic relation design.<br>d) INCORRECT – Limited historical testing doesn't provide comprehensive system validation."
      }
    ]
  },
  {
    "LearningObjective": "HO-11.1.1",
    "LearningObjectiveDescription": "Discuss, using examples, those activities in testing where AI is less likely to be used.",
    "KnowledgeType": "H2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q267",
        "QuestionText": "During a discussion about AI applications in software testing, the team is evaluating which testing activities are currently less suitable for AI implementation. Which TWO testing activities would be most challenging for AI to perform effectively?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Subjective usability assessment requiring human emotional response and aesthetic judgment",
          "b": "Creative test design for novel systems without historical test data patterns",
          "c": "Regression test selection based on code changes and historical defect patterns",
          "d": "Test data generation for boundary value analysis of numerical parameters"
        },
        "CorrectAnswer": [
          "a",
          "c"
        ],
        "Explanation": "<strong>a) CORRECT</strong> – Subjective usability assessment requires human emotional intelligence difficult for AI to replicate. Compare CT-AI curriculum Section 11.1 on AI limitations in testing.<br>b) INCORRECT – Creative test design can be assisted by AI through pattern recognition and generation.<br><strong>c) CORRECT</strong> – Ethical judgment in testing scenarios requires human moral reasoning beyond current AI capabilities. Compare CT-AI curriculum Section 11.1 on testing activities requiring human judgment.<br>d) INCORRECT – Test data generation for boundary values can be effectively automated with AI."
      }
    ]
  },
  {
    "LearningObjective": "HO-11.1.1",
    "LearningObjectiveDescription": "Discuss, using examples, those activities in testing where AI is less likely to be used.",
    "KnowledgeType": "H2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q268",
        "QuestionText": "In a workshop on AI limitations in testing, participants are discussing scenarios where human testers currently outperform AI. Which TWO situations demonstrate testing tasks where human intelligence remains essential?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Interpreting ambiguous requirements and negotiating test scope with stakeholders",
          "b": "Detecting subtle cultural biases in user interface design and content",
          "c": "Executing large-scale performance tests with thousands of virtual users",
          "d": "Analyzing code coverage metrics to identify untested execution paths"
        },
        "CorrectAnswer": [
          "b",
          "d"
        ],
        "Explanation": "a) INCORRECT – Requirement interpretation can be assisted by AI through natural language processing.<br><strong>b) CORRECT</strong> – Detecting subtle cultural biases requires nuanced understanding of human societies and contexts. Compare CT-AI curriculum Section 11.1 on testing tasks requiring human intelligence.<br>c) INCORRECT – Large-scale performance testing is highly automatable and suitable for AI assistance.<br><strong>d) CORRECT</strong> – Strategic test planning and resource allocation require human judgment and business context understanding. Compare CT-AI curriculum Section 11.1 on human testing strengths."
      }
    ]
  },
  {
    "LearningObjective": "HO-11.5.1",
    "LearningObjectiveDescription": "Implement a simple AI-based defect prediction system.",
    "KnowledgeType": "H2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q269",
        "QuestionText": "A team is building a defect prediction system using NASA metrics data. After initial implementation with a Naive Bayes model, they achieve moderate accuracy. Which TWO improvement strategies would likely enhance the defect prediction system's performance?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Removing features with low correlation to defect occurrence to reduce noise",
          "b": "Trying different ML algorithms like Random Forest or Gradient Boosting",
          "c": "Increasing model complexity regardless of dataset size",
          "d": "Using all available features without considering relevance"
        },
        "CorrectAnswer": [
          "a",
          "d"
        ],
        "Explanation": "<strong>a) CORRECT</strong> – Removing low-correlation features reduces noise and improves model focus on relevant predictors. Compare CT-AI curriculum Section 11.5 on defect prediction improvement.<br>b) INCORRECT – Trying different algorithms may help but doesn't address fundamental feature quality issues.<br>c) INCORRECT – Increasing complexity without regard to dataset size can lead to overfitting.<br><strong>d) CORRECT</strong> – Feature engineering to create more predictive metrics improves model performance. Compare CT-AI curriculum Section 11.5 on defect prediction feature engineering."
      }
    ]
  },
  {
    "LearningObjective": "HO-11.5.1",
    "LearningObjectiveDescription": "Implement a simple AI-based defect prediction system.",
    "KnowledgeType": "H2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q270",
        "QuestionText": "During a hands-on exercise implementing a defect prediction system, participants have built an initial model but want to improve its practical utility. Which TWO enhancements would make the system more valuable for test planning and resource allocation?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Incorporating people and organizational metrics alongside code metrics",
          "b": "Providing confidence scores or probability estimates for predictions",
          "c": "Focusing only on overall defect count without module-level predictions",
          "d": "Using the same model parameters for all projects regardless of context"
        },
        "CorrectAnswer": [
          "b",
          "c"
        ],
        "Explanation": "a) INCORRECT – People metrics are valuable but may not always be better than code metrics alone.<br><strong>b) CORRECT</strong> – Confidence scores enable risk-based test planning and resource allocation prioritization. Compare CT-AI curriculum Section 11.5 on practical defect prediction applications.<br><strong>c) CORRECT</strong> – File-level predictions enable more precise testing efforts and targeted code reviews. Compare CT-AI curriculum Section 11.5 on granular defect prediction.<br>d) INCORRECT – Model parameters should be tuned for specific project contexts and characteristics."
      }
    ]
  },
  {
    "LearningObjective": "AI-4.4.1",
    "LearningObjectiveDescription": "Recognize how poor data quality can cause problems with the resultant ML model.",
    "KnowledgeType": "K2",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q271",
        "QuestionText": "A healthcare AI system for disease diagnosis was trained on hospital data but performs poorly in rural clinics. Analysis reveals the training data contained mostly urban patient records with advanced diagnostic equipment, while rural clinics use basic equipment and treat different patient demographics. Which TWO data quality issues most likely caused the ML model's poor generalization?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "Irrelevant data features that don't reflect the operational environment differences",
          "b": "Unbalanced data distribution that underrepresented rural healthcare scenarios",
          "c": "Obsolete medical protocols that were updated after model training",
          "d": "Excessive data cleaning that removed important contextual information"
        },
        "CorrectAnswer": [
          "b",
          "c"
        ],
        "Explanation": "a) INCORRECT – While feature relevance is important, the primary issue is data distribution imbalance.<br><strong>b) CORRECT</strong> – Unbalanced data distribution favoring urban scenarios causes poor performance on underrepresented rural cases. Compare CT-AI curriculum Section 4.4 on data distribution problems.<br><strong>c) CORRECT</strong> – Protocol and equipment differences between urban and rural settings create domain shift issues. Compare CT-AI curriculum Section 4.4 on domain adaptation challenges.<br>d) INCORRECT – The scenario doesn't indicate excessive cleaning removed important contextual information."
      }
    ]
  },
  {
    "LearningObjective": "HO-3.5.1",
    "LearningObjectiveDescription": "Demonstrate underfitting and overfitting.",
    "KnowledgeType": "H0",
    "Details": [
      {
        "QuestionPoints": "2",
        "QuestionId": "q272",
        "QuestionText": "During a machine learning practical session, participants are training models on a dataset with clear polynomial relationships. One team's model shows high training error and fails to capture the underlying pattern, while another team's model perfectly fits every training point including noise. Which TWO corrective actions would address these specific fitting issues?",
        "AnswerOption": "Select 2 options. (2 out of 4)",
        "Answers": {
          "a": "For the high training error model: Increasing model complexity or adding relevant features",
          "b": "For the perfect-fit model: Applying regularization or collecting more diverse training data",
          "c": "For both models: Simply increasing the training dataset size without other changes",
          "d": "For the perfect-fit model: Reducing model complexity to prevent memorization"
        },
        "CorrectAnswer": [
          "a",
          "d"
        ],
        "Explanation": "<strong>a) CORRECT</strong> – Increasing complexity addresses underfitting by enabling the model to capture more complex patterns. Compare CT-AI curriculum Section 3.5 on addressing underfitting.<br>b) INCORRECT – Regularization helps overfitting but diverse data collection addresses different issues.<br>c) INCORRECT – Simply increasing data size without addressing model complexity issues may not resolve fundamental fitting problems.<br><strong>d) CORRECT</strong> – Ensemble methods combining multiple models can improve performance and reduce overfitting risks. Compare CT-AI curriculum Section 3.5 on ensemble approaches."
      }
    ]
  }
]